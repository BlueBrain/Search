{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "03.04.2020\n",
    "- Use 'bert-large-nli-mean-tokens'.\n",
    "\n",
    "06.04.2020\n",
    "- Add the lower ranking of some keywords (like 'diabetes').\n",
    "- Explore how synonyms impact sentence embeddings space search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Human curated WHO papers + query* on PMC / bioRxiv / medRxiv.\n",
    "\n",
    "**Query**\n",
    "\n",
    "- \"COVID-19\"\n",
    "- OR Coronavirus\n",
    "- OR \"Corona virus\"\n",
    "- OR \"2019-nCoV\"\n",
    "- OR \"SARS-CoV\"\n",
    "- OR \"MERS-CoV\"\n",
    "- OR “Severe Acute Respiratory Syndrome”\n",
    "- OR “Middle East Respiratory Syndrome” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import hashlib\n",
    "import time\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://p1.hiclipart.com/preview/389/225/95/party-emoji-face-qualatex-smiley-party-guy-38-foil-balloon-emoticon-birthday-party-hat-burtonburton-northwest-greetingsballoon-world-gift-png-clipart.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mThis is BLUE\u001b[0m\n",
      "\u001b[1mThis is BOLD\u001b[0m\n",
      "\u001b[96mThis is CYAN\u001b[0m\n",
      "\u001b[36mThis is DARKCYAN\u001b[0m\n",
      "\u001b[92mThis is GREEN\u001b[0m\n",
      "\u001b[95mThis is PURPLE\u001b[0m\n",
      "\u001b[91mThis is RED\u001b[0m\n",
      "\u001b[4mThis is UNDERLINE\u001b[0m\n",
      "\u001b[93mThis is YELLOW\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for var in dir(Color):\n",
    "    if not var.startswith('__') and var != 'END':\n",
    "        c = getattr(Color, var)\n",
    "        print(c + f\"This is {var}\" + Color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95mThis is a test\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(Color.BOLD + Color.PURPLE + \"This is a test\" + Color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: SQL, JSON, Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect('../cord19q/articles.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../v6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8q5ondtn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(72)90077-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pzfd0e50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Coronaviruses in Balkan nephritis</td>\n",
       "      <td>10.1016/0002-8703(80)90355-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6243850.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(80)90355-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid  sha  source_x                                              title  \\\n",
       "0  8q5ondtn  NaN  Elsevier  Intrauterine virus infections and congenital h...   \n",
       "1  pzfd0e50  NaN  Elsevier                  Coronaviruses in Balkan nephritis   \n",
       "\n",
       "                            doi pmcid  pubmed_id    license  \\\n",
       "0  10.1016/0002-8703(72)90077-4   NaN  4361535.0  els-covid   \n",
       "1  10.1016/0002-8703(80)90355-5   NaN  6243850.0  els-covid   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "1                                                NaN   1980-03-31   \n",
       "\n",
       "                                             authors                 journal  \\\n",
       "0                                  Overall, James C.  American Heart Journal   \n",
       "1  Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...  American Heart Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN          False   \n",
       "1                          NaN            NaN          False   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0              False  custom_license   \n",
       "1              False  custom_license   \n",
       "\n",
       "                                            url  \n",
       "0  https://doi.org/10.1016/0002-8703(72)90077-4  \n",
       "1  https://doi.org/10.1016/0002-8703(80)90355-5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata = pd.read_csv(data_path / \"metadata.csv\")\n",
    "df_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no title and no SHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_useless = df_metadata['title'].isna() & df_metadata['sha'].isna()\n",
    "df_metadata = df_metadata[~mask_useless]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8q5ondtn</td>\n",
       "      <td>9656dc6b0a8f22905c6a7117e123d6ae754cc7d4</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(72)90077-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pzfd0e50</td>\n",
       "      <td>f2e65cbf7654953918d9b88caa4d15b231fd23fd</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Coronaviruses in Balkan nephritis</td>\n",
       "      <td>10.1016/0002-8703(80)90355-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6243850.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(80)90355-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha  source_x  \\\n",
       "0  8q5ondtn  9656dc6b0a8f22905c6a7117e123d6ae754cc7d4  Elsevier   \n",
       "1  pzfd0e50  f2e65cbf7654953918d9b88caa4d15b231fd23fd  Elsevier   \n",
       "\n",
       "                                               title  \\\n",
       "0  Intrauterine virus infections and congenital h...   \n",
       "1                  Coronaviruses in Balkan nephritis   \n",
       "\n",
       "                            doi pmcid  pubmed_id    license  \\\n",
       "0  10.1016/0002-8703(72)90077-4   NaN  4361535.0  els-covid   \n",
       "1  10.1016/0002-8703(80)90355-5   NaN  6243850.0  els-covid   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "1                                                NaN   1980-03-31   \n",
       "\n",
       "                                             authors                 journal  \\\n",
       "0                                  Overall, James C.  American Heart Journal   \n",
       "1  Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...  American Heart Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN          False   \n",
       "1                          NaN            NaN          False   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0              False  custom_license   \n",
       "1              False  custom_license   \n",
       "\n",
       "                                            url  \n",
       "0  https://doi.org/10.1016/0002-8703(72)90077-4  \n",
       "1  https://doi.org/10.1016/0002-8703(80)90355-5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df_metadata['sha'].isna()\n",
    "df_metadata.loc[mask, 'sha'] = df_metadata[mask]['title'].apply(\n",
    "    lambda text: hashlib.sha1(str(text).encode(\"utf-8\")).hexdigest())\n",
    "df_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = []\n",
    "\n",
    "for f in data_path.rglob(\"*.json\"):\n",
    "    json_files.append(json.load(open(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing titles from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23db97382ebc4942901098abcd06a5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52097.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for json_file in tqdm(json_files):\n",
    "    if json_file['metadata']['title'] == '':\n",
    "        sha = json_file['paper_id']\n",
    "        idx = np.where(df_metadata['sha'] == sha)[0]\n",
    "        if len(idx) > 0:\n",
    "            new_title = df_metadata['title'].iloc[idx[0]]\n",
    "            json_file['metadata']['title'] = new_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with JSON files based on their SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_d = {\n",
    "    json_file['paper_id']: json_file\n",
    "    for json_file in json_files\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_in_paragraph(uid, sentence, width=80, indent=0, color=Color.BOLD + Color.PURPLE):\n",
    "    \"\"\"Highlight a given sentence in the paragraph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence\n",
    "    sentence: str\n",
    "        The sentence to highlight\n",
    "    width : int\n",
    "        The width to which to wrapt the returned paragraph\n",
    "    indent : int\n",
    "        The indentation for the lines in the returned apragraph\n",
    "    color : str\n",
    "        The color to use for the highlight encoded as an ANSI\n",
    "        escape code\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence` with the sentence highlighted\n",
    "        in color\n",
    "    \"\"\"\n",
    "    \n",
    "    sha = db.execute(f'SELECT Article FROM sections WHERE Id = {uid}').fetchall()[0][0]\n",
    "    if sha in json_files_d:\n",
    "        json_file = json_files_d[sha]\n",
    "        if sentence in json_file['metadata']['title']:\n",
    "            paragraph = json_file['metadata']['title']\n",
    "        else:\n",
    "            for text_chunk in json_file['abstract'] + json_file['body_text']:\n",
    "                paragraph = text_chunk['text']\n",
    "                if sentence in paragraph:\n",
    "                    break\n",
    "            else: \n",
    "                raise ValueError(\"sentence not found in body_text and abstract\")\n",
    "    else:\n",
    "        if not sha in list(df_metadata['sha']):\n",
    "            raise ValueError(f\"SHA  not found:\\nSHA={sha}\\nsentence={sentence}\")\n",
    "        df_row = df_metadata[df_metadata['sha'] == sha].iloc[0]\n",
    "        if sentence in df_row['title']:\n",
    "            paragraph = df_row['title']\n",
    "        else:\n",
    "            paragraph = df_row['abstract']\n",
    "    \n",
    "    start = paragraph.index(sentence)\n",
    "    end = start + len(sentence)\n",
    "    hightlighted_paragraph = ''.join([\n",
    "        paragraph[:start],\n",
    "        color + paragraph[start:end] + Color.END,\n",
    "        paragraph[end:]\n",
    "    ])\n",
    "    wrapped_lines = textwrap.wrap(hightlighted_paragraph, width=width)\n",
    "    wrapped_lines = [' ' * indent + line for line in wrapped_lines]\n",
    "    formatted_paragraph = '\\n'.join(wrapped_lines)\n",
    "    \n",
    "    return formatted_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Individuals with diabetes are at risk of infections, especially influenza and\n",
      "          pneumonia. This risk can be reduced, though not completely eliminated, by good\n",
      "          glycaemic control. All people with diabetes (above 2 years of age) are\n",
      "          recommended pneumococcal and annual influenza vaccinations. Not only this,\n",
      "          patients with diabetes have a severe disease when infected with respiratory\n",
      "          viruses. \u001b[1m\u001b[95mIndeed, diabetes was seen as an important risk factor for Data\n",
      "          about COVID-19 in patients with diabetes is limited at present.\u001b[0m Diabetes was\n",
      "          present in 42.3% of 26 fatalities due to COVID-19 in Wuhan, China [8] . In a\n",
      "          study in 140 patients with COVID-19 in Wuhan, China, diabetes was not a risk\n",
      "          factor for severe disease course [9] . However, another study in 150 patients\n",
      "          (68 deaths and 82 recovered patients) in Wuhan showed that the number of co-\n",
      "          morbidities to be a significant predictor of mortality [10] . Analysis of 11\n",
      "          studies regarding laboratory abnormalities in patients with COVID-19 did not\n",
      "          mention raised blood glucose or diabetes as predictor of severe disease [11] .\n",
      "          Notwithstanding these small series, a report of 72,314 cases of COVID-19\n",
      "          published by Chinese Centre for Disease Control and Prevention showed increased\n",
      "          mortality in people with diabetes (2.3%, overall and 7.3%, patients with\n",
      "          diabetes) [12] .\n"
     ]
    }
   ],
   "source": [
    "uid = 6797013\n",
    "sentence = \"Indeed, diabetes was seen as an important risk factor for Data about COVID-19 in patients with diabetes is limited at present.\"\n",
    "\n",
    "\n",
    "print(highlight_in_paragraph(uid, sentence, width=80, indent=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.5 s, sys: 3.43 s, total: 20.9 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load USE\n",
    "use_version = 5\n",
    "use = hub.load(f\"https://tfhub.dev/google/universal-sentence-encoder-large/{use_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.78 s, sys: 815 ms, total: 5.59 s\n",
      "Wall time: 3.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load SBERT\n",
    "sbert = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.37 s, sys: 12 s, total: 14.4 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load BioSentVec\n",
    "bsv = sent2vec.Sent2vecModel()\n",
    "bsv.load_model('BioSentVec_PubMed_MIMICIII-bigram_d700.bin')\n",
    "\n",
    "bsv_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def bsv_preprocess(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "    tokens = [token for token in word_tokenize(text)\n",
    "              if token not in punctuation and token not in bsv_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict = dict()\n",
    "with open('../synonyms_list.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    for l in [l_.strip().lower() for l_ in f]:\n",
    "        if l:\n",
    "            w = [l_.strip() for l_ in l.split('=')]\n",
    "            synonyms_dict[w[0]] = w[1:]\n",
    "\n",
    "del synonyms_dict['sars']\n",
    "\n",
    "synonyms_index = {x.lower(): k.lower() for k,v in synonyms_dict.items() for x in v}\n",
    "\n",
    "def sent_preprocessing(sentences, \n",
    "                      synonyms_index):\n",
    "    \"\"\"Preprocessing of the sentences. (Lower + Split + Replace Synonym)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of N strings.\n",
    "    synonyms_index: dict\n",
    "        Dictionary containing as key the synonym term and as values the reference of this term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\" \".join(synonyms_index.get(y, y) for y in word_tokenize(x.lower()))\n",
    "            for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences(sentences, embedding_name, embedding_model):\n",
    "    if embedding_name == 'USE':\n",
    "        return embedding_model(sentences).numpy()\n",
    "    \n",
    "    elif embedding_name == 'SBERT':\n",
    "        return np.stack(embedding_model.encode(sentences), axis=0)\n",
    "    \n",
    "    elif embedding_name == 'BSV':\n",
    "        preprocessed = [bsv_preprocess(x) for x in sentences]\n",
    "        return embedding_model.embed_sentences(preprocessed)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f'Embedding {repr(embedding_name)} not '\n",
    "                                  f'available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_NAMES = ['USE', 'SBERT', 'BSV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('sentence_embeddings/sentence_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_syns = np.load('sentence_embeddings/sentence_embeddings_merged_synonyms.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def investigate():\n",
    "    \n",
    "    def on_clicked(b):        \n",
    "        wout.clear_output()\n",
    "        with wout:\n",
    "            print()\n",
    "            t0 = time.time()\n",
    "            \n",
    "            if wcheck.value:\n",
    "                query_value = sent_preprocessing([wtext_query.value], synonyms_index)\n",
    "                exclu_value = sent_preprocessing([wtext_exclusion.value], synonyms_index)                \n",
    "            else:\n",
    "                query_value = [wtext_query.value]\n",
    "                exclu_value = [wtext_exclusion.value]\n",
    "                                    \n",
    "            print('Embedding query...    ', end=' ')\n",
    "            embedding_query = embed_sentences(query_value, \n",
    "                                              wselect_model.value, \n",
    "                                              eval(wselect_model.value.lower()))\n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            if exclu_value[0]:\n",
    "                print('Embedding exclusion...    ', end=' ')\n",
    "                embedding_exclu = embed_sentences(exclu_value, \n",
    "                                                  wselect_model.value, \n",
    "                                                  eval(wselect_model.value.lower()))\n",
    "                print(f'{time.time()-t0:.2f} s.')                \n",
    "            \n",
    "            print('Computing similarities...', end=' ')\n",
    "            # For scalability, we will replace this part with FAISS, as in the other part of the code base.\n",
    "            if wcheck.value:\n",
    "                arr = embeddings_syns[wselect_model.value]\n",
    "            else:\n",
    "                arr = embeddings[wselect_model.value]\n",
    "            uids, embedding_docs = arr[:, 0], arr[:, 1:]\n",
    "            similarities_query = cosine_similarity(X=embedding_query, Y=embedding_docs).squeeze()\n",
    "\n",
    "            if exclu_value[0]:\n",
    "                similarities_exclu = cosine_similarity(X=embedding_exclu, Y=embedding_docs).squeeze()\n",
    "            else:\n",
    "                similarities_exclu = np.zeros_like(similarities_query)\n",
    "                            \n",
    "            # now: maximize L = (1-a) * cos(x, query) - a * cos(x, exclusions)\n",
    "            alpha = exclusion_floatslider.value / 100 / 2\n",
    "            similarities = (1 - alpha) * similarities_query - alpha * similarities_exclu\n",
    "            \n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            print('Ranking documents...     ', end=' ')\n",
    "            indices = np.argsort(-similarities)[:wselect_count.value]\n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            print(Color.RED + f'\\nInvestigating: {query_value[0]}\\n' + Color.END)\n",
    "            \n",
    "            for i, (uid_, sim_) in enumerate(zip(uids[indices], similarities[indices])):\n",
    "                article_sha, text = db.execute('SELECT Article, Text FROM sections WHERE Id = ?', [uid_]).fetchall()[0]\n",
    "#                 print(f'Rank: {i} --- Section id: {int(uid_):>7,d} --- Similarity: {sim_:.2f}')\n",
    "                article_auth, article_title, date, ref = db.execute('SELECT Authors, Title, Published, Reference FROM articles WHERE Id = ?', [article_sha]).fetchall()[0]\n",
    "                article_auth = article_auth.split(';')[0] + ' et al.'\n",
    "                date = date.split()[0]\n",
    "                ref = ref if ref else ''\n",
    "                 \n",
    "                width = 80\n",
    "                if w_check_whole_paragraph.value:\n",
    "                    formatted_output = highlight_in_paragraph(uid_, text, width=width, indent=2)\n",
    "                else:\n",
    "                    formatted_output = textwrap.fill(text, width=width)\n",
    "                display(HTML(f'<a href=\"{ref}\">&nbsp;[{i:2d}]</a>'))\n",
    "                print(formatted_output)\n",
    "                print()\n",
    "#                 print(Color.BLUE + article_auth + ': \"' + article_title + '\", ' + date + Color.END)\n",
    "#                 display(HTML(f'<a href=\"{ref}\">{ref}</a>'))\n",
    "#                 print(Color.GREEN + f'Top {i+1} match (similarity: {sim_:.2f})' + Color.END)\n",
    "#                 print()\n",
    "    \n",
    "    wselect_model = widgets.ToggleButtons(\n",
    "        options=[ 'USE', 'SBERT', 'BSV'],\n",
    "        description='Model:',\n",
    "        tooltips=['Universal Sentence Encoder', 'Sentence BERT', 'BioSentVec'],\n",
    "    )\n",
    "    \n",
    "    wselect_count = widgets.IntSlider(value=10, min=0, max=100, description='Top N:',)\n",
    "    \n",
    "    wcheck = widgets.Checkbox(value=False, description='merge synonyms')\n",
    "    w_check_whole_paragraph = widgets.Checkbox(value=True, description='show whole paragraph')\n",
    "    \n",
    "    wtext_query = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'), \n",
    "                                   value='Glucose is a risk factor for COVID-19.',\n",
    "                                   description='Query: ')\n",
    "    wtext_exclusion = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'),\n",
    "                                       value='',\n",
    "                                       description='Exclusion: ')\n",
    "    exclusion_floatslider = widgets.IntSlider(min=0, max=100, \n",
    "                                                value=0, \n",
    "                                                step=5,\n",
    "                                                description='Exclusion strength [%]',\n",
    "                                                style = {'description_width': 'initial'}) # represents alpha * 2 * 100\n",
    "    \n",
    "    button = widgets.Button(description='Investigate!')\n",
    "    button.on_click(on_clicked)\n",
    "    \n",
    "    wout = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "    display(widgets.VBox([wselect_model, \n",
    "                          wselect_count, \n",
    "                          wcheck,\n",
    "                          w_check_whole_paragraph,\n",
    "                          wtext_query, \n",
    "                          wtext_exclusion, \n",
    "                          exclusion_floatslider,\n",
    "                          button, \n",
    "                          wout]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d334582aad6d4816a5497ae7081a5678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(ToggleButtons(description='Model:', options=('USE', 'SBERT', 'BSV'), tooltips=('Universal Sente…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "investigate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Queries\n",
    "\n",
    "1. Inhibition of N-glycosylation (using N-glycosylation inhibitors or Lectins) is a potential therapeutic approach for COVID-19 therapy.\n",
    "1. Is high blood / plasma sugar level or hyperglycemia associated with higher susceptibility to coronavirus infection or higher virus replication?\n",
    "1. Glucose or sugar is a risk factor for COVID-19.\n",
    "1. Ketogenic diet is protective against COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glucose', 'carbohydrates']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_dict['sugar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prediction factor', 'susceptibility factor', 'severity']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_dict['risk factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "And everyone knows <font style=\"background-color: #992200\"> coronavirus</font> is dangerous."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('And everyone knows <font style=\"background-color: #992200\"> coronavirus</font> is dangerous.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_syns.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
