{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "03.04.2020\n",
    "- Use 'bert-large-nli-mean-tokens'.\n",
    "\n",
    "06.04.2020\n",
    "- Add the lower ranking of some keywords (like 'diabetes').\n",
    "- Explore how synonyms impact sentence embeddings space search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Human curated WHO papers + query* on PMC / bioRxiv / medRxiv.\n",
    "\n",
    "**Query**\n",
    "\n",
    "- \"COVID-19\"\n",
    "- OR Coronavirus\n",
    "- OR \"Corona virus\"\n",
    "- OR \"2019-nCoV\"\n",
    "- OR \"SARS-CoV\"\n",
    "- OR \"MERS-CoV\"\n",
    "- OR “Severe Acute Respiratory Syndrome”\n",
    "- OR “Middle East Respiratory Syndrome” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import hashlib\n",
    "import time\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import sent2vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://p1.hiclipart.com/preview/389/225/95/party-emoji-face-qualatex-smiley-party-guy-38-foil-balloon-emoticon-birthday-party-hat-burtonburton-northwest-greetingsballoon-world-gift-png-clipart.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = Path(\"/raid/covid19_kaggle-data\")\n",
    "\n",
    "# data_path = main_dir / \"v6\"\n",
    "# sql_db_path = main_dir / \"cord19q\" / \"articles.sqlite\"\n",
    "# pafe_path = main_dir / \"pafe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"../data/2020-04-08\")\n",
    "cord_path = data_path / \"CORD-19-research-challenge\"\n",
    "databases_path = data_path / \"databases\"\n",
    "embeddings_path = data_path / \"embeddings\"\n",
    "assets_path = Path(\"../assets\")\n",
    "\n",
    "assert data_path.exists()\n",
    "assert cord_path.exists()\n",
    "assert databases_path.exists()\n",
    "assert embeddings_path.exists()\n",
    "assert assets_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mThis is BLUE\u001b[0m\n",
      "\u001b[1mThis is BOLD\u001b[0m\n",
      "\u001b[96mThis is CYAN\u001b[0m\n",
      "\u001b[36mThis is DARKCYAN\u001b[0m\n",
      "\u001b[92mThis is GREEN\u001b[0m\n",
      "\u001b[95mThis is PURPLE\u001b[0m\n",
      "\u001b[91mThis is RED\u001b[0m\n",
      "\u001b[4mThis is UNDERLINE\u001b[0m\n",
      "\u001b[93mThis is YELLOW\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "for var in dir(Color):\n",
    "    if not var.startswith('__') and var != 'END':\n",
    "        c = getattr(Color, var)\n",
    "        print(c + f\"This is {var}\" + Color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95mThis is a test\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(Color.BOLD + Color.PURPLE + \"This is a test\" + Color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user git+https://github.com/neuml/cord19q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scispacy model\n",
    "# !pip install --user https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.load('en_core_sci_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cord19q.etl.execute import Execute as Etl\n",
    "\n",
    "# Build SQLite database for metadata.csv and json full text files\n",
    "# Etl.run(str(cord_path), str(databases_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: SQL, JSON, Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(str(databases_path / \"articles.sqlite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8q5ondtn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(72)90077-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pzfd0e50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Coronaviruses in Balkan nephritis</td>\n",
       "      <td>10.1016/0002-8703(80)90355-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6243850.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(80)90355-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid  sha  source_x                                              title  \\\n",
       "0  8q5ondtn  NaN  Elsevier  Intrauterine virus infections and congenital h...   \n",
       "1  pzfd0e50  NaN  Elsevier                  Coronaviruses in Balkan nephritis   \n",
       "\n",
       "                            doi pmcid  pubmed_id    license  \\\n",
       "0  10.1016/0002-8703(72)90077-4   NaN  4361535.0  els-covid   \n",
       "1  10.1016/0002-8703(80)90355-5   NaN  6243850.0  els-covid   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "1                                                NaN   1980-03-31   \n",
       "\n",
       "                                             authors                 journal  \\\n",
       "0                                  Overall, James C.  American Heart Journal   \n",
       "1  Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...  American Heart Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN          False   \n",
       "1                          NaN            NaN          False   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0              False  custom_license   \n",
       "1              False  custom_license   \n",
       "\n",
       "                                            url  \n",
       "0  https://doi.org/10.1016/0002-8703(72)90077-4  \n",
       "1  https://doi.org/10.1016/0002-8703(80)90355-5  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metadata_original = pd.read_csv(cord_path / \"metadata.csv\")\n",
    "df_metadata_original.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no title and no SHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_useless = df_metadata_original['title'].isna() & df_metadata_original['sha'].isna()\n",
    "df_metadata = df_metadata_original[~mask_useless]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_pdf_parse</th>\n",
       "      <th>has_pmc_xml_parse</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8q5ondtn</td>\n",
       "      <td>9656dc6b0a8f22905c6a7117e123d6ae754cc7d4</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(72)90077-4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pzfd0e50</td>\n",
       "      <td>f2e65cbf7654953918d9b88caa4d15b231fd23fd</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Coronaviruses in Balkan nephritis</td>\n",
       "      <td>10.1016/0002-8703(80)90355-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6243850.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "      <td>https://doi.org/10.1016/0002-8703(80)90355-5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha  source_x  \\\n",
       "0  8q5ondtn  9656dc6b0a8f22905c6a7117e123d6ae754cc7d4  Elsevier   \n",
       "1  pzfd0e50  f2e65cbf7654953918d9b88caa4d15b231fd23fd  Elsevier   \n",
       "\n",
       "                                               title  \\\n",
       "0  Intrauterine virus infections and congenital h...   \n",
       "1                  Coronaviruses in Balkan nephritis   \n",
       "\n",
       "                            doi pmcid  pubmed_id    license  \\\n",
       "0  10.1016/0002-8703(72)90077-4   NaN  4361535.0  els-covid   \n",
       "1  10.1016/0002-8703(80)90355-5   NaN  6243850.0  els-covid   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "1                                                NaN   1980-03-31   \n",
       "\n",
       "                                             authors                 journal  \\\n",
       "0                                  Overall, James C.  American Heart Journal   \n",
       "1  Georgescu, Leonida; Diosi, Peter; Buţiu, Ioan;...  American Heart Journal   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_pdf_parse  \\\n",
       "0                          NaN            NaN          False   \n",
       "1                          NaN            NaN          False   \n",
       "\n",
       "   has_pmc_xml_parse  full_text_file  \\\n",
       "0              False  custom_license   \n",
       "1              False  custom_license   \n",
       "\n",
       "                                            url  \n",
       "0  https://doi.org/10.1016/0002-8703(72)90077-4  \n",
       "1  https://doi.org/10.1016/0002-8703(80)90355-5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = df_metadata['sha'].isna()\n",
    "df_metadata.loc[mask, 'sha'] = df_metadata.loc[mask, 'title'].apply(\n",
    "    lambda text: hashlib.sha1(str(text).encode(\"utf-8\")).hexdigest())\n",
    "df_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1549cd751fd641ae8094a9b4f11934dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52097.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_json = len(list(data_path.rglob(\"*.json\")))\n",
    "json_files = []\n",
    "\n",
    "for f in tqdm(data_path.rglob(\"*.json\"), total=n_json):\n",
    "    json_files.append(json.load(open(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing titles from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af650f53a9c4a27b9118d4d41b64ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=52097.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for json_file in tqdm(json_files):\n",
    "    if json_file['metadata']['title'] == '':\n",
    "        sha = json_file['paper_id']\n",
    "        idx = np.where(df_metadata['sha'] == sha)[0]\n",
    "        if len(idx) > 0:\n",
    "            new_title = df_metadata['title'].iloc[idx[0]]\n",
    "            json_file['metadata']['title'] = new_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with JSON files based on their SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_d = {\n",
    "    json_file['paper_id']: json_file\n",
    "    for json_file in json_files\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Using /tmp/tfhub_modules to cache modules.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.6 s, sys: 2.08 s, total: 16.7 s\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load USE\n",
    "use_version = 5\n",
    "use = hub.load(f\"https://tfhub.dev/google/universal-sentence-encoder-large/{use_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.73 s, sys: 749 ms, total: 5.48 s\n",
      "Wall time: 3.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load SBERT\n",
    "sbert = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/sschmidt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/ncbi-nlp/BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.42 s, sys: 11.5 s, total: 13.9 s\n",
      "Wall time: 13.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Load BioSentVec\n",
    "bsv = sent2vec.Sent2vecModel()\n",
    "bsv.load_model(str(assets_path / 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'))\n",
    "\n",
    "bsv_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def bsv_preprocess(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "    tokens = [token for token in word_tokenize(text)\n",
    "              if token not in punctuation and token not in bsv_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict = dict()\n",
    "with open(assets_path / 'synonyms_list.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    for l in [l_.strip().lower() for l_ in f]:\n",
    "        if l:\n",
    "            w = [l_.strip() for l_ in l.split('=')]\n",
    "            synonyms_dict[w[0]] = w[1:]\n",
    "\n",
    "del synonyms_dict['sars']\n",
    "\n",
    "synonyms_index = {x.lower(): k.lower() for k,v in synonyms_dict.items() for x in v}\n",
    "\n",
    "def sent_preprocessing(sentences, \n",
    "                      synonyms_index):\n",
    "    \"\"\"Preprocessing of the sentences. (Lower + Split + Replace Synonym)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of N strings.\n",
    "    synonyms_index: dict\n",
    "        Dictionary containing as key the synonym term and as values the reference of this term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\" \".join(synonyms_index.get(y, y) for y in word_tokenize(x.lower()))\n",
    "            for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences(sentences, embedding_name, embedding_model):\n",
    "    if embedding_name == 'USE':\n",
    "        return embedding_model(sentences).numpy()\n",
    "    \n",
    "    elif embedding_name == 'SBERT':\n",
    "        return np.stack(embedding_model.encode(sentences), axis=0)\n",
    "    \n",
    "    elif embedding_name == 'BSV':\n",
    "        preprocessed = [bsv_preprocess(x) for x in sentences]\n",
    "        return embedding_model.embed_sentences(preprocessed)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f'Embedding {repr(embedding_name)} not '\n",
    "                                  f'available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_NAMES = ['USE', 'SBERT', 'BSV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(embeddings_path / 'sentence_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_syns = np.load(embeddings_path / 'sentence_embeddings_merged_synonyms.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"My logger\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paragraph(uid, sentence):\n",
    "    \"\"\"Find the paragraph corresponding to the given sentece\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence\n",
    "    sentence: str\n",
    "        The sentence to highlight\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence`\n",
    "    \"\"\"\n",
    "    \n",
    "    sha, where_from = db.execute(f'SELECT Article, Name FROM sections WHERE Id = {uid}').fetchall()[0]\n",
    "    logger.debug(f\"uid = {uid}\")\n",
    "    logger.debug(f\"sha = {sha}\")\n",
    "    logger.debug(f\"where_from = {where_from}\")\n",
    "    logger.debug(f\"sentence = {sentence}\")\n",
    "    if sha in list(df_metadata['sha']) and where_from in ['TITLE', 'ABSTRACT']:\n",
    "        df_row = df_metadata[df_metadata['sha'] == sha].iloc[0]\n",
    "        if sentence in df_row['title']:\n",
    "            paragraph = df_row['title']\n",
    "        elif sentence in df_row['abstract']:\n",
    "            paragraph = df_row['abstract']\n",
    "        else:\n",
    "            raise ValueError(\"Sentence not found in title nor in abstract\")\n",
    "    elif sha in json_files_d:\n",
    "        json_file = json_files_d[sha]\n",
    "        if sentence in json_file['metadata']['title']:\n",
    "            paragraph = json_file['metadata']['title']\n",
    "        else:\n",
    "            for text_chunk in json_file['abstract'] + json_file['body_text']:\n",
    "                paragraph = text_chunk['text']\n",
    "                if sentence in paragraph:\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"sentence not found in body_text and abstract\")\n",
    "    else:\n",
    "        raise ValueError(\"SHA not found\")\n",
    "        \n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_in_paragraph(paragraph, sentence, width=80, indent=0, color=Color.BOLD + Color.PURPLE):\n",
    "    \"\"\"Highlight a given sentence in the paragraph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence\n",
    "    sentence: str\n",
    "        The sentence to highlight\n",
    "    width : int\n",
    "        The width to which to wrapt the returned paragraph\n",
    "    indent : int\n",
    "        The indentation for the lines in the returned apragraph\n",
    "    color : str\n",
    "        The color to use for the highlight encoded as an ANSI\n",
    "        escape code\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence` with the sentence highlighted\n",
    "        in color\n",
    "    \"\"\"\n",
    "    \n",
    "    start = paragraph.index(sentence)\n",
    "    end = start + len(sentence)\n",
    "    hightlighted_paragraph = ''.join([\n",
    "        paragraph[:start],\n",
    "        color + paragraph[start:end] + Color.END,\n",
    "        paragraph[end:]\n",
    "    ])\n",
    "    wrapped_lines = textwrap.wrap(hightlighted_paragraph, width=width)\n",
    "    wrapped_lines = [' ' * indent + line for line in wrapped_lines]\n",
    "    formatted_paragraph = '\\n'.join(wrapped_lines)\n",
    "    \n",
    "    return formatted_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Both of these compounds belong to a series of pyridazinamine analogues\n",
      "          synthesized by Janssen Research Foundation (Fig. 10) . They block the\n",
      "          replication of different rhin,'virus serotypes by stabilizing the viral particle\n",
      "          and inhibiting uncoating (Andries et al., 1988; Rombaut et al., 1991) .\n",
      "          Internalization of poliovirus particles into cells is not affected by R 78206,\n",
      "          whereas uncoating and release of these particles into the cytosol are blocked\n",
      "          (Rombaut et al., 1991; Ofori-Anyinam et al., 1993) . Assuming that these agents\n",
      "          directly inactivate viral particles (Andries et al., 1988) and that mutants\n",
      "          resistant to R 61837 are cross-resistant to other antiviral agents, including\n",
      "          WIN 51711 , it is likely that these agents can bind into the same hydrophobic\n",
      "          pocket, located beneath the canyon of human rhinoviruses, that is used by WIN\n",
      "          compounds (Chapman et al., 1991) . Direct evidence for this interaction has been\n",
      "          obtained by X-ray crystallographic analysis of human rhinovirus 14 complexed\n",
      "          with R 61837 (Chapman et al., 1991) . \u001b[1m\u001b[95mThis agent binds towards the\n",
      "          pocket entrance, but fails to occupy the end of the pocket (Chapman et al.,\n",
      "          1991) .\u001b[0m It has been suggested that adding a tail to R 61837 to fit into the\n",
      "          end of the pocket would improve the drug's activity against rhinovirus 14\n",
      "          (Chapman et al., 1991) . Clinical efficacy against the common cold was first\n",
      "          demonstrated for R 77975 (pirodavir) (Hayden et al., 1992) . Administration of R\n",
      "          61837, applied as a nasal spray with hydroxy-fl-cyclodextrin as a vehicle, to\n",
      "          human volunteers infected with rhinovirus 9 shows efficacy when given\n",
      "          prophylactically, but not when used therapeutically (AI-Nakib et al., 1989;\n",
      "          Barrow et al., 1990) .\n"
     ]
    }
   ],
   "source": [
    "uid = 81135\n",
    "sentence = \"This agent binds towards the pocket entrance, but fails to occupy the end of the pocket (Chapman et al., 1991) .\"\n",
    "\n",
    "paragraph = find_paragraph(uid, sentence)\n",
    "print(highlight_in_paragraph(paragraph, sentence, width=80, indent=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104c04618c7d463f9c7aa6c1ff24c6c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(ToggleButtons(description='Model:', options=('USE', 'SBERT', 'BSV'), tooltips=('Universal Sente…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def investigate():\n",
    "    \n",
    "    def on_clicked(b):        \n",
    "        wout.clear_output()\n",
    "        with wout:\n",
    "            print()\n",
    "            t0 = time.time()\n",
    "            \n",
    "            if wcheck.value:\n",
    "                query_value = sent_preprocessing([wtext_query.value], synonyms_index)\n",
    "                exclu_value = sent_preprocessing([wtext_exclusion.value], synonyms_index)                \n",
    "            else:\n",
    "                query_value = [wtext_query.value]\n",
    "                exclu_value = [wtext_exclusion.value]\n",
    "                                    \n",
    "            print('Embedding query...    ', end=' ')\n",
    "            embedding_query = embed_sentences(query_value, \n",
    "                                              wselect_model.value, \n",
    "                                              eval(wselect_model.value.lower()))\n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            if exclu_value[0]:\n",
    "                print('Embedding exclusion...    ', end=' ')\n",
    "                embedding_exclu = embed_sentences(exclu_value, \n",
    "                                                  wselect_model.value, \n",
    "                                                  eval(wselect_model.value.lower()))\n",
    "                print(f'{time.time()-t0:.2f} s.')                \n",
    "            \n",
    "            print('Computing similarities...', end=' ')\n",
    "            # For scalability, we will replace this part with FAISS, as in the other part of the code base.\n",
    "            if wcheck.value:\n",
    "                arr = embeddings_syns[wselect_model.value]\n",
    "            else:\n",
    "                arr = embeddings[wselect_model.value]\n",
    "            uids, embedding_docs = arr[:, 0], arr[:, 1:]\n",
    "            similarities_query = cosine_similarity(X=embedding_query, Y=embedding_docs).squeeze()\n",
    "\n",
    "            if exclu_value[0]:\n",
    "                similarities_exclu = cosine_similarity(X=embedding_exclu, Y=embedding_docs).squeeze()\n",
    "            else:\n",
    "                similarities_exclu = np.zeros_like(similarities_query)\n",
    "                            \n",
    "            deprioritizations ={\n",
    "                'None': (1, 0),\n",
    "                'Weak': (0.9, 0.1),\n",
    "                'Mild': (0.8, 0.3),\n",
    "                'Strong': (0.5, 0.5),\n",
    "                'Stronger': (0.5, 0.7), \n",
    "            }\n",
    "            # now: maximize L = a1 * cos(x, query) - a2 * cos(x, exclusions)\n",
    "            alpha_1, alpha_2 = deprioritizations[deprioritization_toggles.value]\n",
    "            similarities = alpha_1 * similarities_query - alpha_2 * similarities_exclu\n",
    "            \n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            print('Ranking documents...     ', end=' ')\n",
    "\n",
    "            # SUBSTRING EXCLUSIONS\n",
    "            excluded_words = [x for x in wtext_str_exclusion.value.lower().split('\\n') if x] # remove empty strings\n",
    "            \n",
    "            indices = np.argsort(-similarities)\n",
    "            indices_without_excluded = []\n",
    "            \n",
    "            ix = 0\n",
    "            while len(indices_without_excluded) < wselect_count.value:\n",
    "                sentence_text = db.execute('SELECT Text FROM sections WHERE Id = ?', [uids[indices[ix]]]).fetchall()[0][0].lower()\n",
    "                is_contained = any([w in sentence_text for w in excluded_words])\n",
    "                \n",
    "                if not is_contained:\n",
    "                    indices_without_excluded.append(indices[ix])\n",
    "\n",
    "                ix += 1\n",
    "            \n",
    "\n",
    "            print(f'{time.time()-t0:.2f} s. Excluded {ix - wselect_count.value} items based on substrings.')\n",
    "            \n",
    "            print(Color.RED + f'\\nInvestigating: {query_value[0]}\\n' + Color.END)\n",
    "            \n",
    "            for i, (uid_, sim_) in enumerate(zip(uids[indices_without_excluded], similarities[indices_without_excluded])):\n",
    "                article_sha, text = db.execute('SELECT Article, Text FROM sections WHERE Id = ?', [uid_]).fetchall()[0]\n",
    "                article_auth, article_title, date, ref = db.execute('SELECT Authors, Title, Published, Reference FROM articles WHERE Id = ?', [article_sha]).fetchall()[0]\n",
    "                article_auth = article_auth.split(';')[0] + ' et al.'\n",
    "                date = date.split()[0]\n",
    "                ref = ref if ref else ''\n",
    "                 \n",
    "                width = 80\n",
    "                if w_check_whole_paragraph.value:\n",
    "                    logger.debug(f\"UID={uid_}\")\n",
    "                    try:\n",
    "                        paragraph = find_paragraph(uid_, text)\n",
    "                        formatted_output = highlight_in_paragraph(paragraph, text, width=width, indent=2)\n",
    "                    except:\n",
    "                        formatted_output = \"<there was a problem retrieving the paragraph, the original sentence is:>\\n\"\n",
    "                        formatted_output += text\n",
    "                else:\n",
    "                    formatted_output = textwrap.fill(text, width=width)\n",
    "                display(HTML(f'<a href=\"{ref}\">&nbsp;[{i+1:2d}]</a>'))\n",
    "                print(formatted_output)\n",
    "                print()\n",
    "    \n",
    "    wselect_model = widgets.ToggleButtons(\n",
    "        options=[ 'USE', 'SBERT', 'BSV'],\n",
    "        description='Model:',\n",
    "        tooltips=['Universal Sentence Encoder', 'Sentence BERT', 'BioSentVec'],\n",
    "    )\n",
    "    \n",
    "    wselect_count = widgets.IntSlider(value=10, min=0, max=100, description='Top N:',)\n",
    "    \n",
    "    wcheck = widgets.Checkbox(value=True, description='merge synonyms')\n",
    "    w_check_whole_paragraph = widgets.Checkbox(value=True, description='show whole paragraph')\n",
    "    \n",
    "    wtext_query = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'), \n",
    "                                   value='Glucose is a risk factor for COVID-19.',\n",
    "                                   description='Query: ')\n",
    "    wtext_exclusion = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'),\n",
    "                                       value='',\n",
    "                                       description='Deprioritize: ')\n",
    "    deprioritization_toggles = widgets.ToggleButtons(\n",
    "        options=['None', 'Weak', 'Mild', 'Strong', 'Stronger'],\n",
    "        description='Deprioritization strength',\n",
    "        disabled=False,\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "#         tooltips=['Description of slow', 'Description of regular', 'Description of fast'],\n",
    "#         icons=['check'] * 5\n",
    "        style={'description_width': 'initial', 'button_width': '80px'},\n",
    "#         layout=widgets.Layout(width='100%', height='80px'),\n",
    "    )\n",
    "\n",
    "    wtext_str_exclusion = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'),\n",
    "                                       value='',\n",
    "                                       description='Substring Exclusion (newline separated): ',\n",
    "                                       style={'description_width': 'initial'})\n",
    "    button = widgets.Button(description='Investigate!')\n",
    "    button.on_click(on_clicked)\n",
    "    \n",
    "    wout = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "    display(widgets.VBox([wselect_model, \n",
    "                          wselect_count, \n",
    "                          wcheck,\n",
    "                          w_check_whole_paragraph,\n",
    "                          wtext_query, \n",
    "                          wtext_exclusion,\n",
    "                          deprioritization_toggles,\n",
    "                          wtext_str_exclusion,\n",
    "                          button, \n",
    "                          wout]))\n",
    "investigate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Queries\n",
    "\n",
    "1. Inhibition of N-glycosylation (using N-glycosylation inhibitors or Lectins) is a potential therapeutic approach for COVID-19 therapy.\n",
    "1. Is high blood / plasma sugar level or hyperglycemia associated with higher susceptibility to coronavirus infection or higher virus replication?\n",
    "1. Glucose or sugar is a risk factor for COVID-19.\n",
    "1. Ketogenic diet is protective against COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['glucose', 'carbohydrates']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_dict['sugar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prediction factor', 'susceptibility factor', 'severity']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synonyms_dict['risk factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "And everyone knows <font style=\"background-color: #992200\"> coronavirus</font> is dangerous."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML('And everyone knows <font style=\"background-color: #992200\"> coronavirus</font> is dangerous.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_syns.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
