{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODOs**\n",
    "- [ ] The trained SciBERT model `scibert_chemprot.tar.gz` stores inside itself \n",
    "  absolute paths to vocabulary text and weights! So it cannot be move around\n",
    "  without rewriting its metadata inside.\n",
    "- [ ] SciBERT can not be obtained with `pip install`, so currently one needs to \n",
    "    1. `git clone https://github.com/allenai/scibert.git`\n",
    "    2. `export PYTHONPATH=$PYTHONPATH:PATH_TO_SCIBERT`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the notebook\n",
    "(to be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "import IPython\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scispacy\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "import bbsearch.embedding_models as embedding_models\n",
    "from bbsearch.mining import ChemProt, TextMiningPipeline\n",
    "from bbsearch.widget import Widget\n",
    "from bbsearch.article_saver import ArticleSaver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Project\n",
    "The User choses/creates a project to host a KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set topic\n",
    "The user defines its topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "The user loads data from a data source (CORD-19).\n",
    "The loaded data forms the corpus.\n",
    "The user searches the CORPUS in Blue Brain Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = 'v7'\n",
    "\n",
    "data_path = Path(\"/raid/covid_data/data/\") / DATASET_VERSION\n",
    "assets_path = Path(\"/raid/covid_data/assets\")\n",
    "embeddings_path = data_path / \"embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "models_to_load = [\"SBioBERT\",\"BSV\"]\n",
    "embeddings_models = dict()\n",
    "precomputed_embeddings = dict()\n",
    "for model in models_to_load:\n",
    "    if model == 'BSV':\n",
    "        embeddings_models[model] = embedding_models.BSV(checkpoint_model_path=assets_path / 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin')\n",
    "    else:\n",
    "        embeddings_models[model] = getattr(embedding_models, model)()\n",
    "        \n",
    "    precomputed_embeddings[model] = np.load(embeddings_path / model / f'{model}.npy').astype('float32')  \n",
    "    # astype('float32') speeds up the search\n",
    "database = sqlite3.connect(data_path / 'cord19_v1.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_saver = ArticleSaver(database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs_widget = Widget(embeddings_models, precomputed_embeddings, database, article_saver=article_saver)\n",
    "bbs_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status of the Article Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = article_saver.summary_table()\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set schemas\n",
    "The user defines the KG schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a knowledge graph according to schemas\n",
    "The user extracts data from the text of a set of papers using selected Named Entity Recognizers and Relation Extractors from Blue Brain Search.\n",
    "The user can preview the extracted data.\n",
    "The user curates extracted data.\n",
    "The user links the extracted entities and relations to ontologies.\n",
    "The user saves data into Knowledge Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: raw text\n",
    "- **output**: csv table of extracted entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEXT = \"\"\"Autophagy maintains tumour growth through circulating\n",
    "arginine. Autophagy captures intracellular components and delivers them to\n",
    "lysosomes, where they are degraded and recycled to sustain metabolism and to\n",
    "enable survival during starvation. Acute, whole-body deletion of the essential \n",
    "autophagy gene Atg7 in adult mice causes a systemic metabolic defect that \n",
    "manifests as starvation intolerance and gradual loss of white adipose tissue, \n",
    "liver glycogen and muscle mass.  Cancer cells also benefit from autophagy. \n",
    "Deletion of essential autophagy genes impairs the metabolism, proliferation, \n",
    "survival and malignancy of spontaneous tumours in models of autochthonous \n",
    "cancer. Acute, systemic deletion of Atg7 or acute, systemic expression of a \n",
    "dominant-negative ATG4b in mice induces greater regression of KRAS-driven \n",
    "cancers than does tumour-specific autophagy deletion, which suggests that host \n",
    "autophagy promotes tumour growth.\n",
    "\"\"\".replace('\\n', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entities Extractors (EE)\n",
    "ee_model = spacy.load(\"en_ner_craft_md\")\n",
    "\n",
    "# Relations Extractors (RE)\n",
    "PATH_CHEMPROT_TRAINED_MODEL = assets_path / 'scibert_chemprot.tar.gz'\n",
    "re_models = {('CHEBI', 'GGP'): [ChemProt(PATH_CHEMPROT_TRAINED_MODEL)]}\n",
    "\n",
    "# Full Pipeline\n",
    "text_mining_pipeline = TextMiningPipeline(ee_model, re_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the output: csv table of extracted entities/relations.\n",
    "table_extractions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Widgets\n",
    "bbs_widgets = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs_widgets['articles_button'] = ipywidgets.Button(\n",
    "    description='Mine Selected Articles!',\n",
    "    layout=ipywidgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "def article_button(b):\n",
    "    global table_extractions\n",
    "    bbs_widgets['out'].clear_output()\n",
    "    complete_text = ''\n",
    "    with bbs_widgets['out']:\n",
    "        article_saver.retrieve_text()\n",
    "        table_extractions = pd.DataFrame()\n",
    "        for article_infos, text in article_saver.articles_text.items():\n",
    "            table_extractions = table_extractions.append(text_mining_pipeline(text, \n",
    "                                                                              article_id=article_infos[0], \n",
    "                                                                              return_prob=True))\n",
    "        display(table_extractions)\n",
    "        \n",
    "bbs_widgets['articles_button'].on_click(article_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Input Text\" Widget\n",
    "bbs_widgets['input_text'] = ipywidgets.Textarea(\n",
    "        value=DEFAULT_TEXT,\n",
    "        layout=ipywidgets.Layout(width='75%', height='300px')\n",
    "    )\n",
    "\n",
    "# \"Submit!\" Button\n",
    "bbs_widgets['submit_button'] = ipywidgets.Button(\n",
    "    description='Mine This Text!',\n",
    "    layout=ipywidgets.Layout(width='30%')\n",
    ")\n",
    "def cb(b):\n",
    "    global table_extractions\n",
    "    bbs_widgets['out'].clear_output()\n",
    "    with bbs_widgets['out']:\n",
    "        text = bbs_widgets['input_text'].value\n",
    "        table_extractions = text_mining_pipeline(text, return_prob=True)\n",
    "        display(table_extractions)\n",
    "bbs_widgets['submit_button'].on_click(cb)\n",
    "\n",
    "# \"Output Area\" Widget\n",
    "bbs_widgets['out'] = ipywidgets.Output(layout={'border': '0.5px solid black'})\n",
    "\n",
    "# Finalize Widgets\n",
    "ordered_widgets = list(bbs_widgets.values())\n",
    "main_widget = ipywidgets.VBox(ordered_widgets)\n",
    "IPython.display.display(main_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: csv table of extracted entities/relations\n",
    "- **output**: knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY Enable people to run the following part outside of DGX-1.\n",
    "import pandas as pd\n",
    "\n",
    "! wget -O extractions_example.csv 'https://drive.google.com/uc?export=download&id=11BBtkKsamru4kjUNev8lO_ulNMf7m3Ta'\n",
    "\n",
    "table_extractions = pd.read_csv('extractions_example.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The table has {table_extractions.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def represent_as_annotations(df: pd.DataFrame) -> Iterator[Dict]:\n",
    "    def _(row):\n",
    "        return {\n",
    "            '@context': 'http://www.w3.org/ns/anno.jsonld',\n",
    "            '@id': f'https://bbp.epfl.ch/covid19/{row.Index}',\n",
    "            '@type': 'Annotation',\n",
    "            'target': {\n",
    "                'source': row.paper_id,\n",
    "                'selector': {\n",
    "                    '@type': 'TextPositionSelector',\n",
    "                    'start': row.start,\n",
    "                    'end': row.end,\n",
    "                    'value': row.entity,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    return (_(x) for x in df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = list(represent_as_annotations(table_extractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Candidate:\n",
    "    \n",
    "    def __init__(self, identifier, term, score):\n",
    "        self.identifier = identifier\n",
    "        self.term = term\n",
    "        self.score = score\n",
    "    \n",
    "    def __repr__(self):\n",
    "        attrs = (f\"{k}={v!r}\" for k, v in self.__dict__.items())\n",
    "        return f\"Candidate({', '.join(attrs)})\"\n",
    "\n",
    "class EntityLinker:\n",
    "    \n",
    "    def __init__(self, ontology):\n",
    "        self.terms = None\n",
    "        self.model = None\n",
    "        self.index = None\n",
    "        self.ontology = list(ontology.items())\n",
    "    \n",
    "    def link(self, mentions):\n",
    "        # TODO Entity disambiguation is coming as the next step.\n",
    "        return self.candidates(mentions, limit=1)\n",
    "    \n",
    "    def disambiguate(self):\n",
    "        # TODO Entity disambiguation is coming as the next step.\n",
    "        pass\n",
    "    \n",
    "    def candidates(self, mentions, limit=3, threshold=0.6):\n",
    "        embeddings = self.model.transform(mentions)\n",
    "        distances, indexes = self.index.search(embeddings.toarray(), limit)\n",
    "        return [[Candidate(*self.terms[i], d) for i, d in zip(indexes[k], distances[k])\n",
    "                 if d <= threshold] for k in range(len(mentions))]\n",
    "    \n",
    "    def train(self):\n",
    "        self.model = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), max_df=0.95,\n",
    "                                     max_features=int(len(self.ontology)*0.1),\n",
    "                                     dtype=np.float32, norm='l2')\n",
    "        terms = [x for _, x in self.ontology]\n",
    "        embeddings = self.model.fit_transform(terms)\n",
    "        flags = np.array(embeddings.sum(axis=1) != 0).reshape(-1)\n",
    "        filtered_embeddings = embeddings[flags]\n",
    "        self.terms = [term for term, flag in zip(self.ontology, flags) if flag]\n",
    "        self.index = faiss.IndexFlatL2(filtered_embeddings.shape[1])\n",
    "        self.index.add(filtered_embeddings.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPORARY Retrieve entity_linking-terms.pkl (~ 5 MB).\n",
    "! wget -O entity_linking-terms.pkl 'https://drive.google.com/uc?export=download&id=1DyA9WL1YpEBO37KkDSCY3f1LWFfscO7F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('entity_linking-terms.pkl', 'rb') as f:\n",
    "    # TODO Linking to more ontologies than ChEBI is coming in the next steps.\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "linker = EntityLinker(ontology)\n",
    "\n",
    "linker.train()\n",
    "\n",
    "# Note: Takes around 12 secs on a BBP issued MacBook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Iterator\n",
    "from copy import deepcopy\n",
    "\n",
    "def enrich_annotations(annotations: Iterable[Dict], linker: EntityLinker) -> Iterator[Dict]:\n",
    "    def _(ann, can):\n",
    "        new = deepcopy(ann)\n",
    "        if can:\n",
    "            # TODO Entity disambiguation is coming as the next step.\n",
    "            new['body'] = {\n",
    "                '@id': can[0].identifier,\n",
    "                'label': can[0].term,\n",
    "            }\n",
    "        return new\n",
    "    mentions = [x['target']['selector']['value'] for x in annotations]\n",
    "    linked_mentions = linker.link(mentions)\n",
    "    return (_(ann, can) for ann, can in zip(annotations, linked_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_annotations = list(enrich_annotations(annotations, linker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Iterable, Dict\n",
    "from rdflib import Graph\n",
    "\n",
    "def load_knowledge_graph(jsonlds: Iterable[Dict]) -> Graph:\n",
    "    g = Graph()\n",
    "    for x in jsonlds:\n",
    "        g.parse(data=json.dumps(x), format='json-ld')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_graph = load_knowledge_graph(enriched_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The knowledge graph has {len(knowledge_graph)} triples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the knowledge graph\n",
    "Thee User reviews content of Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct knowledge graph\n",
    "The correct the Knowledge Graph is errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access the knowledge graph\n",
    "The user can search, visualize, and export the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version the knowledge graph\n",
    "The user can save a knowledge graph with a version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
