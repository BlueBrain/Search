{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the notebook\n",
    "(to be completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "import IPython\n",
    "import ipywidgets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import scispacy\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "from bbsearch.article_saver import ArticleSaver\n",
    "from bbsearch.remote_searcher import RemoteSearcher\n",
    "from bbsearch.widget import Widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Project\n",
    "The User choses/creates a project to host a KG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set topic\n",
    "The user defines its topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "The user loads data from a data source (CORD-19).\n",
    "The loaded data forms the corpus.\n",
    "The user searches the CORPUS in Blue Brain Search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_ENGINE_URL = os.getenv(\"SEARCH_ENGINE_URL\")\n",
    "BBS_DATA_PATH = Path(os.getenv(\"BBS_DATA_PATH\")) if \"BBS_DATA_PATH\" in os.environ else Path('/raid/bbs_data/')\n",
    "CORD19_VERSION = 'v7'\n",
    "\n",
    "cord_path = BBS_DATA_PATH / f'cord19_{CORD19_VERSION}'\n",
    "db_path = cord_path / 'databases' / 'cord19.db'\n",
    "trained_models_path = BBS_DATA_PATH / 'trained_models'\n",
    "\n",
    "assert db_path.is_file()\n",
    "assert SEARCH_ENGINE_URL is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"{}/hello\".format(SEARCH_ENGINE_URL))\n",
    "assert response.ok and response.json(), \"The server is not accessible\"\n",
    "\n",
    "searcher = RemoteSearcher(SEARCH_ENGINE_URL)\n",
    "\n",
    "database = sqlite3.connect(str(db_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_saver = ArticleSaver(database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs_widget = Widget(searcher, database, article_saver=article_saver)\n",
    "bbs_widget.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Status of the Article Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = article_saver.summary_table()\n",
    "display(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set schemas\n",
    "The user defines the KG schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a knowledge graph according to schemas\n",
    "The user extracts data from the text of a set of papers using selected Named Entity Recognizers and Relation Extractors from Blue Brain Search.\n",
    "The user can preview the extracted data.\n",
    "The user curates extracted data.\n",
    "The user links the extracted entities and relations to ontologies.\n",
    "The user saves data into Knowledge Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: raw text\n",
    "- **output**: csv table of extracted entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_TEXT = \"\"\"Autophagy maintains tumour growth through circulating\n",
    "arginine. Autophagy captures intracellular components and delivers them to\n",
    "lysosomes, where they are degraded and recycled to sustain metabolism and to\n",
    "enable survival during starvation. Acute, whole-body deletion of the essential \n",
    "autophagy gene Atg7 in adult mice causes a systemic metabolic defect that \n",
    "manifests as starvation intolerance and gradual loss of white adipose tissue, \n",
    "liver glycogen and muscle mass.  Cancer cells also benefit from autophagy. \n",
    "Deletion of essential autophagy genes impairs the metabolism, proliferation, \n",
    "survival and malignancy of spontaneous tumours in models of autochthonous \n",
    "cancer. Acute, systemic deletion of Atg7 or acute, systemic expression of a \n",
    "dominant-negative ATG4b in mice induces greater regression of KRAS-driven \n",
    "cancers than does tumour-specific autophagy deletion, which suggests that host \n",
    "autophagy promotes tumour growth.\n",
    "\"\"\".replace('\\n', ' ').replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mining_pipeline(mining_server_url):\n",
    "    \n",
    "    mining_server = False\n",
    "    \n",
    "    if mining_server_url is not None:\n",
    "        \n",
    "        import io\n",
    "        \n",
    "        response = requests.post(mining_server_url + \"/help\")\n",
    "        \n",
    "        if response.ok and response.json()['name'] == 'MiningServer':\n",
    "            mining_server = True\n",
    "            print('MiningServer')\n",
    "            \n",
    "            def textmining_pipeline(text, article_id=None, return_prob=False, debug=False):\n",
    "                request_json = {\n",
    "                    \"text\": text,\n",
    "                    \"article_id\": article_id,\n",
    "                    \"return_prob\": return_prob,\n",
    "                    \"debug\": debug,\n",
    "                }\n",
    "                response = requests.post(mining_server_url, json=request_json)\n",
    "                if response.headers[\"Content-Type\"] == \"text/csv\":\n",
    "                    with io.StringIO(response.text) as f:\n",
    "                        table_extractions = pd.read_csv(f)\n",
    "                else:\n",
    "                    print(\"Response content type is not text/csv.\")\n",
    "                    \n",
    "                return table_extractions\n",
    "         \n",
    "    if not mining_server:\n",
    "        print('Local Implementation')\n",
    "        \n",
    "        from bbsearch.mining import ChemProt, TextMiningPipeline\n",
    "        \n",
    "        # Entities Extractors (EE)\n",
    "        ee_model = spacy.load(\"en_ner_craft_md\")\n",
    "\n",
    "        # Relations Extractors (RE)\n",
    "        PATH_CHEMPROT_TRAINED_MODEL = trained_models_path / 'scibert_chemprot.tar.gz'\n",
    "        re_models = {('CHEBI', 'GGP'): [ChemProt(PATH_CHEMPROT_TRAINED_MODEL)]}\n",
    "\n",
    "        # Full Pipeline\n",
    "        text_mining_pipeline = TextMiningPipeline(ee_model, re_models)\n",
    "        \n",
    "        def textmining_pipeline(text, article_id=None, return_prob=False, debug=False):\n",
    "            table_extractions = text_mining_pipeline(text, article_id, return_prob=return_prob, debug=debug)\n",
    "            return table_extractions\n",
    "        \n",
    "    return textmining_pipeline\n",
    "\n",
    "mining_server_url = 'http://dgx1.bbp.epfl.ch:8852'\n",
    "text_mining = get_mining_pipeline(mining_server_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the output: csv table of extracted entities/relations.\n",
    "table_extractions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Widgets\n",
    "bbs_widgets = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbs_widgets['articles_button'] = ipywidgets.Button(\n",
    "    description='Mine Selected Articles!',\n",
    "    layout=ipywidgets.Layout(width='60%')\n",
    ")\n",
    "\n",
    "def article_button(b):\n",
    "    global table_extractions\n",
    "    bbs_widgets['out'].clear_output()\n",
    "    complete_text = ''\n",
    "    with bbs_widgets['out']:\n",
    "        article_saver.retrieve_text()\n",
    "        table_extractions = pd.DataFrame()\n",
    "        for article_id, section_name, paragraph_id, text \\\n",
    "                  in article_saver.df_chosen_texts[['article_id', 'section_name', 'paragraph_id', 'text']].values:\n",
    "            text_identifier = f'{article_id}:\"{section_name}\":{paragraph_id}'\n",
    "            table_extractions = table_extractions.append(\n",
    "                text_mining(text, article_id=text_identifier, return_prob=True), \n",
    "                ignore_index=True)\n",
    "        display(table_extractions)\n",
    "        \n",
    "bbs_widgets['articles_button'].on_click(article_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Input Text\" Widget\n",
    "bbs_widgets['input_text'] = ipywidgets.Textarea(\n",
    "        value=DEFAULT_TEXT,\n",
    "        layout=ipywidgets.Layout(width='75%', height='300px')\n",
    "    )\n",
    "\n",
    "# \"Submit!\" Button\n",
    "bbs_widgets['submit_button'] = ipywidgets.Button(\n",
    "    description='Mine This Text!',\n",
    "    layout=ipywidgets.Layout(width='30%')\n",
    ")\n",
    "def cb(b):\n",
    "    global table_extractions\n",
    "    bbs_widgets['out'].clear_output()\n",
    "    with bbs_widgets['out']:\n",
    "        text = bbs_widgets['input_text'].value\n",
    "        table_extractions = text_mining(text, return_prob=True)\n",
    "        display(table_extractions)\n",
    "bbs_widgets['submit_button'].on_click(cb)\n",
    "\n",
    "# \"Output Area\" Widget\n",
    "bbs_widgets['out'] = ipywidgets.Output(layout={'border': '0.5px solid black'})\n",
    "\n",
    "# Finalize Widgets\n",
    "ordered_widgets = list(bbs_widgets.values())\n",
    "main_widget = ipywidgets.VBox(ordered_widgets)\n",
    "IPython.display.display(main_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: csv table of extracted entities/relations\n",
    "- **output**: knowledge graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "if globals().get('table_extractions') is None:\n",
    "    ! wget -O extractions_example.csv 'https://drive.google.com/uc?export=download&id=11BBtkKsamru4kjUNev8lO_ulNMf7m3Ta'\n",
    "    table_extractions = pd.read_csv('extractions_example.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The table has {table_extractions.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Dict\n",
    "import pandas as pd\n",
    "\n",
    "def represent_as_annotations(df: pd.DataFrame) -> Iterator[Dict]:\n",
    "    def _(row):\n",
    "        return {\n",
    "            '@context': 'http://www.w3.org/ns/anno.jsonld',\n",
    "            '@id': f'https://bbp.epfl.ch/covid19/{row.Index}',\n",
    "            '@type': 'Annotation',\n",
    "            'target': {\n",
    "                'source': row.paper_id,\n",
    "                'selector': {\n",
    "                    '@type': 'TextPositionSelector',\n",
    "                    'start': row.start,\n",
    "                    'end': row.end,\n",
    "                    'value': row.entity,\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    return (_(x) for x in df.itertuples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = list(represent_as_annotations(table_extractions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Note: The file is around 26 MB.\n",
    "! wget -O entity_linking-terms.pkl 'https://drive.google.com/uc?export=download&id=1DyA9WL1YpEBO37KkDSCY3f1LWFfscO7F'\n",
    "\n",
    "with open('entity_linking-terms.pkl', 'rb') as f:\n",
    "    ontology = pickle.load(f)\n",
    "\n",
    "ontologies = ['ChEBI', 'SO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The entity linking considers {len(ontology)} terms from:', *ontologies, sep='\\n - ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class Candidate:\n",
    "    \n",
    "    def __init__(self, identifier, term, definition, score):\n",
    "        self.score = score\n",
    "        self.identifier = identifier\n",
    "        self.term = term\n",
    "        self.definition = definition\n",
    "    \n",
    "    def __repr__(self):\n",
    "        attrs = (f\"{k}={v!r}\" for k, v in self.__dict__.items())\n",
    "        return f\"Candidate({', '.join(attrs)})\"\n",
    "\n",
    "class EntityLinker:\n",
    "    \n",
    "    def __init__(self, ontology, nlp):\n",
    "        self.terms = None\n",
    "        self.model = None\n",
    "        self.index = None\n",
    "        self.ontology = [(k, v[0], v[1]) for k, v in ontology.items()]\n",
    "        self.nlp = nlp\n",
    "    \n",
    "    def link(self, mentions):\n",
    "        selections = self.candidates(mentions)\n",
    "        return [self.disambiguate(mention, candidates)\n",
    "                for mention, candidates in zip(mentions, selections)]\n",
    "    \n",
    "    def disambiguate(self, mention, candidates, threshold=0.6):\n",
    "        def _(cands):\n",
    "            # FIXME Used as a proxy until this is done during the other NLP operations.\n",
    "            doc = self.nlp(mention)\n",
    "            similarities = [(doc.similarity(self.nlp(x.definition.replace(mention, ''))), x)\n",
    "                            for x in cands if x.definition]\n",
    "            ranked = sorted(similarities, key=lambda x: x[0], reverse=True)\n",
    "            return ranked[0]\n",
    "        zeros = [x for x in candidates if x.score == 0]\n",
    "        if zeros:\n",
    "            chosen = _(zeros)\n",
    "            return chosen\n",
    "        else:\n",
    "            chosen = _(candidates)\n",
    "            return chosen if chosen[0] >= threshold else None\n",
    "    \n",
    "    def candidates(self, mentions, limit=3):\n",
    "        embeddings = self.model.transform(mentions)\n",
    "        distances, indexes = self.index.search(embeddings.toarray(), limit)\n",
    "        return [[Candidate(*self.terms[i], d) for i, d in zip(indexes[k], distances[k])]\n",
    "                for k in range(len(mentions))]\n",
    "    \n",
    "    def train(self):\n",
    "        self.model = TfidfVectorizer(analyzer='char', ngram_range=(3, 3), max_df=0.95,\n",
    "                                     max_features=int(len(self.ontology)*0.1),\n",
    "                                     dtype=np.float32, norm='l2')\n",
    "        terms = [x for _, x, _ in self.ontology]\n",
    "        embeddings = self.model.fit_transform(terms)\n",
    "        flags = np.array(embeddings.sum(axis=1) != 0).reshape(-1)\n",
    "        filtered_embeddings = embeddings[flags]\n",
    "        self.terms = [term for term, flag in zip(self.ontology, flags) if flag]\n",
    "        self.index = faiss.IndexFlatL2(filtered_embeddings.shape[1])\n",
    "        self.index.add(filtered_embeddings.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    nlp = ee_model\n",
    "except NameError:\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_ner_craft_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "linker = EntityLinker(ontology, nlp)\n",
    "linker.train()\n",
    "# Note: Takes around 45 secs on a BBP issued MacBook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Dict, Iterator\n",
    "from copy import deepcopy\n",
    "\n",
    "def enrich_annotations(annotations: Iterable[Dict], linker: EntityLinker) -> Iterator[Dict]:\n",
    "    def _(ann, can):\n",
    "        new = deepcopy(ann)\n",
    "        if can:\n",
    "            new['body'] = {\n",
    "                '@id': can[1].identifier,\n",
    "                'label': can[1].term,\n",
    "            }\n",
    "        return new\n",
    "    mentions = [x['target']['selector']['value'] for x in annotations]\n",
    "    linked_mentions = linker.link(mentions)\n",
    "    return (_(ann, can) for ann, can in zip(annotations, linked_mentions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "enriched_annotations = list(enrich_annotations(annotations, linker))\n",
    "# Note: Takes around 20 secs on a BBP issued MacBook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Iterable, Dict\n",
    "from rdflib import Graph\n",
    "\n",
    "def load_knowledge_graph(jsonlds: Iterable[Dict]) -> Graph:\n",
    "    g = Graph()\n",
    "    for x in jsonlds:\n",
    "        g.parse(data=json.dumps(x), format='json-ld')\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "knowledge_graph = load_knowledge_graph(enriched_annotations)\n",
    "# Note: Takes around 8 secs on a BBP issued MacBook Pro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The knowledge graph has {len(knowledge_graph)} triples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the knowledge graph\n",
    "Thee User reviews content of Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct knowledge graph\n",
    "The correct the Knowledge Graph is errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access the knowledge graph\n",
    "The user can search, visualize, and export the knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version the knowledge graph\n",
    "The user can save a knowledge graph with a version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
