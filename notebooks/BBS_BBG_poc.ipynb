{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of the notebook\n",
    "End to end pipeline for searching articles of interest, extracting entities of interest, building, accessing and deploying a knowled graph and a co-mention graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import ipywidgets\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "\n",
    "import jwt\n",
    "\n",
    "from bbsearch.widgets import ArticleSaver, MiningSchema, MiningWidget, SearchWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_dash.comms import _send_jupyter_config_comm_request, _jupyter_config\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "import dash_cytoscape as cyto\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JupyterDash configs\n",
    "_send_jupyter_config_comm_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "JupyterDash.infer_jupyter_proxy_config()\n",
    "cyto.load_extra_layouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cord_analytics.utils import (generate_curation_table,\n",
    "                                  link_ontology,\n",
    "                                  generate_comention_analysis,\n",
    "                                  build_cytoscape_data,\n",
    "                                  merge_with_ontology_linking,\n",
    "                                  resolve_taxonomy_to_types,\n",
    "                                  list_papers)\n",
    "            \n",
    "from bbg_apps.curation_app import (curation_app)\n",
    "from bbg_apps.visualization_app import (visualization_app)\n",
    "from bbg_apps.topic_app import (TopicWidget, DataSaverWidget)\n",
    "\n",
    "from kganalytics.export import load_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgforge.core import KnowledgeGraphForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the ontology linking data...\")\n",
    "linking = pd.read_pickle(\"/gpfs/bbp.cscs.ch/project/proj116/network_analytics/data/cord_47_linking.pkl\")\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Project\n",
    "\n",
    "The user chooses / creates a project to host a KG.\n",
    "\n",
    "* Use the [Nexus web application](https://bbp.epfl.ch/nexus/web) to get a token.\n",
    "* Once a token is obtained then proceed to paste it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a 'forge' to manage (create, access and deploy) the knowledge graph within a given Blue Brain Nexus Project.\n",
    "FORGE_CONFIG_FILE = os.getenv(\"FORGE_CONFIG_FILE\") \n",
    "assert (FORGE_CONFIG_FILE is not None) \n",
    "forge = KnowledgeGraphForge(FORGE_CONFIG_FILE,token=TOKEN, debug=True)\n",
    "agent_username = jwt.decode(TOKEN,  verify=False)['preferred_username']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set topic\n",
    "The user defines a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = TopicWidget(forge, agent_username)\n",
    "widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    table_extractions,\n",
    "    curated_table_extractions,\n",
    "    curation_meta_data,\n",
    "    loaded_graphs,\n",
    "    visualization_configs,\n",
    "    topic_resource_id\n",
    ") = widget.get_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "The user loads data from a data source (CORD-19). The loaded data forms the corpus. The user searches the CORPUS in Blue Brain Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search server URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_ENGINE_URL = os.getenv(\"SEARCH_ENGINE_URL\", \"http://dgx1.bbp.epfl.ch:8850\")\n",
    "assert SEARCH_ENGINE_URL is not None\n",
    "\n",
    "response = requests.post(\"{}/help\".format(SEARCH_ENGINE_URL))\n",
    "assert response.ok and response.json()['name'] == 'SearchServer', \"The server is not accessible\"\n",
    "print(f\"This server is using the database: {response.json()['database']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MySQL URL and engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MYSQL_DB_URI = os.getenv(\"MYSQL_DB_URI\", \"dgx1.bbp.epfl.ch:8853\")\n",
    "bbs_mysql_engine = sqlalchemy.create_engine(f'mysql+pymysql://guest:guest@{MYSQL_DB_URI}/cord19_v47')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_saver = ArticleSaver(connection=bbs_mysql_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_widget = SearchWidget(\n",
    "    bbs_search_url=SEARCH_ENGINE_URL,\n",
    "    bbs_mysql_engine=bbs_mysql_engine,\n",
    "    article_saver=article_saver,\n",
    "    results_per_page=3)\n",
    "search_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show saved articles and paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = search_widget.saved_results()\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"For information: \\n \n",
    "      - The query showed {len(df_results['Article ID'].unique())} different articles.\n",
    "      - Saved {len(df_results[(df_results['Paragraph']=='✓') & (df_results['Article'] != '✓')])} paragraph(s)\n",
    "      - Saved {len(df_results[df_results['Article']=='✓']['Article ID'].unique())} article(s)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set schemas\n",
    "The user defines the KG schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_schema = MiningSchema()\n",
    "\n",
    "mining_schema.add_entity(\"CELL_COMPARTMENT\")\n",
    "mining_schema.add_entity(\"CELL_TYPE\")\n",
    "mining_schema.add_entity(\"CHEMICAL\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"CONDITION\")\n",
    "mining_schema.add_entity(\"DISEASE\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"DRUG\")\n",
    "mining_schema.add_entity(\"ORGAN\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"ORGANISM\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"PATHWAY\", ontology_source=\"Reactome\")\n",
    "mining_schema.add_entity(\"PROTEIN\", ontology_source=\"NCIT\")\n",
    "\n",
    "mining_schema.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a knowledge graph according to schemas\n",
    "The user extracts data from the text of a set of papers using selected Named Entity Recognizers and Relation Extractors from Blue Brain Search.\n",
    "The user can preview the extracted data.\n",
    "The user curates extracted data.\n",
    "The user links the extracted entities and relations to ontologies.\n",
    "The user saves data into Knowledge Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: raw text\n",
    "- **output**: csv table of extracted entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_MINING_URL = os.getenv(\"TEXT_MINING_URL\", \"http://dgx1.bbp.epfl.ch:8852\")\n",
    "response = requests.post(TEXT_MINING_URL + \"/help\")\n",
    "assert response.ok and response.json()['name'] == 'MiningServer'\n",
    "print(f\"This server is using the database: {response.json()['database']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_widget = MiningWidget(\n",
    "    mining_server_url=TEXT_MINING_URL,\n",
    "    mining_schema=mining_schema,\n",
    "    article_saver=article_saver,\n",
    ")\n",
    "mining_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrame of extractions\n",
    "table_extractions = mining_widget.get_extracted_table()\n",
    "\n",
    "# Drop duplicates in DataFrame\n",
    "columns_duplicates = table_extractions.columns.tolist()\n",
    "try:\n",
    "    columns_duplicates.remove('entity_type')\n",
    "    table_extractions = table_extractions.drop_duplicates(subset=columns_duplicates, keep='first', ignore_index=True)\n",
    "    table_extractions = table_extractions.dropna(subset=[\"entity\"])\n",
    "except ValueError:\n",
    "    raise ValueError(\n",
    "        \"Could not find the extraction table, make sure you have launched the mining procedure in the widget above\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate the table with extracted entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: csv table of extracted entities/relations\n",
    "- **output**: csv table with curated and ontology linked entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The table has {table_extractions.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Setting default term filters: the user can remove them later on in the UI if need be ...\")\n",
    "default_term_filters = 'Glucose; Covid-19; SARS-CoV-2; Diabetes; IL-1; ACE2; glycosylation; hyperglycemia; shock; fatigue; CVD; vasoconstriction; lactate; insulin; SP-D; HbA1c; LDH; glycolysis; GLUT; macrophage; lymphocytes; ventilation;SARS; ARDS; Cytokine Storm; pneumonia; multi-organs failure; thrombosis; inflammation; IL-6; CRP; D-Dimer; Ferritin; Lung Disease; Hypertension; Aging; COPD; angiotensin 2 (or angiotensin II or AngII); Obesity; ICU (intensive care unit); ventilation; ketogenic diet'.split(\"; \")\n",
    "filtered_table_extractions = table_extractions.copy()\n",
    "\n",
    "default_found_term_filters = set() \n",
    "for term_filter in default_term_filters:\n",
    "    entities_to_keep = filtered_table_extractions[\n",
    "        filtered_table_extractions[\"entity\"].apply(lambda x: x.lower() == term_filter.lower())][\"entity\"].unique()\n",
    "    if entities_to_keep is not None and len(entities_to_keep) > 0:\n",
    "        default_found_term_filters.add(tuple(entities_to_keep))\n",
    "term_filter_options = [term_filter[0] for term_filter in default_found_term_filters]\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Prepating curatation data...\")\n",
    "curation_input_table, factor_counts = generate_curation_table(filtered_table_extractions)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Loading default ontology type mapping...\")\n",
    "with open('/gpfs/bbp.cscs.ch/project/proj116/bbg/ontology-linking/ncit_to_mltypes_mapping.json', \"rb\") as f:\n",
    "    default_type_mapping = json.load(f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the curation app. In case of the error 'Address already in use', try specifying another port (for example, in the range 8072-8099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_app.set_default_terms_to_include(term_filter_options)\n",
    "curation_app.set_table(curation_input_table.copy())\n",
    "curation_app.set_ontology_linking_callback(lambda x: link_ontology(linking, default_type_mapping, x))\n",
    "\n",
    "curation_app.run(port=8070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_table_extractions = curation_app.get_curated_table()\n",
    "curation_meta_data = {\n",
    "    \"factor_counts\": factor_counts,\n",
    "    \"nodes_to_keep\": curation_app.get_terms_to_include(),\n",
    "    \"n_most_frequent\": curation_app.n_most_frequent if curation_app.n_most_frequent else 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_table_extractions[\"paper\"] = curated_table_extractions[\"paper\"].apply(lambda x: set(x))\n",
    "curated_table_extractions[\"paragraph\"] = curated_table_extractions[\"paragraph\"].apply(lambda x: set(x))\n",
    "curated_table_extractions[\"section\"] = curated_table_extractions[\"section\"].apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a co-mention graph from curated entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: csv table with curated and ontology linked entities/relations\n",
    "- **output**: graph objects with co-occurrence network and its spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_data = curated_table_extractions[[\"entity_type\"]].rename(columns={\"entity_type\": \"type\"})\n",
    "graphs, trees = generate_comention_analysis(\n",
    "    curated_table_extractions,  curation_meta_data[\"factor_counts\"],\n",
    "    n_most_frequent=curation_meta_data[\"n_most_frequent\"], type_data=type_data, \n",
    "    factors=[\"paper\", \"paragraph\"], keep=curation_meta_data[\"factor_counts\"], cores=10)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graphs = None\n",
    "GRAPH_OBJECTS = {\n",
    "    \"Topic-centered network (paper-based)\": {\n",
    "        \"graph\": graphs[\"paper\"],\n",
    "        \"tree\": trees[\"paper\"],\n",
    "        \"default_top_n\": 100\n",
    "    },\n",
    "    \"Topic-centered network (paragraph-based)\": {\n",
    "        \"graph\": graphs[\"paragraph\"],\n",
    "        \"tree\": trees[\"paragraph\"],\n",
    "        \"default_top_n\": 100\n",
    "    },\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the co-mention graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_graph = None\n",
    "paragraph_graph = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "prefix = \"/gpfs/bbp.cscs.ch/project/proj116/network_analytics/data/graphs/cord_47/full_3000\"\n",
    "\n",
    "print(\"Loading pre-generated graphs with 3'000 entities...\")\n",
    "print(\"\\t - paper-based network\")\n",
    "paper_graph = load_network(\"{}_paper_edge_list.pkl\".format(prefix), \"{}_paper_node_list.pkl\".format(prefix))\n",
    "paper_spanning_tree = load_network(\"{}_paper_tree_edge_list.pkl\".format(prefix), \"{}_paper_tree_node_list.pkl\".format(prefix))\n",
    "nx.set_node_attributes(\n",
    "    paper_spanning_tree, {\n",
    "        n: len(paper_spanning_tree.nodes[n][\"paper\"])\n",
    "        for n in paper_spanning_tree.nodes()\n",
    "    },\n",
    "    \"paper_frequency\")\n",
    "\n",
    "print(\"\\t - paragraph-based network\")\n",
    "paragraph_graph = load_network(\"{}_paragraph_edge_list.pkl\".format(prefix), \"{}_paragraph_node_list.pkl\".format(prefix))\n",
    "paragraph_spanning_tree = load_network(\"{}_paragraph_tree_edge_list.pkl\".format(prefix), \"{}_paragraph_node_list.pkl\".format(prefix))\n",
    "nx.set_node_attributes(\n",
    "    paragraph_spanning_tree, {\n",
    "        n: len(paragraph_spanning_tree.nodes[n][\"paper\"])\n",
    "        for n in paragraph_spanning_tree.nodes()\n",
    "    },\n",
    "    \"paper_frequency\")\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Loading pre-computed node positions...\")\n",
    "with open(\"/gpfs/bbp.cscs.ch/project/proj116/network_analytics/data/positions/paper_3000.json\", \"r\") as f:\n",
    "    paper_positions = json.load(f)\n",
    "\n",
    "with open(\"/gpfs/bbp.cscs.ch/project/proj116/network_analytics/data/positions/paragraph_3000.json\", \"r\") as f:\n",
    "    paragraph_positions = json.load(f)\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "print(\"Building a cytoscape representation of loaded graphs...\")\n",
    "paper_3000_cyto = build_cytoscape_data(paper_spanning_tree, positions=paper_positions)\n",
    "paragraph_3000_cyto = build_cytoscape_data(paragraph_spanning_tree, positions=paragraph_positions)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH_OBJECTS = {}\n",
    "if loaded_graphs is not None:\n",
    "    for g, data in loaded_graphs.items():\n",
    "        GRAPH_OBJECTS[g] = {\n",
    "            \"graph\": data[\"graph\"],\n",
    "            \"tree\": data[\"tree\"] if \"tree\" in data else None,\n",
    "            \"default_top_n\": 100\n",
    "        }\n",
    "\n",
    "# Add precomputed graphs\n",
    "if paper_graph and paragraph_graph:\n",
    "    GRAPH_OBJECTS.update({\n",
    "        \"Naive pre-computed network (paper-based, 3000)\": {\n",
    "            \"graph\": paper_graph,\n",
    "            \"tree\": paper_spanning_tree,\n",
    "            \"positions\": paper_positions\n",
    "        },\n",
    "        \"Naive pre-computed network (paragraph-based, 3000)\": {\n",
    "            \"graph\": paragraph_graph,\n",
    "            \"tree\": paragraph_spanning_tree,\n",
    "            \"positions\": paragraph_positions,\n",
    "        }\n",
    "    })\n",
    "    \n",
    "for k, v in GRAPH_OBJECTS.items():\n",
    "    tree = v[\"tree\"] if \"tree\" in v else None\n",
    "    positions = v[\"positions\"] if \"positions\" in v else None  \n",
    "    default_top_n = v[\"default_top_n\"] if \"default_top_n\" in v else None\n",
    "    full_graph_view = v[\"full_graph_view\"] if \"full_graph_view\" in v else False\n",
    "    visualization_app.set_graph(\n",
    "        k, v[\"graph\"], tree_object=tree, positions=positions,\n",
    "        default_top_n=default_top_n, full_graph_view=full_graph_view)\n",
    "\n",
    "if visualization_configs is None:\n",
    "    visualization_app.set_current_graph(\"Topic-centered network (paper-based)\")\n",
    "    \n",
    "visualization_app.set_list_papers_callback(lambda x: list_papers(bbs_mysql_engine, x))\n",
    "definitions = linking[[\"concept\", \"definition\"]].groupby(\"concept\").aggregate(lambda x: list(x)[0]).to_dict()[\"definition\"]\n",
    "visualization_app.set_entity_definitons(definitions)\n",
    "visualization_app._db_error_message = \"Failed to retreive papers (check if the variable 'bbs_mysql_engine' was initialized or check the DB connection)\"\n",
    "\n",
    "if visualization_configs is not None:\n",
    "    visualization_app._current_graph = visualization_configs[\"current_graph\"]\n",
    "    visualization_app._configure_layout(visualization_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the app will display only top-50 most frequent nodes, you can then choose to show all the nodes in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_app.run(port=8079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the knowledge graph\n",
    "Content of the Knowledge Graph is validated. In this version, syntactic validation (i.e. are the identifiers correct, ...) is performed when building the knowledge graph. If the knowledge graph is successfully built then the validation passes. In case of warning (i.e because of a weird character (+,...) in an extracted entity), the user can go back to the curation step and further curate extracted entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct knowledge graph\n",
    "Correction involves going back to the extraction and/or curation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access the knowledge graph\n",
    "The user can search, visualize, and export the knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version the knowledge graph\n",
    "The user can save a knowledge graph with a version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_graphs = visualization_app.export_graphs(\n",
    "    [\"Topic-centered network (paper-based)\", \"Topic-centered network (paragraph-based)\"], \n",
    ")\n",
    "visualization_configs = visualization_app.get_configs()\n",
    "edit_history = visualization_app.get_edit_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver_widget = DataSaverWidget(\n",
    "    forge, TOKEN, topic_resource_id,\n",
    "    table_extractions,\n",
    "    curated_table_extractions,\n",
    "    curation_meta_data,\n",
    "    exported_graphs,\n",
    "    visualization_configs,\n",
    "    edit_history,\n",
    "    temp_prefix=\"/gpfs/bbp.cscs.ch/project/proj116/\")\n",
    "\n",
    "saver_widget.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BBS-BBG",
   "language": "python",
   "name": "devel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
