{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "BBSearch is a text mining toolbox focused on scientific use cases.\n",
    "\n",
    "Copyright (C) 2020  Blue Brain Project, EPFL.\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Lesser General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public License\n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.\n",
    "-->\n",
    "\n",
    "# Goal of the notebook\n",
    "End to end pipeline for searching articles of interest, extracting entities of interest, building, accessing and deploying a knowled graph and a co-mention graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import requests\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import ipywidgets\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "import sqlalchemy\n",
    "from sqlalchemy.sql import select\n",
    "from sqlalchemy.sql import and_, or_, not_\n",
    "\n",
    "import jwt\n",
    "\n",
    "from bluesearch.widgets import ArticleSaver, MiningSchema, MiningWidget, SearchWidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jupyter_dash.comms import _send_jupyter_config_comm_request, _jupyter_config\n",
    "from jupyter_dash import JupyterDash\n",
    "\n",
    "import dash_cytoscape as cyto\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JupyterDash configs\n",
    "_send_jupyter_config_comm_request()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(3)\n",
    "JupyterDash.infer_jupyter_proxy_config()\n",
    "cyto.load_extra_layouts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kganalytics.export import load_network\n",
    "from kganalytics.utils import top_n\n",
    "\n",
    "from cord19kg.utils import (generate_curation_table,\n",
    "                            link_ontology,\n",
    "                            generate_cooccurrence_analysis,\n",
    "                            build_cytoscape_data,\n",
    "                            resolve_taxonomy_to_types)\n",
    "from cord19kg.apps.curation_app import curation_app\n",
    "from cord19kg.apps.visualization_app import visualization_app\n",
    "from cord19kg.apps.topic_widgets import (TopicWidget, DataSaverWidget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kgforge.core import KnowledgeGraphForge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the ontology linking data...\")\n",
    "\n",
    "ONTOLOGY_LINKING_DATA_FILE = os.getenv(\"ONTOLOGY_LINKING_DATA_FILE\") \n",
    "assert (ONTOLOGY_LINKING_DATA_FILE is not None) \n",
    "linking = pd.read_pickle(ONTOLOGY_LINKING_DATA_FILE)\n",
    "linking = linking.rename(columns=({\"_subclassof_label\": \"taxonomy\"}))\n",
    "print(\"Done.\")\n",
    "GRAPH_OBJECTS = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set a Project\n",
    "\n",
    "The user chooses / creates a project to host a KG.\n",
    "\n",
    "* Use the [Nexus web application](https://bbp.epfl.ch/nexus/web) to get a token.\n",
    "* Once a token is obtained then proceed to paste it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "TOKEN = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a 'forge' to manage (create, access and deploy) the knowledge graph within a given Blue Brain Nexus Project.\n",
    "FORGE_CONFIG_FILE = os.getenv(\"FORGE_CONFIG_FILE\") \n",
    "assert (FORGE_CONFIG_FILE is not None) \n",
    "forge = KnowledgeGraphForge(FORGE_CONFIG_FILE,token=TOKEN, debug=True)\n",
    "agent_username = jwt.decode(TOKEN,  verify=False)['preferred_username']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set topic\n",
    "The user defines a topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget = TopicWidget(forge, TOKEN)\n",
    "widget.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    table_extractions,\n",
    "    curated_table_extractions,\n",
    "    curation_meta_data,\n",
    "    loaded_graphs,\n",
    "    visualization_configs,\n",
    "    topic_resource_id\n",
    ") = widget.get_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import\n",
    "The user loads data from a data source (CORD-19). The loaded data forms the corpus. The user searches the CORPUS in Blue Brain Search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the search server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEARCH_ENGINE_URL = os.getenv(\"SEARCH_ENGINE_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SEARCH_ENGINE_URL:\n",
    "    print(\"The variable SEARCH_ENGINE_URL is not set\", file=sys.stderr)\n",
    "    print(f\"Please fix this before proceeding\", file=sys.stderr)\n",
    "else:\n",
    "    try:\n",
    "        response = requests.post(f\"{SEARCH_ENGINE_URL}/help\")\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        print(f\"Could not connect to the search server on {SEARCH_ENGINE_URL}\", file=sys.stderr)\n",
    "        print(f\"Error: {exc}\", file=sys.stderr)\n",
    "        print(f\"Please fix this before proceeding\", file=sys.stderr)\n",
    "    else:\n",
    "        if not response.ok or response.json().get(\"name\") != \"SearchServer\":\n",
    "            print(f\"The server at {SEARCH_ENGINE_URL} is not a valid search server\", file=sys.stderr)\n",
    "        else:\n",
    "            print(f\"Connected to the search server on {SEARCH_ENGINE_URL}\")\n",
    "            print(f\"This server is using the database: {response.json().get('database')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the MySQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_URL = os.getenv(\"DB_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not DB_URL:\n",
    "    print(\"The variable DB_URL is not set\", file=sys.stderr)\n",
    "    print(f\"Please fix this before proceeding\", file=sys.stderr)\n",
    "else:\n",
    "    try:\n",
    "        bbs_mysql_engine = sqlalchemy.create_engine(f\"mysql+pymysql://guest:guest@{DB_URL}\")\n",
    "        result = bbs_mysql_engine.execute(\"select 1\").fetchone()\n",
    "    except sqlalchemy.exc.OperationalError as exc:\n",
    "        print(f\"Can't connect to the MySQL server on {DB_URL}, please fix this before proceeding.\", file=sys.stderr)\n",
    "        print(f\"Error: {exc}\", file=sys.stderr)\n",
    "        print(f\"Please fix this before proceeding\", file=sys.stderr)\n",
    "    else:\n",
    "        print(f\"Connected to the MySQL server on {DB_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_saver = ArticleSaver(connection=bbs_mysql_engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_widget = SearchWidget(\n",
    "    bbs_search_url=SEARCH_ENGINE_URL,\n",
    "    bbs_mysql_engine=bbs_mysql_engine,\n",
    "    article_saver=article_saver,\n",
    "    results_per_page=3)\n",
    "search_widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show saved articles and paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = search_widget.saved_results()\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\"\"For information: \\n \n",
    "      - The query showed {len(df_results['Article ID'].unique())} different articles.\n",
    "      - Saved {len(df_results[(df_results['Paragraph']=='✓') & (df_results['Article'] != '✓')])} paragraph(s)\n",
    "      - Saved {len(df_results[df_results['Article']=='✓']['Article ID'].unique())} article(s)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set schemas\n",
    "The user defines the KG schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_schema = MiningSchema()\n",
    "\n",
    "mining_schema.add_entity(\"CELL_COMPARTMENT\")\n",
    "mining_schema.add_entity(\"CELL_TYPE\")\n",
    "mining_schema.add_entity(\"CHEMICAL\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"CONDITION\")\n",
    "mining_schema.add_entity(\"DISEASE\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"DRUG\")\n",
    "mining_schema.add_entity(\"ORGAN\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"ORGANISM\", ontology_source=\"NCIT\")\n",
    "mining_schema.add_entity(\"PATHWAY\", ontology_source=\"Reactome\")\n",
    "mining_schema.add_entity(\"PROTEIN\", ontology_source=\"NCIT\")\n",
    "\n",
    "mining_schema.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a knowledge graph according to schemas\n",
    "The user extracts data from the text of a set of papers using selected Named Entity Recognizers and Relation Extractors from Blue Brain Search.\n",
    "The user can preview the extracted data.\n",
    "The user curates extracted data.\n",
    "The user links the extracted entities and relations to ontologies.\n",
    "The user saves data into Knowledge Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: raw text\n",
    "- **output**: csv table of extracted entities/relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the mining server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_MINING_URL = os.getenv(\"TEXT_MINING_URL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not TEXT_MINING_URL:\n",
    "    print(\"The variable TEXT_MINING_URL is not set\", file=sys.stderr)\n",
    "    print(f\"Please fix this before proceeding\", file=sys.stderr)\n",
    "else:\n",
    "    try:\n",
    "        response = requests.post(f\"{TEXT_MINING_URL}/help\")\n",
    "    except requests.exceptions.RequestException as exc:\n",
    "        print(f\"Could not connect to the server on {TEXT_MINING_URL}\", file=sys.stderr)\n",
    "        print(f\"Error: {exc}\", file=sys.stderr)\n",
    "        print(f\"Please fix this before proceeding\", file=sys.stderr)\n",
    "    else:\n",
    "        if not response.ok or response.json().get(\"name\") != \"MiningServer\":\n",
    "            print(f\"The server at {TEXT_MINING_URL} is not a valid mining server\", file=sys.stderr)\n",
    "        else:\n",
    "            print(f\"Connected to the mining server on {TEXT_MINING_URL}\")\n",
    "            print(f\"This server is using the database: {response.json().get('database')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mining_widget = MiningWidget(\n",
    "    mining_server_url=TEXT_MINING_URL,\n",
    "    mining_schema=mining_schema,\n",
    "    article_saver=article_saver,\n",
    ")\n",
    "mining_widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DataFrame of extractions\n",
    "table_extractions = mining_widget.get_extracted_table()\n",
    "\n",
    "# Drop duplicates in DataFrame\n",
    "columns_duplicates = table_extractions.columns.tolist()\n",
    "try:\n",
    "    columns_duplicates.remove('entity_type')\n",
    "    table_extractions = table_extractions.drop_duplicates(subset=columns_duplicates, keep='first', ignore_index=True)\n",
    "    table_extractions = table_extractions.dropna(subset=[\"entity\"])\n",
    "except ValueError:\n",
    "    raise ValueError(\n",
    "        \"Could not find the extraction table, make sure you have launched the mining procedure in the widget above\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Curate the table with extracted entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: csv table of extracted entities/relations\n",
    "- **output**: csv table with curated and ontology linked entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The table has {table_extractions.shape[0]} rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Setting default term filters: the user can remove them later on in the UI if need be ...\")\n",
    "default_term_filters = 'Glucose; Covid-19; SARS-CoV-2; Diabetes; IL-1; ACE2; glycosylation; hyperglycemia; shock; fatigue; CVD; vasoconstriction; lactate; insulin; SP-D; HbA1c; LDH; glycolysis; GLUT; macrophage; lymphocytes; ventilation;SARS; ARDS; Cytokine Storm; pneumonia; multi-organs failure; thrombosis; inflammation; IL-6; CRP; D-Dimer; Ferritin; Lung Disease; Hypertension; Aging; COPD; angiotensin 2 (or angiotensin II or AngII); Obesity; ICU (intensive care unit); ventilation; ketogenic diet'.split(\"; \")\n",
    "filtered_table_extractions = table_extractions.copy()\n",
    "filtered_table_extractions = filtered_table_extractions.rename(columns={\"paper_id\": \"occurrence\"})\n",
    "\n",
    "default_found_term_filters = set() \n",
    "for term_filter in default_term_filters:\n",
    "    entities_to_keep = filtered_table_extractions[\n",
    "        filtered_table_extractions[\"entity\"].apply(lambda x: x.lower() == term_filter.lower())][\"entity\"].unique()\n",
    "    if entities_to_keep is not None and len(entities_to_keep) > 0:\n",
    "        default_found_term_filters.add(tuple(entities_to_keep))\n",
    "term_filter_options = [term_filter[0] for term_filter in default_found_term_filters]\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Prepating curatation data...\")\n",
    "curation_input_table, factor_counts = generate_curation_table(filtered_table_extractions)\n",
    "print(\"Done.\")\n",
    "\n",
    "print(\"Loading default ontology type mapping...\")\n",
    "TYPE_MAPPING_FILE = os.getenv(\"TYPE_MAPPING_FILE\") \n",
    "assert (TYPE_MAPPING_FILE is not None) \n",
    "with open(TYPE_MAPPING_FILE, \"rb\") as f:\n",
    "    default_type_mapping = json.load(f)\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the curation app. In case of the error 'Address already in use', try specifying another port (for example, in the range 8072-8099)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curation_app.set_default_terms_to_include(term_filter_options)\n",
    "curation_app.set_table(curation_input_table.copy())\n",
    "curation_app.set_ontology_linking_callback(lambda x: link_ontology(linking, default_type_mapping, x))\n",
    "\n",
    "curation_app.run(port=8070)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_table_extractions = curation_app.get_curated_table()\n",
    "curation_meta_data = {\n",
    "    \"factor_counts\": factor_counts,\n",
    "    \"nodes_to_keep\": curation_app.get_terms_to_include(),\n",
    "    \"n_most_frequent\": curation_app.n_most_frequent if curation_app.n_most_frequent else 100\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curated_table_extractions[\"paper\"] = curated_table_extractions[\"paper\"].apply(lambda x: set(x))\n",
    "curated_table_extractions[\"paragraph\"] = curated_table_extractions[\"paragraph\"].apply(lambda x: set(x))\n",
    "curated_table_extractions[\"section\"] = curated_table_extractions[\"section\"].apply(lambda x: set(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a co-mention graph from curated entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **input**: csv table with curated and ontology linked entities/relations\n",
    "- **output**: graph objects with co-occurrence network and its spanning tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_data = curated_table_extractions[[\"entity_type\"]].rename(columns={\"entity_type\": \"type\"})\n",
    "graphs, trees = generate_cooccurrence_analysis(\n",
    "    curated_table_extractions,  curation_meta_data[\"factor_counts\"],\n",
    "    n_most_frequent=curation_meta_data[\"n_most_frequent\"], type_data=type_data, \n",
    "    factors=[\"paper\", \"paragraph\"], keep=curation_meta_data[\"factor_counts\"], cores=10)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_graphs = None\n",
    "GRAPH_OBJECTS = {\n",
    "    \"Topic-centered network (paper-based)\": {\n",
    "        \"graph\": graphs[\"paper\"],\n",
    "        \"tree\": trees[\"paper\"],\n",
    "        \"default_top_n\": 100\n",
    "    },\n",
    "    \"Topic-centered network (paragraph-based)\": {\n",
    "        \"graph\": graphs[\"paragraph\"],\n",
    "        \"tree\": trees[\"paragraph\"],\n",
    "        \"default_top_n\": 100\n",
    "    },\n",
    "}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the co-mention graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loaded_graphs is not None:\n",
    "    for g, data in loaded_graphs.items():\n",
    "        GRAPH_OBJECTS[g] = {\n",
    "            \"graph\": data[\"graph\"],\n",
    "            \"tree\": data[\"tree\"] if \"tree\" in data else None,\n",
    "            \"default_top_n\": 100\n",
    "        }\n",
    "\n",
    "for k, v in GRAPH_OBJECTS.items():\n",
    "    tree = v[\"tree\"] if \"tree\" in v else None\n",
    "    positions = v[\"positions\"] if \"positions\" in v else None  \n",
    "    default_top_n = v[\"default_top_n\"] if \"default_top_n\" in v else None\n",
    "    full_graph_view = v[\"full_graph_view\"] if \"full_graph_view\" in v else False\n",
    "    visualization_app.set_graph(\n",
    "        k, v[\"graph\"], tree_object=tree, positions=positions,\n",
    "        default_top_n=default_top_n, full_graph_view=full_graph_view)\n",
    "\n",
    "if visualization_configs is None:\n",
    "    visualization_app.set_current_graph(\"Topic-centered network (paper-based)\")\n",
    "\n",
    "# Set paper meta-data look up callback\n",
    "def list_papers(mysql_engine, papers, limit=200):\n",
    "    META_DATA = sqlalchemy.MetaData(bind=mysql_engine, reflect=True)\n",
    "    articles = META_DATA.tables[\"articles\"]\n",
    "    clauses = or_( *[articles.c.article_id == x for x in papers[:limit]] )\n",
    "    s = select([\n",
    "        articles.c.title,\n",
    "        articles.c.authors,\n",
    "        articles.c.abstract,\n",
    "        articles.c.doi,\n",
    "        articles.c.url,\n",
    "        articles.c.journal,\n",
    "        articles.c.pmcid,\n",
    "        articles.c.pubmed_id,\n",
    "        articles.c.publish_time\n",
    "    ]).where(clauses)\n",
    "    result = mysql_engine.execute(s)\n",
    "    results = []\n",
    "    for row in result:\n",
    "        results.append(row)\n",
    "    return results    \n",
    "visualization_app.set_list_papers_callback(lambda x: list_papers(bbs_mysql_engine, x))\n",
    "\n",
    "# Set definitions look up callback\n",
    "definitions = linking[[\"concept\", \"definition\"]].groupby(\"concept\").aggregate(lambda x: list(x)[0]).to_dict()[\"definition\"]\n",
    "visualization_app.set_entity_definitons(definitions)\n",
    "visualization_app._db_error_message = \"Failed to retreive papers (check if the variable 'bbs_mysql_engine' was initialized or check the DB connection)\"\n",
    "\n",
    "# Set aggregated entities look up callback\n",
    "def get_aggregated_entities(entity, n):\n",
    "    if \"aggregated_entities\" in curated_table_extractions.columns:\n",
    "        aggregated = curated_table_extractions.loc[entity][\"aggregated_entities\"]\n",
    "    else:\n",
    "        aggregated = [entity]\n",
    "    if table_extractions is not None:\n",
    "        df = curation_input_table.set_index(\"entity\")\n",
    "        if entity in curated_table_extractions.index:\n",
    "            freqs = df.loc[aggregated][\"paper_frequency\"].to_dict()\n",
    "        else:\n",
    "            return {}\n",
    "    else:\n",
    "        df = table_extractions.copy()\n",
    "        df[\"entity\"] = data[\"entity\"].apply(lambda x: x.lower())\n",
    "        freqs = df[df[\"entity\"].apply(lambda x: x.lower() in aggregated)].groupby(\"entity\").aggregate(\n",
    "            lambda x: len(x))[\"entity_type\"].to_dict()\n",
    "    if len(freqs) == 0:\n",
    "        return {}\n",
    "    return {e: freqs[e] for e in top_n(freqs, n)}\n",
    "\n",
    "visualization_app.set_aggregated_entities_callback(\n",
    "    lambda x: get_aggregated_entities(x, 10))\n",
    "\n",
    "if visualization_configs is not None:\n",
    "    visualization_app._current_graph = visualization_configs[\"current_graph\"]\n",
    "    visualization_app._configure_layout(visualization_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the app will display only top-50 most frequent nodes, you can then choose to show all the nodes in the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualization_app.run(port=8079)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the knowledge graph\n",
    "Content of the Knowledge Graph is validated. In this version, syntactic validation (i.e. are the identifiers correct, ...) is performed when building the knowledge graph. If the knowledge graph is successfully built then the validation passes. In case of warning (i.e because of a weird character (+,...) in an extracted entity), the user can go back to the curation step and further curate extracted entities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct knowledge graph\n",
    "Correction involves going back to the extraction and/or curation steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access the knowledge graph\n",
    "The user can search, visualize, and export the knowledge graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version the knowledge graph\n",
    "The user can save a knowledge graph with a version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_graphs = visualization_app.export_graphs(\n",
    "    [\"Topic-centered network (paper-based)\", \"Topic-centered network (paragraph-based)\"], \n",
    ")\n",
    "visualization_configs = visualization_app.get_configs()\n",
    "edit_history = visualization_app.get_edit_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_FILE_PATH = os.getenv(\"TEMP_FILE_PATH\") \n",
    "assert (TEMP_FILE_PATH is not None)\n",
    "\n",
    "saver_widget = DataSaverWidget(\n",
    "    forge, TOKEN, topic_resource_id,\n",
    "    table_extractions,\n",
    "    curated_table_extractions,\n",
    "    curation_meta_data,\n",
    "    exported_graphs,\n",
    "    visualization_configs,\n",
    "    edit_history,\n",
    "    temp_prefix=TEMP_FILE_PATH)\n",
    "\n",
    "saver_widget.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
