{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import pathlib\n",
    "import sys\n",
    "sys.path.append('/raid/users/krepl/BlueBrainSearch/submodules/scibert')\n",
    "\n",
    "from allennlp.predictors import Predictor\n",
    "import ipywidgets\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import scispacy\n",
    "from scibert.models.text_classifier import TextClassifier\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_re(doc, sent, ent_1, ent_2, etype_symbols):\n",
    "    \"\"\"Preprocess sentence for the SciBERT relation extraction model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        The entire document (input text). Use for absolute referencing.\n",
    "    \n",
    "    sent : spacy.tokens.Span\n",
    "        One sentence from the `doc` where we look for relations.\n",
    "\n",
    "    ent_1 : spacy.tokens.Span\n",
    "        The first entity in the sentence. One can get its type by using the `label_` attribute.\n",
    "    \n",
    "    ent_2 : spacy.tokens.Span\n",
    "        The second entity in the sentence.One can get its type by using the `label_` attribute.\n",
    "    \n",
    "    etype_symbols: dict\n",
    "        Keys represent different entity types (\"GGP\", \"CHEBBI\") and the values are tuples of size 2.\n",
    "        Each of these tuples represents the starting, ending symbol to wrap the recognized entity with.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : str\n",
    "        String representing an annotated sentence created out of the original one. \n",
    "    \"\"\"\n",
    "    \n",
    "    etype_1 = ent_1.label_\n",
    "    etype_2 = ent_2.label_\n",
    "    \n",
    "    if not (etype_1 in etype_symbols and etype_2 in etype_symbols):\n",
    "        raise ValueError('Please specify the special symbols for both of the entity types.')\n",
    "    \n",
    "    tokens = []\n",
    "    i = sent.start\n",
    "    while i < sent.end:\n",
    "        new_token = ' ' # hack to keep the punctuation nice\n",
    "\n",
    "        if ent_1.start == i:\n",
    "            start, end = ent_1.start, ent_1.end\n",
    "            new_token += etype_symbols[etype_1][0] + doc[start:end].text + etype_symbols[etype_1][1]\n",
    "\n",
    "        elif ent_2.start == i:\n",
    "            start, end = ent_2.start, ent_2.end\n",
    "            new_token += etype_symbols[etype_2][0] + doc[start:end].text + etype_symbols[etype_2][1]\n",
    "\n",
    "        else:\n",
    "            start, end = i, i + 1\n",
    "            new_token = doc[i].text if doc[i].is_punct else new_token + doc[i].text\n",
    "\n",
    "        tokens.append(new_token)\n",
    "        i += end - start\n",
    "    \n",
    "    return ''.join(tokens).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers\n",
    "def preprocess_re_OLD(doc, sent, etype_1, etype_2, etype_symbols):\n",
    "    \"\"\"Preprocess sentence for the SciBERT relation extraction model.\n",
    "    \n",
    "    Given one sentence, one extracts all possible pairs for etype_1 and etype_1\n",
    "    entities and annotates the original sentence accordingly.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : spacy.tokens.Doc\n",
    "        The entire document (input text). Use for absolute referencing.\n",
    "    \n",
    "    sent : spacy.tokens.Span\n",
    "        One sentence from the `doc` where we look for relations.\n",
    "\n",
    "    etype_1 : str\n",
    "        Name of the subject entity type - i.e. \"GGP\". See Spacy and SciSpacy pretrained models.\n",
    "    \n",
    "    etype_2 : str\n",
    "        Name of the object entity type - i.e \"CHEBI\". See Spacy and SciSpacy pretrained models.\n",
    "    \n",
    "    etype_symbols: dict\n",
    "        Keys represent different entity types (\"GGP\", \"CHEBBI\") and the values are tuples of size 2.\n",
    "        Each of these tuples represents the starting, ending symbol to wrap the recognized entity with.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    result : list\n",
    "        List of strings representing all annotated sentences created out of the original one. Might\n",
    "        be empty.\n",
    "    \"\"\"\n",
    "    if not (etype_1 in etype_symbols and etype_2 in etype_symbols):\n",
    "        raise ValueError('Please specify the special symbols for both of the entity types.')\n",
    "    \n",
    "    if etype_1 == etype_2:\n",
    "        raise ValueError('The two entity types need to be different')\n",
    "    \n",
    "    entities_1 = {ent for ent in sent.ents if ent.label_ == etype_1}  # remove duplicates\n",
    "    entities_2 = {ent for ent in sent.ents if ent.label_ == etype_2}  # remove duplicates\n",
    "    \n",
    "    all_pairs = itertools.product(entities_1, entities_2)\n",
    "    \n",
    "    result = []\n",
    "    for first, second in all_pairs:\n",
    "        # add sanity check no overlap (should be impossible)\n",
    "        \n",
    "        tokens = []\n",
    "        i = sent.start\n",
    "\n",
    "        while i < sent.end:\n",
    "            new_token = ' ' # hack to keep the punctuation nice\n",
    "    \n",
    "            if first.start == i:\n",
    "                start, end = first.start, first.end\n",
    "                new_token += etype_symbols[etype_1][0] + doc[start:end].text + etype_symbols[etype_1][1]\n",
    "    \n",
    "            elif second.start == i:\n",
    "                start, end = second.start, second.end\n",
    "                new_token += etype_symbols[etype_2][0] + doc[start:end].text + etype_symbols[etype_2][1]\n",
    "            \n",
    "            else:\n",
    "                start, end = i, i + 1\n",
    "                new_token = doc[i].text if doc[i].is_punct else new_token + doc[i].text\n",
    "             \n",
    "            tokens.append(new_token)\n",
    "            i += end - start\n",
    "    \n",
    "        result.append(''.join(tokens).strip())\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REModel(ABC):\n",
    "    \"\"\"Abstract interface for relationship extraction models.\n",
    "    \n",
    "    Inspired by SciBERT.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def predict(self, preprocessed_sentence):\n",
    "        \"\"\"Given an annotated sentence predict the relationship.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        preprocessed_sentence : str\n",
    "            Sentence with entities being annotated accordingly.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        relation : str\n",
    "            Relation type.\n",
    "        \"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def symbols(self):\n",
    "        \"\"\"Dictionary mapping entity types to their annotation symbols.\n",
    "        \n",
    "        General structure: {'ENTITY_TYPE': ('SYMBOL_LEFT', 'SYMBOL_RIGHT')}\n",
    "        \"\"\"\n",
    "    \n",
    "\n",
    "class ChemProt(REModel):\n",
    "    \"\"\"Pretrained model extracting 13 relations between chemicals and proteins.\"\"\"\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        self.model_ = Predictor.from_path(model_path, predictor_name='text_classifier')\n",
    "        self.labels = [\n",
    "            'INHIBITOR',\n",
    "            'SUBSTRATE',\n",
    "            'INDIRECT-DOWNREGULATOR',\n",
    "            'INDIRECT-UPREGULATOR',\n",
    "            'ACTIVATOR',\n",
    "            'ANTAGONIST',\n",
    "            'PRODUCT-OF',\n",
    "            'AGONIST',\n",
    "            'DOWNREGULATOR',\n",
    "            'UPREGULATOR',\n",
    "            'AGONIST-ACTIVATOR',\n",
    "            'SUBSTRATE_PRODUCT-OF',\n",
    "            'AGONIST-INHIBITOR']\n",
    "    \n",
    "    @property\n",
    "    def symbols(self):\n",
    "        return {'GGP': ('[[ ', ' ]]'),\n",
    "                 'CHEBI': ('<< ', ' >>')}\n",
    "\n",
    "        \n",
    "    def predict(self, processed_sentence):\n",
    "        s = pd.Series(self.model_.predict(sentence=processed_sentence)['class_probs'], index=self.labels)\n",
    "\n",
    "        return s.idxmax()\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlueBrainSearch to BlueBrainGraph: POC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how from raw text we can apply BlueBrainSearch and then BlueBrainGraph tools in order to generate first a list of extracted objects of interest, and then a knowledge graph out of it.\n",
    "\n",
    "It is intended to be just a proof of concept of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlueBrainSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first part of the pipeline starts with the raw text of a scientific paper as an input, and generates a CSV table out of it. The table contains all the extracted entities and relations that were identified in the text.\n",
    "- **input**: raw text\n",
    "- **output**: csv table of extracted entities/relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "chemprot_path = pathlib.Path('/raid/users/krepl/BlueBrainSearch/submodules/scibert/training_results/first_simple/model.tar.gz')\n",
    "\n",
    "# Models\n",
    "NER_PIPELINE = spacy.load(\"en_ner_craft_md\")\n",
    "RE_MODELS = {('CHEBI', 'GGP'): ChemProt(chemprot_path)}\n",
    "\n",
    "# Table headers\n",
    "HEADERS = ['entity',\n",
    "           'entity_type',\n",
    "           'property',\n",
    "           'property_value',\n",
    "           'property_type',\n",
    "           'property_value_type',\n",
    "           'ontology_source',\n",
    "           'paper_id',\n",
    "           'start_pos',\n",
    "           'end_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bbs_widgets = OrderedDict()\n",
    "bbs_widgets['input_text'] = ipywidgets.Textarea(\n",
    "        value=\"Autophagy maintains tumour growth through circulating arginine. Autophagy captures intracellular components and delivers them to lysosomes, where they are degraded and recycled to sustain metabolism and to enable survival during starvation1-5. Acute, whole-body deletion of the essential autophagy gene Atg7 in adult mice causes a systemic metabolic defect that manifests as starvation intolerance and gradual loss of white adipose tissue, liver glycogen and muscle mass1. Cancer cells also benefit from autophagy. Deletion of essential autophagy genes impairs the metabolism, proliferation, survival and malignancy of spontaneous tumours in models of autochthonous cancer6,7. Acute, systemic deletion of Atg7 or acute, systemic expression of a dominant-negative ATG4b in mice induces greater regression of KRAS-driven cancers than does tumour-specific autophagy deletion, which suggests that host autophagy promotes tumour growth1,8. Here we show that host-specific deletion of Atg7 impairs the growth of multiple allografted tumours, although not all tumour lines were sensitive to host autophagy status. Loss of autophagy in the host was associated with a reduction in circulating arginine, and the sensitive tumour cell lines were arginine auxotrophs owing to the lack of expression of the enzyme argininosuccinate synthase 1. Serum proteomic analysis identified the arginine-degrading enzyme arginase I (ARG1) in the circulation of Atg7-deficient hosts, and in vivo arginine metabolic tracing demonstrated that serum arginine was degraded to ornithine. ARG1 is predominantly expressed in the liver and can be released from hepatocytes into the circulation. Liver-specific deletion of Atg7 produced circulating ARG1, and reduced both serum arginine and tumour growth. Deletion of Atg5 in the host similarly regulated [corrected] circulating arginine and suppressed tumorigenesis, which demonstrates that this phenotype is specific to autophagy function rather than to deletion of Atg7. Dietary supplementation of Atg7-deficient hosts with arginine partially restored levels of circulating arginine and tumour growth. Thus, defective autophagy in the host leads to the release of ARG1 from the liver and the degradation of circulating arginine, which is essential for tumour growth; this identifies a metabolic vulnerability of cancer. (PMID:30429607)\",\n",
    "        layout=ipywidgets.Layout(width='80%', height='400px')\n",
    "    )\n",
    "bbs_widgets['submit_button'] = ipywidgets.Button(\n",
    "    description='Extract Entities & Properties!',\n",
    "    layout=ipywidgets.Layout(width='30%')\n",
    ")\n",
    "\n",
    "bbs_widgets['out'] = ipywidgets.Output(layout={'border': '0.5px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_and_relations(b):\n",
    "    bbs_widgets['out'].clear_output()\n",
    "    with bbs_widgets['out']:\n",
    "        doc = NER_PIPELINE(bbs_widgets['input_text'].value)\n",
    "\n",
    "        lines = []\n",
    "        for sent in doc.sents:\n",
    "            detected_entities = [ent for ent in sent.ents]\n",
    "\n",
    "            for s_ent in detected_entities:\n",
    "                # add single lines for entities\n",
    "                lines.append({'entity': s_ent.text, \n",
    "                              'entity_type': s_ent.label_,\n",
    "                              'start_pos': s_ent.start_char,\n",
    "                              'end_pos': s_ent.end_char,\n",
    "                             })\n",
    "\n",
    "                # extract relations\n",
    "                for o_ent in detected_entities:\n",
    "                    so = (s_ent.label_, o_ent.label_)\n",
    "                    if so in RE_MODELS:\n",
    "                        preproceses_sent = preprocess_re(doc, sent, s_ent, o_ent, RE_MODELS[so].symbols)\n",
    "                        property_ = RE_MODELS[so].predict(preproceses_sent)\n",
    "                        lines.append({'entity': s_ent.text, \n",
    "                                      'entity_type': s_ent.label_,\n",
    "                                      'start_pos': s_ent.start_char,\n",
    "                                      'end_pos': s_ent.end_char,\n",
    "                                      'property_type': 'relation',\n",
    "                                      'property': property_,\n",
    "                                      'property_value': o_ent.text,\n",
    "                                      'property_value_type': o_ent.label_\n",
    "                                     })\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(lines, columns=HEADERS)\n",
    "        display(df)\n",
    "\n",
    "bbs_widgets['submit_button'].on_click(extract_entities_and_relations)\n",
    "\n",
    "ordered_widgets = list(bbs_widgets.values())\n",
    "main_widget = ipywidgets.VBox(ordered_widgets)\n",
    "IPython.display.display(main_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BlueBrainGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second part of the pipeline starts with the extracted entities and relations in a CSV table, and generates a knowledge graph out of it.\n",
    "\n",
    "- **input**: csv table of extracted entities/relations\n",
    "- **output**: knowledge graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
