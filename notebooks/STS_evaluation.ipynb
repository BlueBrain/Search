{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 'cord19_v47'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_RUN = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG_RUN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 9173"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pyarrow  # Needed by Pandas for Parquet operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bbsearch.utils import H5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'sentences_{VERSION}.parquet'\n",
    "\n",
    "if FIRST_RUN:\n",
    "    import sqlalchemy\n",
    "    engine = sqlalchemy.create_engine(f'mysql+pymysql://guest:guest@dgx1.bbp.epfl.ch:8853/{VERSION}')\n",
    "    sentences = pd.read_sql(f\"SELECT sentence_id, text FROM sentences\", engine, 'sentence_id')\n",
    "    sentences.to_parquet(filename, index=True)\n",
    "else:\n",
    "    sentences = pd.read_parquet(filename)\n",
    "\n",
    "if DEBUG_RUN:\n",
    "    sentences = sentences.sample(10000)\n",
    "\n",
    "scount = sentences.size\n",
    "print(f'{scount:,} sentences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences.drop_duplicates()\n",
    "\n",
    "dcount = sentences.size\n",
    "print(f'{dcount:,} sentences (- {scount-dcount:,} duplicates)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences.text.str.len().hist(figsize=(8, 6), bins=75, log=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies:\n",
    "- [x] keywords\n",
    "- [ ] annotations\n",
    "- [ ] k-means\n",
    "- [ ] LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All keywords in bold from BBS Ontology v0.3 on 17.09.2020.\n",
    "\n",
    "keywords = {'pathogens', 'cardiac injury', 'cardiovascular disease', 'sars',\n",
    "            'acute respiratory distress syndrome', 'gas exchange', 'inflammation',\n",
    "            'sars-cov-2 infection', 'viral entry', 'glucose metabolism', 'golgi', 'human',\n",
    "            'dry cough', 'mammals', 'cardiovascular injury', 'glycation', 'endoplasmic reticulum',\n",
    "            'carbohydrates', 'innate immunity', 'igt', 'polysaccharide', 'hypertension',\n",
    "            'thrombotic events', 'neutrophils', 'dc cells', 'obesity', 'congested cough',\n",
    "            'influenzavirus', 'viral replication', 'septic shock', 'macrophages', 'cvd', 'lactate',\n",
    "            'myalgia', 'chest pain', 'oxygen', 'mucociliary clearance', 'high blood sugar level',\n",
    "            'respiratory failure', 'fever', 'systemic disorder', 'flu', 'influenzae',\n",
    "            'hyperglycemia', 'impaired glucose tolerance', 'iron',\n",
    "            'severe acute respiratory syndrome', 'immunity', 'host defense',\n",
    "            'respiratory viral infection', 'multi-organs failure', 'blood clot',\n",
    "            'viral infection', 'hypoxia', 'glucose homeostasis', 'vasoconstriction', 'covid-19',\n",
    "            'sars-cov-2', 'fatigue', 'multiple organ failure', 'productive cough',\n",
    "            'adaptive immunity', 'atp', 'bacteria', 'nk cells', 'coagulation', 'ards', 'diarrhea',\n",
    "            'cytokine storm', 'dendritic cells', 'pneumonia', 'thrombosis', 'phagocytosis',\n",
    "            'alveolar macrophages', 'glucose', 'clearance', 'epithelial cells', 'glucose uptake',\n",
    "            'coronavirus', 'plasma membrane', 'lymphocytes', 'oxidative stress', 'glycans',\n",
    "            'glycolysis', 'pulmonary embolism', 'glycosylation', 'viruses',\n",
    "            'viral respiratory tract infection', 'diabetes', 'life-cycle', 'mammalia',\n",
    "            'antimicrobials activity', 'ketones', 'immune system', 'pathogen'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ok(text: str) -> pd.Series:\n",
    "    conditions = (\n",
    "        100 >= len(text) <= 300,\n",
    "        re.match('^[A-Z][a-z]+ .*', text),\n",
    "        # TODO Improve matching.\n",
    "        not {x.lower() for x in text.split()}.isdisjoint(keywords),\n",
    "        # TODO Keep only English.\n",
    "    )\n",
    "    return all(conditions)\n",
    "\n",
    "filtered = sentences[sentences.text.map(lambda x: ok(x))].copy()\n",
    "\n",
    "fcount = filtered.size\n",
    "print(f'{fcount:,} sentences ({scount-fcount:,} not selected)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered['mapping'] = np.arange(fcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "\n",
    "sampled = filtered.sample(n, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model: str, version: str) -> torch.Tensor:\n",
    "    # 'model' in ['Sent2Vec', 'BSV']\n",
    "    path = Path(f'/raid/sync/proj115/bbs_data/{version}/embeddings/embeddings.h5')\n",
    "    # TODO Load only filtered indices.\n",
    "    # NB H5.load(...) changes the ordering if given indices.\n",
    "    embeddings = H5.load(path, model, 10000)\n",
    "    tensor = torch.from_numpy(embeddings)\n",
    "    norm = torch.norm(tensor, dim=1, keepdim=True)\n",
    "    norm[norm == 0] = 1\n",
    "    tensor /= norm\n",
    "    return tensor\n",
    "\n",
    "mapping = filtered.index.values - 1\n",
    "embeddings = load_embeddings('Sent2Vec', VERSION)[mapping]\n",
    "\n",
    "ecount = embeddings.size()[0]\n",
    "print(f'{ecount == fcount} (- {fcount-ecount:,})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strategies:\n",
    "- [ ] random\n",
    "- [x] most similar\n",
    "- [ ] quartiles\n",
    "- [ ] power law"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_sci_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(index: int, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "    embedding = embeddings[index]\n",
    "    norm = torch.norm(embedding).item()\n",
    "    norm = 1 if norm == 0 else norm\n",
    "    embedding /= norm\n",
    "    return torch.nn.functional.linear(embedding, embeddings)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for x in tqdm(sampled.itertuples(), total=n):\n",
    "    similarities = compute_similarities(x.mapping, embeddings)\n",
    "    sims, idxs = similarities.sort(descending=True)\n",
    "    \n",
    "    # TODO Add other strategies.\n",
    "    sim, idx = sims[1:][0].item(), idxs[1:][0].item()\n",
    "    \n",
    "    i0, s0 = x.Index, x.text\n",
    "    row = filtered.loc[filtered.mapping == idx]\n",
    "    i1, s1 = row.index.item(), row.text.item()\n",
    "    \n",
    "    doc0, doc1 = nlp(s0), nlp(s1)\n",
    "    set0, set1 = {x.lemma_ for x in doc0 if x.is_alpha}, {x.lemma_ for x in doc1 if x.is_alpha}\n",
    "    dissimilarity = min(len(set0 - set1) / len(set0), len(set1 - set0) / len(set1))\n",
    "    \n",
    "    rows.append((i0, s0, i1, s1, sim, 1 - dissimilarity))\n",
    "    \n",
    "cols = ['sentence_id_1', 'sentence_text_1', 'sentence_id_2', 'sentence_text_2',\n",
    "        'vectors_similarity', 'words_similarity']\n",
    "\n",
    "pairs = pd.DataFrame(rows, columns=cols).sort_values('vectors_similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_results(pairs: pd.DataFrame) -> str:\n",
    "    def _(i, x):\n",
    "        return (\n",
    "            f'pair: {i}  id_1: {x.sentence_id_1}  id_2: {x.sentence_id_2}  '\n",
    "            f'vectors_sim: {x.vectors_similarity:.3f}  words_sim: {x.words_similarity:.3f}\\n'\n",
    "            f'-\\n'\n",
    "            f'{x.sentence_text_1.strip()}\\n'\n",
    "            f'-\\n'\n",
    "            f'{x.sentence_text_2.strip()}\\n'\n",
    "        )\n",
    "    formatted = (_(i, x) for i, x in enumerate(pairs.itertuples()))\n",
    "    return '\\n\\n'.join(formatted)\n",
    "\n",
    "print(format_results(pairs[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_txt(pairs: pd.DataFrame, n: int, directory: str) -> None:\n",
    "    time =  datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "    filename = f'pairs_n{n}_{time}.txt'\n",
    "    path = Path(directory, filename)\n",
    "    content = format_results(pairs)\n",
    "    path.write_text(content, encoding='utf-8')\n",
    "    print(f'<file> {filename}')\n",
    "\n",
    "write_results_txt(pairs, n, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
