{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Blue Brain Search is a text mining toolbox focused on scientific use cases.\n",
    "\n",
    "Copyright (C) 2020  Blue Brain Project, EPFL.\n",
    "\n",
    "This program is free software: you can redistribute it and/or modify\n",
    "it under the terms of the GNU Lesser General Public License as published by\n",
    "the Free Software Foundation, either version 3 of the License, or\n",
    "(at your option) any later version.\n",
    "\n",
    "This program is distributed in the hope that it will be useful,\n",
    "but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "GNU Lesser General Public License for more details.\n",
    "\n",
    "You should have received a copy of the GNU Lesser General Public License\n",
    "along with this program. If not, see <https://www.gnu.org/licenses/>.\n",
    "-->\n",
    "\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "We would want to improve the way our models find hypotheses in CORD-19 papers.\n",
    "We need to build a benchmark dataset of sentence pairs adapted to CORD-19 and the task.\n",
    "This benchmark dataset will help to build models which better capture the semantic similarity.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "A notebook helping to investigate and build such a dataset of sentence pairs.\n",
    "\n",
    "This notebook follow these major steps:\n",
    "1. Select sentences with some keywords.\n",
    "2. Sample randomly a subset of N of them.\n",
    "3. Pair the subset with the most similar ones.\n",
    "4. Compute a word-based similarity for each pair.\n",
    "5. Print & Export pairs in a human-readable format.\n",
    "\n",
    "**Getting Started**\n",
    "\n",
    "This notebook requires that:\n",
    "\n",
    "- a database of sentences has been created.\n",
    "- sentence embeddings have been computed.\n",
    "\n",
    "The logic in the notebook is agnostic of the dataset and the model.\n",
    "Any dataset of sentences and any sentence embedding model could be used.\n",
    "\n",
    "However, for demonstration purposes, we reuse below the dataset and the model from the `README`.\n",
    "This means that we reuse the values of:\n",
    "\n",
    "- `DATABASE_URL` from [Create the database](https://github.com/BlueBrain/Search#initialize-the-database-server),\n",
    "- `EMBEDDING_MODEL` and `BBS_SEARCH_EMBEDDINGS_PATH` from [Compute the sentence embeddings](https://github.com/BlueBrain/Search#compute-the-sentence-embeddings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Number of sentence pairs to sample.\n",
    "N = 100\n",
    "\n",
    "# Maximum number of sentences to consider.\n",
    "# This could be used for experimenting or debugging.\n",
    "# If None, all sentences are loaded.\n",
    "LIMIT = None\n",
    "\n",
    "# File path to a dump of all the sentences.\n",
    "# This allows a faster loading of the sentences.\n",
    "# If None, sentences are loaded from the DATABASE (see below).\n",
    "# Example: 'sentences.parquet'.\n",
    "DUMP = None\n",
    "\n",
    "# Sentence embedding model to use for guiding pairing.\n",
    "# A key in the EMBEDDINGS file (see below).\n",
    "# Example: 'Sent2Vec'.\n",
    "MODEL = os.getenv('EMBEDDING_MODEL')\n",
    "print(f\"MODEL='{MODEL}'\")\n",
    "\n",
    "# Seed for reproducibility of the random sampling.\n",
    "SEED = 9173\n",
    "\n",
    "# SQLAlchemy database URL.\n",
    "# Example: '<dialect>+<driver>://<username>:<password>@<host>:<port>/<database>'.\n",
    "DATABASE = f\"mysql+pymysql://guest:guest@{os.getenv('DATABASE_URL')}\"\n",
    "print(f\"DATABASE='{DATABASE}'\")\n",
    "\n",
    "# Path to the pre-computed sentence embeddings.\n",
    "# They must be indexed on the sentence_id from the DATABASE (see above).\n",
    "# Example: './embeddings.h5'.\n",
    "EMBEDDINGS = os.getenv('BBS_SEARCH_EMBEDDINGS_PATH')\n",
    "print(f\"EMBEDDINGS='{EMBEDDINGS}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from bluesearch.utils import H5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DUMP:\n",
    "    print('<loading> from dump')\n",
    "    sentences = pd.read_parquet(DUMP)\n",
    "    if LIMIT:\n",
    "        sentences = sentences[:LIMIT]\n",
    "else:\n",
    "    print('<loading> from database')\n",
    "    engine = sqlalchemy.create_engine(DATABASE)\n",
    "    statement = 'SELECT sentence_id, text FROM sentences'\n",
    "    if LIMIT:\n",
    "        statement += f' LIMIT {LIMIT}'\n",
    "    sentences = pd.read_sql(statement, engine, 'sentence_id')\n",
    "\n",
    "scount = sentences.size\n",
    "print(f'{scount:,} sentences')\n",
    "\n",
    "# Takes for 20.5 millions sentences:\n",
    "#  - Parquet: 25 secs\n",
    "#  - MySQL: 5 mins\n",
    "# 20,510,932 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dump of the sentences.\n",
    "# sentences.to_parquet('sentences.parquet', index=True)\n",
    "\n",
    "# Takes 20 secs for 20.5 millions sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences.drop_duplicates()\n",
    "\n",
    "dcount = sentences.size\n",
    "print(f'{dcount:,} sentences (- {scount-dcount:,} duplicates)')\n",
    "\n",
    "# Takes 30 secs for 20.5 millions sentences.\n",
    "# 19,131,302 sentences (- 1,379,630 duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All keywords in bold from BBS Ontology v0.3 on 17.09.2020.\n",
    "# keywords = {\n",
    "#     'pathogens', 'cardiac injury', 'cardiovascular disease', 'sars',\n",
    "#     'acute respiratory distress syndrome', 'gas exchange', 'inflammation',\n",
    "#     'sars-cov-2 infection', 'viral entry', 'glucose metabolism', 'golgi', 'human',\n",
    "#     'dry cough', 'mammals', 'cardiovascular injury', 'glycation', 'endoplasmic reticulum',\n",
    "#     'carbohydrates', 'innate immunity', 'igt', 'polysaccharide', 'hypertension',\n",
    "#     'thrombotic events', 'neutrophils', 'dc cells', 'obesity', 'congested cough',\n",
    "#     'influenzavirus', 'viral replication', 'septic shock', 'macrophages', 'cvd', 'lactate',\n",
    "#     'myalgia', 'chest pain', 'oxygen', 'mucociliary clearance', 'high blood sugar level',\n",
    "#     'respiratory failure', 'fever', 'systemic disorder', 'flu', 'influenzae',\n",
    "#     'hyperglycemia', 'impaired glucose tolerance', 'iron',\n",
    "#     'severe acute respiratory syndrome', 'immunity', 'host defense',\n",
    "#     'respiratory viral infection', 'multi-organs failure', 'blood clot',\n",
    "#     'viral infection', 'hypoxia', 'glucose homeostasis', 'vasoconstriction', 'covid-19',\n",
    "#     'sars-cov-2', 'fatigue', 'multiple organ failure', 'productive cough',\n",
    "#     'adaptive immunity', 'atp', 'bacteria', 'nk cells', 'coagulation', 'ards', 'diarrhea',\n",
    "#     'cytokine storm', 'dendritic cells', 'pneumonia', 'thrombosis', 'phagocytosis',\n",
    "#     'alveolar macrophages', 'glucose', 'clearance', 'epithelial cells', 'glucose uptake',\n",
    "#     'coronavirus', 'plasma membrane', 'lymphocytes', 'oxidative stress', 'glycans',\n",
    "#     'glycolysis', 'pulmonary embolism', 'glycosylation', 'viruses',\n",
    "#     'viral respiratory tract infection', 'diabetes', 'life-cycle', 'mammalia',\n",
    "#     'antimicrobials activity', 'ketones', 'immune system', 'pathogen'\n",
    "# }\n",
    "\n",
    "# Pierre-Alexandre's keywords for glucose AND (covid-19 OR sars-cov-2).\n",
    "keywords = {\n",
    "    # COVID-19\n",
    "    'covid-19', 'covid', 'cytokine', 'cytokines', 'hypercytokinemia',\n",
    "    # SARS-CoV-2\n",
    "    'sars-cov-2', '2019-ncov', 'hcov-19', 'coronavirus',\n",
    "    # Glucose\n",
    "    'glucose', 'd-glucose', 'l-glucose', 'sugar', 'sugars',\n",
    "    'carbohydrate', 'carbohydrates', 'monosaccharide', 'monosaccharides',\n",
    "    'polysaccharide', 'polysaccharides', 'glycan', 'glycans', 'glucan', 'glucans', 'glycogen',\n",
    "    'glycation', 'glycogenolysis', 'glycosylation', 'glycolysis', 'glycosidic',\n",
    "    'hyperglycemia', 'diabetes', 'diabetic', 'diabetics', 'insulin', 'obesity', 'obese',\n",
    "}\n",
    "\n",
    "def ok(text):\n",
    "    \"\"\"Check if a sentence should be kept according to its content.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        The sentence content.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the sentence should be kept. False otherwise.\n",
    "    \"\"\"\n",
    "    conditions = (\n",
    "        # Keep sentences of length between 100 to 300 characters.\n",
    "        # These sentences are long enough to be meaningful.\n",
    "        # They are short enough for humans to evaluate semantic similarity.\n",
    "        100 <= len(text) <= 300,\n",
    "        # Keep sentences starting with a capitalized word.\n",
    "        # Sentences which don't are incorrect sentences (extraction issue, tokenization error, ...).\n",
    "        re.match('[A-Z][a-z]+ ', text),\n",
    "        # Keep sentences which contains some keywords.\n",
    "        # Sentences which do are more interesting for training / evaluating a model for a domain.\n",
    "        not {x.lower() for x in text.split()}.isdisjoint(keywords),\n",
    "    )\n",
    "    return all(conditions)\n",
    "\n",
    "filtered = sentences[sentences.text.map(ok)].copy()\n",
    "\n",
    "fcount = filtered.size\n",
    "print(f'{fcount:,} sentences ({scount-fcount:,} not selected)')\n",
    "\n",
    "# Takes 2 mins 45 for 20.5 millions sentences and 40 or 100 keywords.\n",
    "# 631,854 sentences (19,879,078 not selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between the sentence ID and the index in the embeddings.\n",
    "# The embeddings are indexed on the sentence ID.\n",
    "# This is no more the case when loading a subset of the embeddings.\n",
    "# The loaded subset is indexed from 0 to fcount - 1.\n",
    "filtered['mapping'] = np.arange(fcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = filtered.sample(N, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model, sentence_ids):\n",
    "    \"\"\"Load pre-computed embeddings of sentences for a given model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : str\n",
    "        The sentence embedding model.\n",
    "    sentence_ids : np.ndarray\n",
    "        The identifiers of the sentences.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor\n",
    "        The pre-computed embeddings of the specified sentences.\n",
    "    \"\"\"\n",
    "    path = Path(EMBEDDINGS)\n",
    "    embeddings = H5.load(path, model, indices=sentence_ids)\n",
    "    tensor = torch.from_numpy(embeddings)\n",
    "    norm = torch.norm(tensor, dim=1, keepdim=True)\n",
    "    norm[norm == 0] = 1\n",
    "    tensor /= norm\n",
    "    return tensor\n",
    "\n",
    "embeddings = load_embeddings(MODEL, filtered.index.values)\n",
    "\n",
    "ecount = embeddings.size()[0]\n",
    "print(f'{ecount:,} embeddings (same as selected sentences? {ecount == fcount})')\n",
    "\n",
    "# Takes 25 secs for 630 thousands embeddings to select from 20.5 millions in total.\n",
    "# 631,854 embeddings (same as selected sentences? True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_sci_lg')\n",
    "\n",
    "# Takes 15 secs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pair(mapping, embeddings, sentences):\n",
    "    \"\"\"Match a sentence with a given one in a meaningful way.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping : int\n",
    "        The index in `sentences` of the sentence to match with another one.\n",
    "    embeddings : torch.Tensor\n",
    "        The pre-computed embeddings of the sentences in `sentences`.\n",
    "    sentences : pd.DataFrame\n",
    "        The sentences in which to match the sentence specified by `mapping`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    sentence_id : int\n",
    "        The identifier of the matched sentence.\n",
    "    sentence_text : str\n",
    "        The content of the matched sentence.\n",
    "    vectors_similarity : float\n",
    "        The cosine similarity between the specified sentence and the matched one.\n",
    "    \"\"\"\n",
    "    embedding = embeddings[mapping]\n",
    "    similarities = torch.nn.functional.linear(embedding, embeddings)\n",
    "    # The top element is the sampled sentence.\n",
    "    stop, itop = similarities.topk(2)\n",
    "    # The most similar sentence is then the second top element.\n",
    "    sim, idx = stop[1].item(), itop[1].item()\n",
    "    # Retrieve paired sentence ID and content.\n",
    "    row = sentences.loc[sentences.mapping == idx]\n",
    "    return row.index.item(), row.text.item(), sim\n",
    "\n",
    "def words_similarity(text1, text2, nlp):\n",
    "    \"\"\"Compute a word-based similarity between sentences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text1 : str\n",
    "        The content of the first sentence.\n",
    "    text2 : str\n",
    "        The content of the second sentence.\n",
    "    nlp : spacy.lang.en.English\n",
    "        The spaCy model to use for tokenization and lemmatization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float:\n",
    "        The word-based similarity between the two sentences.\n",
    "    \"\"\"\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    set1 = {x.lemma_ for x in doc1 if not x.is_punct}\n",
    "    set2 = {x.lemma_ for x in doc2 if not x.is_punct}\n",
    "    dissimilarity12 = len(set1 - set2) / len(set1)\n",
    "    dissimilarity21 = len(set2 - set1) / len(set2)\n",
    "    return 1 - min(dissimilarity12, dissimilarity21)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for x in tqdm(sampled.itertuples(), total=N):\n",
    "    sid1, stext1 = x.Index, x.text\n",
    "\n",
    "    # Pair the sentence with the most similar sentence.\n",
    "    # The most similar sentence is the sentence with the highest cosine similarity.\n",
    "    sid2, stext2, vsimilarity = pair(x.mapping, embeddings, filtered)\n",
    "    \n",
    "    # Compute a word-based similarity for each pair.\n",
    "    # When 1, the two sentences use exactly the same wording.\n",
    "    # When 0, the two sentences use a completely different wording.\n",
    "    wsimilarity = words_similarity(stext1, stext2, nlp)\n",
    "    \n",
    "    rows.append((sid1, sid2, stext1, stext2, vsimilarity, wsimilarity))\n",
    "\n",
    "cols = ['sentence_id_1', 'sentence_id_2', 'sentence_text_1', 'sentence_text_2',\n",
    "        'vectors_similarity', 'words_similarity']\n",
    "pairs = pd.DataFrame(rows, columns=cols).sort_values('vectors_similarity', ascending=False)\n",
    "\n",
    "# Takes 10 secs for 100 pairs amongst 630 thousands selected embeddings / sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_results(pairs):\n",
    "    \"\"\"Format sentence pairs in a human-readable format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : pd.DataFrame\n",
    "        The sentence pairs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The sentences pairs formatted in a human-readable format.\n",
    "    \"\"\"\n",
    "    def _(i, x):\n",
    "        return (\n",
    "            f'pair: {i}  id_1: {x.sentence_id_1}  id_2: {x.sentence_id_2}  '\n",
    "            f'vectors_sim: {x.vectors_similarity:.2f}  words_sim: {x.words_similarity:.2f}\\n'\n",
    "            f'-\\n'\n",
    "            f'{x.sentence_text_1.strip()}\\n'\n",
    "            f'-\\n'\n",
    "            f'{x.sentence_text_2.strip()}\\n'\n",
    "        )\n",
    "    return '\\n\\n'.join(_(i, x) for i, x in enumerate(pairs.itertuples()))\n",
    "\n",
    "print(format_results(pairs[:3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_txt(pairs, n, directory):\n",
    "    \"\"\"Write sentence pairs to disk in a human-readable format.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    pairs : pd.DataFrame\n",
    "        The sentence pairs.\n",
    "    n : int\n",
    "        The number of sentence pairs.\n",
    "    directory : str\n",
    "        The directory where to write the sentence pairs.\n",
    "    \"\"\"\n",
    "    time =  datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "    filename = f'pairs_n{n}_{time}.txt'\n",
    "    path = Path(directory, filename)\n",
    "    content = format_results(pairs)\n",
    "    # UTF-8 is necessary as non ASCII characters are present.\n",
    "    path.write_text(content, encoding='utf-8')\n",
    "    print(f'<wrote> {filename}')\n",
    "\n",
    "write_results_txt(pairs, N, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_exp_1 = pairs[(pairs.words_similarity <= 0.9) & (pairs.vectors_similarity <= 0.9)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exp_1 = len(pairs_exp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results_txt(pairs_exp_1, n_exp_1, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
