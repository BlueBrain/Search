{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Context**\n",
    "\n",
    "We would want to improve the way our models find hypotheses in CORD-19 papers.\n",
    "We need to build a benchmark dataset of sentence pairs adapted to CORD-19 and the task.\n",
    "This benchmark dataset will help to build models which better capture the semantic similarity.\n",
    "\n",
    "**Solution**\n",
    "\n",
    "A notebook helping to investigate and build such a dataset of sentence pairs.\n",
    "\n",
    "This notebook follow these major steps:\n",
    "1. Select sentences with some keywords.\n",
    "2. Sample randomly a subset of N of them.\n",
    "3. Pair the subset with the most similar ones.\n",
    "4. Compute a word-based similarity for each pair.\n",
    "5. Print & Export pairs in a human-readable format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of sentence pairs to sample.\n",
    "N = 20\n",
    "\n",
    "# Maximum number of sentences to consider.\n",
    "# This could be used for experimenting or debugging.\n",
    "# If None, all sentences are loaded.\n",
    "LIMIT = 10000\n",
    "\n",
    "# File path to a dump of all the sentences.\n",
    "# This allows a faster loading of the sentences.\n",
    "# pyarrow should be installed (pip install pyarrow).\n",
    "# If None, sentences are loaded from the DATABASE (see below).\n",
    "DUMP = 'sentences-cord19_v47-2020-10-01.parquet'\n",
    "\n",
    "# Sentence embedding model to use for guiding pairing.\n",
    "# A key in the EMBEDDINGS file (see below), e.g. 'Sent2Vec', BSV'.\n",
    "MODEL = 'Sent2Vec'\n",
    "\n",
    "# Seed for reproducibility of the random sampling.\n",
    "SEED = 9173\n",
    "\n",
    "# SQLAlchemy database URL.\n",
    "DATABASE = 'mysql+pymysql://guest:guest@dgx1.bbp.epfl.ch:8853/cord19_v47'\n",
    "\n",
    "# Path to the pre-computed sentence embeddings.\n",
    "# They must be indexed on the sentence_id from the DATABASE (see above).\n",
    "EMBEDDINGS = '/raid/sync/proj115/bbs_data/cord19_v47/embeddings/embeddings.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import sqlalchemy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from bbsearch.utils import H5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DUMP:\n",
    "    # Takes 25 secs.\n",
    "    print('<loading> from dump')\n",
    "    sentences = pd.read_parquet(DUMP)\n",
    "    if LIMIT:\n",
    "        sentences = sentences[:LIMIT]\n",
    "else:\n",
    "    # Takes 5 mins for 20.5 millions sentences.\n",
    "    print('<loading> from database')\n",
    "    engine = sqlalchemy.create_engine(DATABASE)\n",
    "    statement = 'SELECT sentence_id, text FROM sentences'\n",
    "    if LIMIT:\n",
    "        statement += f' LIMIT {LIMIT}'\n",
    "    sentences = pd.read_sql(statement, engine, 'sentence_id')\n",
    "\n",
    "scount = sentences.size\n",
    "print(f'{scount:,} sentences')\n",
    "# 20,510,932 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dump of the sentences.\n",
    "# Takes 20 secs for 20.5 millions sentences.\n",
    "# sentences.to_parquet('sentences-cord19_v47-2020-10-01.parquet', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deduplicate sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 26 secs for 20.5 millions sentences.\n",
    "sentences = sentences.drop_duplicates()\n",
    "\n",
    "dcount = sentences.size\n",
    "print(f'{dcount:,} sentences (- {scount-dcount:,} duplicates)')\n",
    "# 19,131,302 sentences (- 1,379,630 duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# All keywords in bold from BBS Ontology v0.3 on 17.09.2020.\n",
    "keywords = {'pathogens', 'cardiac injury', 'cardiovascular disease', 'sars',\n",
    "            'acute respiratory distress syndrome', 'gas exchange', 'inflammation',\n",
    "            'sars-cov-2 infection', 'viral entry', 'glucose metabolism', 'golgi', 'human',\n",
    "            'dry cough', 'mammals', 'cardiovascular injury', 'glycation', 'endoplasmic reticulum',\n",
    "            'carbohydrates', 'innate immunity', 'igt', 'polysaccharide', 'hypertension',\n",
    "            'thrombotic events', 'neutrophils', 'dc cells', 'obesity', 'congested cough',\n",
    "            'influenzavirus', 'viral replication', 'septic shock', 'macrophages', 'cvd', 'lactate',\n",
    "            'myalgia', 'chest pain', 'oxygen', 'mucociliary clearance', 'high blood sugar level',\n",
    "            'respiratory failure', 'fever', 'systemic disorder', 'flu', 'influenzae',\n",
    "            'hyperglycemia', 'impaired glucose tolerance', 'iron',\n",
    "            'severe acute respiratory syndrome', 'immunity', 'host defense',\n",
    "            'respiratory viral infection', 'multi-organs failure', 'blood clot',\n",
    "            'viral infection', 'hypoxia', 'glucose homeostasis', 'vasoconstriction', 'covid-19',\n",
    "            'sars-cov-2', 'fatigue', 'multiple organ failure', 'productive cough',\n",
    "            'adaptive immunity', 'atp', 'bacteria', 'nk cells', 'coagulation', 'ards', 'diarrhea',\n",
    "            'cytokine storm', 'dendritic cells', 'pneumonia', 'thrombosis', 'phagocytosis',\n",
    "            'alveolar macrophages', 'glucose', 'clearance', 'epithelial cells', 'glucose uptake',\n",
    "            'coronavirus', 'plasma membrane', 'lymphocytes', 'oxidative stress', 'glycans',\n",
    "            'glycolysis', 'pulmonary embolism', 'glycosylation', 'viruses',\n",
    "            'viral respiratory tract infection', 'diabetes', 'life-cycle', 'mammalia',\n",
    "            'antimicrobials activity', 'ketones', 'immune system', 'pathogen'}\n",
    "\n",
    "def ok(text: str) -> bool:\n",
    "    conditions = (\n",
    "        # Keep sentences of length between 100 to 300 characters.\n",
    "        # These sentences are long enough to be meaningful.\n",
    "        # They are short enough for humans to evaluate semantic similarity.\n",
    "        100 <= len(text) <= 300,\n",
    "        # Keep sentences starting with a capitalized word.\n",
    "        # Sentences which don't are incorrect sentences (extraction issue, tokenization error, ...).\n",
    "        re.match('[A-Z][a-z]+ ', text),\n",
    "        # Keep sentences which contains some keywords.\n",
    "        # Sentences which do are more interesting for training / evaluating a model for a domain.\n",
    "        not {x.lower() for x in text.split()}.isdisjoint(keywords),\n",
    "    )\n",
    "    return all(conditions)\n",
    "\n",
    "# Takes 2 mins 40 for 20.5 millions sentences and 100 keywords.\n",
    "filtered = sentences[sentences.text.map(ok)].copy()\n",
    "\n",
    "fcount = filtered.size\n",
    "print(f'{fcount:,} sentences ({scount-fcount:,} not selected)')\n",
    "# 1,264,496 sentences (19,246,436 not selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping between the sentence ID and the index in the embeddings.\n",
    "# The embeddings are indexed on the sentence ID.\n",
    "# This is no more the case when loading a subset of the embeddings.\n",
    "# The loaded subset is indexed from 0 to fcount - 1.\n",
    "filtered['mapping'] = np.arange(fcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled = filtered.sample(N, random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(model: str, sentence_ids: np.ndarray) -> torch.Tensor:\n",
    "    path = Path(EMBEDDINGS)\n",
    "    embeddings = H5.load(path, model, batch_size=10000, indices=sentence_ids)\n",
    "    tensor = torch.from_numpy(embeddings)\n",
    "    norm = torch.norm(tensor, dim=1, keepdim=True)\n",
    "    norm[norm == 0] = 1\n",
    "    tensor /= norm\n",
    "    return tensor\n",
    "\n",
    "# Takes 5 mins 30 for 20.5 millions embeddings and 225 thousands selected sentences.\n",
    "embeddings = load_embeddings(MODEL, filtered.index.values)\n",
    "\n",
    "ecount = embeddings.size()[0]\n",
    "print(f'{ecount:,} embeddings (same as selected sentences? {ecount == fcount})')\n",
    "# 224,343 embeddings (same as selected sentences? True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pair sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes 25 secs.\n",
    "nlp = spacy.load('en_core_sci_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def pair(mapping: int, embeddings: torch.Tensor, sentences: pd.DataFrame) -> Tuple[int, str, float]:\n",
    "    embedding = embeddings[mapping]\n",
    "    similarities = torch.nn.functional.linear(embedding, embeddings)\n",
    "    # The top element is the sampled sentence.\n",
    "    stop, itop = similarities.topk(2)\n",
    "    # The most similar sentence is then the second top element.\n",
    "    sim, idx = stop[1].item(), itop[1].item()\n",
    "    # Retrieve paired sentence ID and content.\n",
    "    row = sentences.loc[sentences.mapping == idx]\n",
    "    return row.index.item(), row.text.item(), sim\n",
    "\n",
    "def words_similarity(text1: str, text2: str, nlp: spacy.lang.en.English) -> float:\n",
    "    doc1 = nlp(text1)\n",
    "    doc2 = nlp(text2)\n",
    "    set1 = {x.lemma_ for x in doc1 if not x.is_punct}\n",
    "    set2 = {x.lemma_ for x in doc2 if not x.is_punct}\n",
    "    dissimilarity12 = len(set1 - set2) / len(set1)\n",
    "    dissimilarity21 = len(set2 - set1) / len(set2)\n",
    "    return 1 - min(dissimilarity12, dissimilarity21)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for x in tqdm(sampled.itertuples(), total=N):\n",
    "    sid1, stext1 = x.Index, x.text\n",
    "\n",
    "    # Pair the sentence with the most similar sentence.\n",
    "    # The most similar sentence is the sentence with the highest cosine similarity.\n",
    "    sid2, stext2, vsimilarity = pair(x.mapping, embeddings, filtered)\n",
    "    \n",
    "    # Compute a word-based similarity for each pair.\n",
    "    # When 1, the two sentences use exactly the same wording.\n",
    "    # When 0, the two sentences use a completely different wording.\n",
    "    wsimilarity = words_similarity(stext1, stext2, nlp)\n",
    "    \n",
    "    rows.append((sid1, sid2, stext1, stext2, vsimilarity, wsimilarity))\n",
    "\n",
    "cols = ['sentence_id_1', 'sentence_id_2', 'sentence_text_1', 'sentence_text_2',\n",
    "        'vectors_similarity', 'words_similarity']\n",
    "pairs = pd.DataFrame(rows, columns=cols).sort_values('vectors_similarity', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_results(pairs: pd.DataFrame) -> str:\n",
    "    def _(i, x):\n",
    "        return (\n",
    "            f'pair: {i}  id_1: {x.sentence_id_1}  id_2: {x.sentence_id_2}  '\n",
    "            f'vectors_sim: {x.vectors_similarity:.2f}  words_sim: {x.words_similarity:.2f}\\n'\n",
    "            f'-\\n'\n",
    "            f'{x.sentence_text_1.strip()}\\n'\n",
    "            f'-\\n'\n",
    "            f'{x.sentence_text_2.strip()}\\n'\n",
    "        )\n",
    "    return '\\n\\n'.join(_(i, x) for i, x in enumerate(pairs.itertuples()))\n",
    "\n",
    "print(format_results(pairs[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export sentence pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_txt(pairs: pd.DataFrame, n: int, directory: str) -> None:\n",
    "    time =  datetime.now().strftime(\"%Y-%m-%d_%Hh%M\")\n",
    "    filename = f'pairs_n{n}_{time}.txt'\n",
    "    path = Path(directory, filename)\n",
    "    content = format_results(pairs)\n",
    "    # UTF-8 is necessary as non ASCII characters are present.\n",
    "    path.write_text(content, encoding='utf-8')\n",
    "    print(f'<wrote> {filename}')\n",
    "\n",
    "write_results_txt(pairs, N, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
