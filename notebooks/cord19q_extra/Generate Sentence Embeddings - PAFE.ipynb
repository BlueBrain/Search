{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use docker image `pafe`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:09.857152Z",
     "start_time": "2020-04-14T15:34:05.749788Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import sqlite3\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "\n",
    "import sent2vec\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:09.869745Z",
     "start_time": "2020-04-14T15:34:09.858908Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path(\"/raid/covid_data/data/2020-04-08\")\n",
    "\n",
    "cord_path = data_path / \"CORD-19-research-challenge\"\n",
    "databases_path = data_path / \"databases\"\n",
    "embeddings_path = data_path / \"embeddings\"\n",
    "assets_path = Path(\"/raid/covid_data/assets\")\n",
    "\n",
    "assert data_path.exists()\n",
    "assert cord_path.exists()\n",
    "assert databases_path.exists()\n",
    "assert embeddings_path.exists()\n",
    "assert assets_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:09.969413Z",
     "start_time": "2020-04-14T15:34:09.967199Z"
    }
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:26.573030Z",
     "start_time": "2020-04-14T15:34:10.334863Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load universal sentence encoder\n",
    "univsentenc_version = 5\n",
    "univsentenc_embedder = hub.load(f\"https://tfhub.dev/google/universal-sentence-encoder-large/{univsentenc_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:30.111019Z",
     "start_time": "2020-04-14T15:34:26.575051Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load SBERT\n",
    "sbert_embedder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:43.977026Z",
     "start_time": "2020-04-14T15:34:30.114604Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load BioSentVec\n",
    "bsv_embedder = sent2vec.Sent2vecModel()\n",
    "bsv_embedder.load_model(str(assets_path / 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'))\n",
    "\n",
    "bsv_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def bsv_preprocess(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "    tokens = [token for token in word_tokenize(text)\n",
    "              if token not in punctuation and token not in bsv_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.015918Z",
     "start_time": "2020-04-14T15:34:43.979142Z"
    }
   },
   "outputs": [],
   "source": [
    "synonyms_dict = dict()\n",
    "with open(assets_path / 'synonyms_list.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    for l in [l_.strip().lower() for l_ in f]:\n",
    "        if l:\n",
    "            w = [l_.strip() for l_ in l.split('=')]\n",
    "            synonyms_dict[w[0]] = w[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.056757Z",
     "start_time": "2020-04-14T15:34:44.017922Z"
    }
   },
   "outputs": [],
   "source": [
    "del synonyms_dict['sars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.100566Z",
     "start_time": "2020-04-14T15:34:44.058503Z"
    }
   },
   "outputs": [],
   "source": [
    "synonyms_index = {x.lower(): k.lower() for k,v in synonyms_dict.items() for x in v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.142415Z",
     "start_time": "2020-04-14T15:34:44.102372Z"
    }
   },
   "outputs": [],
   "source": [
    "db_filename = str(databases_path / 'articles.sqlite')\n",
    "db = sqlite3.connect(db_filename)\n",
    "curs = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporary: BioBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.184770Z",
     "start_time": "2020-04-14T15:34:44.145048Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"The weather is good today.\",\n",
    "    \"I want to go outside.\",\n",
    "    \"COVID-19 please go away.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.300118Z",
     "start_time": "2020-04-14T15:34:44.186843Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertConfig, BertForPreTraining, BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.304238Z",
     "start_time": "2020-04-14T15:34:44.301847Z"
    }
   },
   "outputs": [],
   "source": [
    "name = \"biobert_v1.1_pubmed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:44.347315Z",
     "start_time": "2020-04-14T15:34:44.305900Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_config_path = assets_path / name / \"bert_config.json\"\n",
    "checkpoint_path = assets_path / f\"{name}.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_json_file(bert_config_path) \n",
    "state_dict = torch.load(checkpoint_path)\n",
    "\n",
    "model_pre_training = BertForPreTraining(config)\n",
    "model_pre_training.load_state_dict(state_dict)\n",
    "biobert_model = model_pre_training.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:11:08.024008Z",
     "start_time": "2020-04-14T14:11:06.510491Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the tokenizer vocabulary is the same as the vocabulary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:26:57.911160Z",
     "start_time": "2020-04-14T14:26:57.881754Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(assets_path / name / \"vocab.txt\", 'r') as f:\n",
    "    vocab = [line.strip() for line in f]\n",
    "    \n",
    "all(v1 == v2 for v1, v2 in zip(vocab, tokenizer.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:47.579289Z",
     "start_time": "2020-04-14T15:34:47.531778Z"
    }
   },
   "outputs": [],
   "source": [
    "class SBioBERT(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, bert_config_path, checkpoint_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        config = BertConfig.from_json_file(bert_config_path) \n",
    "        state_dict = torch.load(checkpoint_path)\n",
    "\n",
    "        model_pre_training = BertForPreTraining(config)\n",
    "        model_pre_training.load_state_dict(state_dict)\n",
    "        \n",
    "        self.biobert_model = model_pre_training.bert\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        \n",
    "    def preprocess_sentence(self, sentence):\n",
    "        # Add the special tokens.\n",
    "        marked_text = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "\n",
    "        # Split the sentence into tokens.\n",
    "        tokenized_text = self.tokenizer.tokenize(marked_text)\n",
    "\n",
    "        # Map the token strings to their vocabulary indices.\n",
    "        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "        # Mark each of the tokens as belonging to sentence \"1\".\n",
    "        segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        segments_tensors = torch.tensor([segments_ids])\n",
    "    \n",
    "        return tokens_tensor, segments_tensors\n",
    "        \n",
    "        \n",
    "    def encode(self, sentences):\n",
    "        device = next(bio_embedder.parameters()).device\n",
    "        preprocessed_sentences = [self.preprocess_sentence(sentence)\n",
    "                                  for sentence in sentences]\n",
    "        \n",
    "        results = []\n",
    "        for tokens_tensor, segments_tensors in preprocessed_sentences:\n",
    "            with torch.no_grad():\n",
    "                tokens_tensor = tokens_tensor.to(device)\n",
    "                segments_tensors = segments_tensors.to(device)\n",
    "                encoded_layers, _ = self.biobert_model(tokens_tensor, segments_tensors)\n",
    "                encoded_layers = torch.stack(encoded_layers)\n",
    "                sentence_encoding = encoded_layers[-1].squeeze().mean(axis=0)\n",
    "                results.append(sentence_encoding.detach().cpu().numpy())\n",
    "                \n",
    "                del tokens_tensor, segments_tensors\n",
    "            \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:34:58.983040Z",
     "start_time": "2020-04-14T15:34:55.063190Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "bio_embedder = SBioBERT(bert_config_path, checkpoint_path)\n",
    "bio_embedder = bio_embedder.to(device)\n",
    "bio_embeddings = bio_embedder.encode(sentences)\n",
    "np.stack(bio_embeddings).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:35:02.996140Z",
     "start_time": "2020-04-14T15:35:02.526446Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:35:12.181689Z",
     "start_time": "2020-04-14T15:35:11.943039Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(np.stack(bio_embeddings).ravel(), bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:35:14.638707Z",
     "start_time": "2020-04-14T15:35:14.374011Z"
    }
   },
   "outputs": [],
   "source": [
    "sbert_embeddings = sbert_embedder.encode(sentences)\n",
    "plt.hist(np.stack(sbert_embeddings).ravel(), bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:29:11.957722Z",
     "start_time": "2020-04-14T14:29:11.952889Z"
    }
   },
   "source": [
    "## Try Sentence Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:51:49.125146Z",
     "start_time": "2020-04-14T14:51:49.122641Z"
    }
   },
   "outputs": [],
   "source": [
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:57:41.333331Z",
     "start_time": "2020-04-14T14:57:37.770268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use BERT for mapping tokens to embeddings\n",
    "word_embedding_model = sentence_transformers.models.BERT(\"bert-base-cased\")\n",
    "word_embedding_model.bert = biobert_model\n",
    "word_embedding_model.tokenizer = tokenizer\n",
    "word_embedding_model.cls_token_id = word_embedding_model.tokenizer.convert_tokens_to_ids(\n",
    "    [word_embedding_model.tokenizer.cls_token])[0]\n",
    "word_embedding_model.sep_token_id = word_embedding_model.tokenizer.convert_tokens_to_ids(\n",
    "    [word_embedding_model.tokenizer.sep_token])[0]\n",
    "\n",
    "# Apply mean pooling to get one fixed sized sentence vector\n",
    "pooling_model = sentence_transformers.models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = sentence_transformers.SentenceTransformer(modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:54:34.651644Z",
     "start_time": "2020-04-14T14:54:34.597707Z"
    }
   },
   "outputs": [],
   "source": [
    "len(model.encode(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T14:54:02.063429Z",
     "start_time": "2020-04-14T14:54:02.034306Z"
    }
   },
   "outputs": [],
   "source": [
    "model.encode(sentences)[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:35:18.521732Z",
     "start_time": "2020-04-14T15:35:18.517006Z"
    }
   },
   "outputs": [],
   "source": [
    "def sent_preprocessing(sentences, \n",
    "                      synonyms_index):\n",
    "    \"\"\"Preprocessing of the sentences. (Lower + Split + Replace Synonym)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of N strings.\n",
    "    synonyms_index: dict\n",
    "        Dictionary containing as key the synonym term and as values the reference of this term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\" \".join(synonyms_index.get(y, y) for y in word_tokenize(x.lower()))\n",
    "            for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-14T15:35:56.581556Z",
     "start_time": "2020-04-14T15:35:56.574614Z"
    }
   },
   "outputs": [],
   "source": [
    "def embed_sentences(sentences,\n",
    "                    embedding_name,\n",
    "                    embedding_model):\n",
    "    '''Sentence embedding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of N strings.\n",
    "    embedding_name : str\n",
    "        Name of the embedding type. One of ('USE', 'SBERT', 'BSV').\n",
    "    embedding_model : tf.Model or torch.Module\n",
    "        Neural net model to create sentence embeddings.\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    encodded_sentences : np.ndarray\n",
    "        Numpy array of shape (N, n_dims).\n",
    "    '''\n",
    "    if embedding_name == 'USE':\n",
    "        return embedding_model(sentences).numpy()\n",
    "    \n",
    "    elif embedding_name == 'SBERT':\n",
    "        return np.stack(embedding_model.encode(sentences), axis=0)\n",
    "    \n",
    "    elif embedding_name == 'SBIOBERT':\n",
    "        return np.stack(embedding_model.encode(sentences), axis=0)\n",
    "    \n",
    "    elif embedding_name == 'BSV':\n",
    "        preprocessed = [bsv_preprocess(x) for x in sentences]\n",
    "        return embedding_model.embed_sentences(preprocessed)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f'Embedding {repr(embedding_name)} not '\n",
    "                                  f'available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentence_embeddings(embedding_name,\n",
    "                              embedding_model,\n",
    "                              preprocessing=False):\n",
    "\n",
    "    batch_size = 1_000\n",
    "\n",
    "    x = defaultdict(list)\n",
    "    arr = defaultdict(list)\n",
    "    all_ids = []\n",
    "\n",
    "    curs.execute('SELECT Id, Text FROM sections WHERE Tags IS NOT NULL')\n",
    "    i = 0\n",
    "    t0 = time()\n",
    "    while True:\n",
    "        i += 1\n",
    "        batch = curs.fetchmany(batch_size)\n",
    "        if not batch:\n",
    "            break\n",
    "        ids, sentences = zip(*batch)  \n",
    "\n",
    "        all_ids.extend(ids)\n",
    "\n",
    "        if preprocessing:\n",
    "            sentences = sent_preprocessing(sentences, synonyms_index)\n",
    "\n",
    "        x_ = embed_sentences(sentences, \n",
    "                             embedding_name=embedding_name, \n",
    "                             embedding_model=embedding_model)\n",
    "        x[embedding_name].append(x_)\n",
    "\n",
    "        print(f'Done processing {batch_size * i} in {time()-t0:.1f} s.')\n",
    "\n",
    "\n",
    "    print('Concatenate...')\n",
    "\n",
    "    print(f'processing: {embedding_name}')\n",
    "    # Concatenate\n",
    "    xx = np.concatenate(x[embedding_name], axis=0)\n",
    "    all_ids = np.array(all_ids).reshape((-1, 1))\n",
    "    arr[embedding_name] = np.concatenate((all_ids, xx), axis=1)\n",
    "    \n",
    "    print('Save...')\n",
    "\n",
    "    if preprocessing:\n",
    "        file_name = f\"{embedding_name}_sentence_embeddings_merged_synonyms.npz\"\n",
    "    else:\n",
    "        file_name = f\"{embedding_name}_sentence_embeddings.npz\"\n",
    "    \n",
    "    np.savez_compressed(file=str(file_name), **arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_names = ['USE', 'SBERT', 'BSV', 'SBIOBERT']\n",
    "embedding_models = [univsentenc_embedder, sbert_embedder, bsv_embedder, bio_embedder]\n",
    "for embedding_name, embedding_model in zip(embedding_names, embedding_models):\n",
    "    create_sentence_embeddings(embedding_name=embedding_name,\n",
    "                               embedding_model=embedding_model,\n",
    "                               preprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for embedding_name, embedding_model in zip(embedding_names, embedding_models):\n",
    "    create_sentence_embeddings(embedding_name=embedding_name,\n",
    "                               embedding_model=embedding_model,\n",
    "                               preprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
