{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "03.04.2020\n",
    "- Use 'bert-large-nli-mean-tokens'.\n",
    "\n",
    "06.04.2020\n",
    "- Add the lower ranking of some keywords (like 'diabetes').\n",
    "- Explore how synonyms impact sentence embeddings space search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Human curated WHO papers + query* on PMC / bioRxiv / medRxiv.\n",
    "\n",
    "**Query**\n",
    "\n",
    "- \"COVID-19\"\n",
    "- OR Coronavirus\n",
    "- OR \"Corona virus\"\n",
    "- OR \"2019-nCoV\"\n",
    "- OR \"SARS-CoV\"\n",
    "- OR \"MERS-CoV\"\n",
    "- OR “Severe Acute Respiratory Syndrome”\n",
    "- OR “Middle East Respiratory Syndrome” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import hashlib\n",
    "import time\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "from functools import partial\n",
    "import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import sent2vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pdfkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_dir = Path(\"/raid/covid19_kaggle-data\")\n",
    "\n",
    "# data_path = main_dir / \"v6\"\n",
    "# sql_db_path = main_dir / \"cord19q\" / \"articles.sqlite\"\n",
    "# pafe_path = main_dir / \"pafe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/raid/sschmidt/covid/data/2020-04-08\")\n",
    "cord_path = data_path / \"CORD-19-research-challenge\"\n",
    "databases_path = data_path / \"databases\"\n",
    "embeddings_path = data_path / \"embeddings\"\n",
    "assets_path = Path(\"/raid/sschmidt/covid/assets\")\n",
    "\n",
    "assert data_path.exists()\n",
    "assert cord_path.exists()\n",
    "assert databases_path.exists()\n",
    "assert embeddings_path.exists()\n",
    "assert assets_path.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Color:\n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in dir(Color):\n",
    "    if not var.startswith('__') and var != 'END':\n",
    "        c = getattr(Color, var)\n",
    "        print(c + f\"This is {var}\" + Color.END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Color.BOLD + Color.PURPLE + \"This is a test\" + Color.END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user git+https://github.com/neuml/cord19q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scispacy model\n",
    "# !pip install --user https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.load('en_core_sci_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cord19q.etl.execute import Execute as Etl\n",
    "\n",
    "# Build SQLite database for metadata.csv and json full text files\n",
    "# Etl.run(str(cord_path), str(databases_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: SQL, JSON, Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(str(databases_path / \"articles.sqlite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_original = pd.read_csv(cord_path / \"metadata.csv\")\n",
    "df_metadata_original.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no title and no SHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_useless = df_metadata_original['title'].isna() & df_metadata_original['sha'].isna()\n",
    "df_metadata = df_metadata_original[~mask_useless]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_metadata['sha'].isna()\n",
    "df_metadata.loc[mask, 'sha'] = df_metadata.loc[mask, 'title'].apply(\n",
    "    lambda text: hashlib.sha1(str(text).encode(\"utf-8\")).hexdigest())\n",
    "df_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_json = len(list(data_path.rglob(\"*.json\")))\n",
    "json_files = []\n",
    "\n",
    "for f in tqdm(data_path.rglob(\"*.json\"), total=n_json):\n",
    "    json_files.append(json.load(open(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing titles from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file in tqdm(json_files):\n",
    "    if json_file['metadata']['title'] == '':\n",
    "        sha = json_file['paper_id']\n",
    "        idx = np.where(df_metadata['sha'] == sha)[0]\n",
    "        if len(idx) > 0:\n",
    "            new_title = df_metadata['title'].iloc[idx[0]]\n",
    "            json_file['metadata']['title'] = new_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with JSON files based on their SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_d = {\n",
    "    json_file['paper_id']: json_file\n",
    "    for json_file in json_files\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load USE\n",
    "use_version = 5\n",
    "use = hub.load(f\"https://tfhub.dev/google/universal-sentence-encoder-large/{use_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load SBERT\n",
    "sbert = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/ncbi-nlp/BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load BioSentVec\n",
    "bsv = sent2vec.Sent2vecModel()\n",
    "bsv.load_model(str(assets_path / 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'))\n",
    "\n",
    "bsv_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def bsv_preprocess(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "    tokens = [token for token in word_tokenize(text)\n",
    "              if token not in punctuation and token not in bsv_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict = dict()\n",
    "with open(assets_path / 'synonyms_list.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    for l in [l_.strip().lower() for l_ in f]:\n",
    "        if l:\n",
    "            w = [l_.strip() for l_ in l.split('=')]\n",
    "            synonyms_dict[w[0]] = w[1:]\n",
    "\n",
    "del synonyms_dict['sars']\n",
    "\n",
    "synonyms_index = {x.lower(): k.lower() for k,v in synonyms_dict.items() for x in v}\n",
    "\n",
    "def sent_preprocessing(sentences, \n",
    "                      synonyms_index):\n",
    "    \"\"\"Preprocessing of the sentences. (Lower + Split + Replace Synonym)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of N strings.\n",
    "    synonyms_index: dict\n",
    "        Dictionary containing as key the synonym term and as values the reference of this term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\" \".join(synonyms_index.get(y, y) for y in word_tokenize(x.lower()))\n",
    "            for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences(sentences, embedding_name, embedding_model):\n",
    "    if embedding_name == 'USE':\n",
    "        return embedding_model(sentences).numpy()\n",
    "    \n",
    "    elif embedding_name == 'SBERT':\n",
    "        return np.stack(embedding_model.encode(sentences), axis=0)\n",
    "    \n",
    "    elif embedding_name == 'BSV':\n",
    "        preprocessed = [bsv_preprocess(x) for x in sentences]\n",
    "        return embedding_model.embed_sentences(preprocessed)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f'Embedding {repr(embedding_name)} not '\n",
    "                                  f'available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_NAMES = ['USE', 'SBERT', 'BSV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load(embeddings_path / 'sentence_embeddings.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_syns = np.load(embeddings_path / 'sentence_embeddings_merged_synonyms.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"My logger\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paragraph(uid, sentence, db):\n",
    "    \"\"\"Find the paragraph corresponding to the given sentece\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence\n",
    "    sentence: str\n",
    "        The sentence to highlight\n",
    "    db: sqlite3.Connection\n",
    "        The database connection\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence`\n",
    "    \"\"\"\n",
    "    \n",
    "    sha, where_from = db.execute(f'SELECT Article, Name FROM sections WHERE Id = {uid}').fetchall()[0]\n",
    "    logger.debug(f\"uid = {uid}\")\n",
    "    logger.debug(f\"sha = {sha}\")\n",
    "    logger.debug(f\"where_from = {where_from}\")\n",
    "    logger.debug(f\"sentence = {sentence}\")\n",
    "    if sha in list(df_metadata['sha']) and where_from in ['TITLE', 'ABSTRACT']:\n",
    "        df_row = df_metadata[df_metadata['sha'] == sha].iloc[0]\n",
    "        if sentence in df_row['title']:\n",
    "            paragraph = df_row['title']\n",
    "        elif sentence in df_row['abstract']:\n",
    "            paragraph = df_row['abstract']\n",
    "        else:\n",
    "            raise ValueError(\"Sentence not found in title nor in abstract\")\n",
    "    elif sha in json_files_d:\n",
    "        json_file = json_files_d[sha]\n",
    "        if sentence in json_file['metadata']['title']:\n",
    "            paragraph = json_file['metadata']['title']\n",
    "        else:\n",
    "            for text_chunk in json_file['abstract'] + json_file['body_text']:\n",
    "                paragraph = text_chunk['text']\n",
    "                if sentence in paragraph:\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"sentence not found in body_text and abstract\")\n",
    "    else:\n",
    "        raise ValueError(\"SHA not found\")\n",
    "        \n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_in_paragraph(paragraph, sentence, width=80, indent=0, color=Color.BOLD + Color.PURPLE):\n",
    "    \"\"\"Highlight a given sentence in the paragraph\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence\n",
    "    sentence: str\n",
    "        The sentence to highlight\n",
    "    width : int\n",
    "        The width to which to wrapt the returned paragraph\n",
    "    indent : int\n",
    "        The indentation for the lines in the returned apragraph\n",
    "    color : str\n",
    "        The color to use for the highlight encoded as an ANSI\n",
    "        escape code\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence` with the sentence highlighted\n",
    "        in color\n",
    "    \"\"\"\n",
    "    \n",
    "    start = paragraph.index(sentence)\n",
    "    end = start + len(sentence)\n",
    "    hightlighted_paragraph = ''.join([\n",
    "        paragraph[:start],\n",
    "        '<font color=\"purple\"> <b>' + paragraph[start:end] + '</b> </font>',\n",
    "        paragraph[end:]\n",
    "    ])\n",
    "    wrapped_lines = textwrap.wrap(hightlighted_paragraph, width=width)\n",
    "    wrapped_lines = [' ' * indent + line for line in wrapped_lines]\n",
    "    formatted_paragraph = '\\n'.join(wrapped_lines)\n",
    "    \n",
    "    return formatted_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 81135\n",
    "sentence = \"This agent binds towards the pocket entrance, but fails to occupy the end of the pocket (Chapman et al., 1991) .\"\n",
    "\n",
    "paragraph = find_paragraph(uid, sentence, db)\n",
    "print(paragraph)\n",
    "# print(highlight_in_paragraph(paragraph, sentence, width=80, indent=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_report = \"\"\n",
    "\n",
    "def investigate():\n",
    "    \n",
    "    def pdf_button_on_click(b):\n",
    "        \n",
    "        print(\"Saving the results to a pdf file.\")\n",
    "#         print(wtext_str_exclusion.value)        \n",
    "    \n",
    "        formatted_html_report =  \"<h1> Parameters </h1>\"\n",
    "        formatted_html_report += f\"\"\"<ul>\n",
    "                                        <li> Model: {wselect_model.value} </li>\n",
    "                                        <li> Merge synonyms enabled: {wcheck.value} </li>\n",
    "                                        <li> Query: {wtext_query.value} </li>\n",
    "                                        <li> Deprioritised text: {wtext_exclusion.value} </li>\n",
    "                                        <li> Deprioritised strength: {deprioritization_toggles.value} </li>\n",
    "                                        <li> Excluded text: {wtext_str_exclusion.value} </li> \n",
    "                                     </ul>\n",
    "                                 \"\"\"\n",
    "        formatted_html_report += f\"<h1> Results </h1> {html_report}\"\n",
    "        pdfkit.from_string(formatted_html_report, f\"report_{datetime.datetime.now()}.pdf\")\n",
    "    \n",
    "    def investigate_on_click(b):    \n",
    "        \n",
    "        global html_report\n",
    "        html_report = \"\"\n",
    "        wout.clear_output()\n",
    "        with wout:\n",
    "            print()\n",
    "            t0 = time.time()\n",
    "            \n",
    "            if wcheck.value:\n",
    "                query_value = sent_preprocessing([wtext_query.value], synonyms_index)\n",
    "                exclu_value = sent_preprocessing([wtext_exclusion.value], synonyms_index)                \n",
    "            else:\n",
    "                query_value = [wtext_query.value]\n",
    "                exclu_value = [wtext_exclusion.value]\n",
    "                                    \n",
    "            print('Embedding query...    ', end=' ')\n",
    "            embedding_query = embed_sentences(query_value, \n",
    "                                              wselect_model.value, \n",
    "                                              eval(wselect_model.value.lower()))\n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            if exclu_value[0]:\n",
    "                print('Embedding exclusion...    ', end=' ')\n",
    "                embedding_exclu = embed_sentences(exclu_value, \n",
    "                                                  wselect_model.value, \n",
    "                                                  eval(wselect_model.value.lower()))\n",
    "                print(f'{time.time()-t0:.2f} s.')                \n",
    "            \n",
    "            print('Computing similarities...', end=' ')\n",
    "            # For scalability, we will replace this part with FAISS, as in the other part of the code base.\n",
    "            if wcheck.value:\n",
    "                arr = embeddings_syns[wselect_model.value]\n",
    "            else:\n",
    "                arr = embeddings[wselect_model.value]\n",
    "            uids, embedding_docs = arr[:, 0], arr[:, 1:]\n",
    "            similarities_query = cosine_similarity(X=embedding_query, Y=embedding_docs).squeeze()\n",
    "\n",
    "            if exclu_value[0]:\n",
    "                similarities_exclu = cosine_similarity(X=embedding_exclu, Y=embedding_docs).squeeze()\n",
    "            else:\n",
    "                similarities_exclu = np.zeros_like(similarities_query)\n",
    "                            \n",
    "            deprioritizations ={\n",
    "                'None': (1, 0),\n",
    "                'Weak': (0.9, 0.1),\n",
    "                'Mild': (0.8, 0.3),\n",
    "                'Strong': (0.5, 0.5),\n",
    "                'Stronger': (0.5, 0.7), \n",
    "            }\n",
    "            # now: maximize L = a1 * cos(x, query) - a2 * cos(x, exclusions)\n",
    "            alpha_1, alpha_2 = deprioritizations[deprioritization_toggles.value]\n",
    "            similarities = alpha_1 * similarities_query - alpha_2 * similarities_exclu\n",
    "            \n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "            \n",
    "            print('Ranking documents...     ', end=' ')\n",
    "\n",
    "            # SUBSTRING EXCLUSIONS\n",
    "            excluded_words = [x for x in wtext_str_exclusion.value.lower().split('\\n') if x] # remove empty strings\n",
    "            \n",
    "            indices = np.argsort(-similarities)\n",
    "            indices_without_excluded = []\n",
    "            \n",
    "            ix = 0\n",
    "            while len(indices_without_excluded) < wselect_count.value:\n",
    "                sentence_text = db.execute('SELECT Text FROM sections WHERE Id = ?', [uids[indices[ix]]]).fetchall()[0][0].lower()\n",
    "                is_contained = any([w in sentence_text for w in excluded_words])\n",
    "                \n",
    "                if not is_contained:\n",
    "                    indices_without_excluded.append(indices[ix])\n",
    "\n",
    "                ix += 1\n",
    "            \n",
    "\n",
    "            print(f'{time.time()-t0:.2f} s. Excluded {ix - wselect_count.value} items based on substrings.')\n",
    "            \n",
    "            print(Color.RED + f'\\nInvestigating: {query_value[0]}\\n' + Color.END)\n",
    "            \n",
    "            for i, (uid_, sim_) in enumerate(zip(uids[indices_without_excluded], similarities[indices_without_excluded])):\n",
    "                article_sha, section_name, text = db.execute('SELECT Article, Name, Text FROM sections WHERE Id = ?', [uid_]).fetchall()[0]\n",
    "                article_auth, article_title, date, ref = db.execute('SELECT Authors, Title, Published, Reference FROM articles WHERE Id = ?', [article_sha]).fetchall()[0]\n",
    "                article_auth = article_auth.split(';')[0] + ' et al.'\n",
    "                date = date.split()[0]\n",
    "                ref = ref if ref else ''\n",
    "                section_name = section_name if section_name else ''\n",
    "                \n",
    "                width = 80\n",
    "                if w_check_whole_paragraph.value:\n",
    "                    logger.debug(f\"UID={uid_}\")\n",
    "                    try:\n",
    "                        paragraph = find_paragraph(uid_, text, db)\n",
    "                        formatted_output = highlight_in_paragraph(paragraph, text, width=width, indent=2)\n",
    "                    except:\n",
    "                        formatted_output = \"<there was a problem retrieving the paragraph, the original sentence is:>\\n\"\n",
    "                        formatted_output += text\n",
    "                else:\n",
    "                    formatted_output = textwrap.fill(text, width=width)\n",
    "                \n",
    "                \n",
    "                formatted_output = f'<em> {formatted_output} </em>'\n",
    "                \n",
    "                article_metadata = f\"\"\"<a href=\"{ref}\">&nbsp;[{i+1:2d}] <br> Source: {article_title} </a>\n",
    "                                   <br> Author: {article_auth}\n",
    "                                   <br> Section: {section_name.lower().title()}\"\"\"\n",
    "                \n",
    "                display(HTML(article_metadata))\n",
    "                display(HTML(formatted_output))\n",
    "                print()\n",
    "                \n",
    "                html_report += article_metadata + f\" <br> <p> {formatted_output} </p> <br>\"\n",
    "    \n",
    "    wselect_model = widgets.ToggleButtons(\n",
    "        options=[ 'USE', 'SBERT', 'BSV'],\n",
    "        description='Model:',\n",
    "        tooltips=['Universal Sentence Encoder', 'Sentence BERT', 'BioSentVec'],\n",
    "    )\n",
    "    \n",
    "    wselect_count = widgets.IntSlider(value=10, min=0, max=100, description='Top N:',)\n",
    "    \n",
    "    wcheck = widgets.Checkbox(value=False, description='merge synonyms')\n",
    "    w_check_whole_paragraph = widgets.Checkbox(value=False, description='show whole paragraph')\n",
    "    \n",
    "    wtext_query = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'), \n",
    "                                   value='Glucose is a risk factor for COVID-19.',\n",
    "                                   description='Query: ')\n",
    "    wtext_exclusion = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'),\n",
    "                                       value='',\n",
    "                                       description='Deprioritize: ')\n",
    "    deprioritization_toggles = widgets.ToggleButtons(\n",
    "        options=['None', 'Weak', 'Mild', 'Strong', 'Stronger'],\n",
    "        description='Deprioritization strength',\n",
    "        disabled=False,\n",
    "        button_style='info', # 'success', 'info', 'warning', 'danger' or ''\n",
    "#         tooltips=['Description of slow', 'Description of regular', 'Description of fast'],\n",
    "#         icons=['check'] * 5\n",
    "        style={'description_width': 'initial', 'button_width': '80px'},\n",
    "#         layout=widgets.Layout(width='100%', height='80px'),\n",
    "    )\n",
    "\n",
    "    wtext_str_exclusion = widgets.Textarea(layout=widgets.Layout(width='90%', height='80px'),\n",
    "                                       value='',\n",
    "                                       description='Substring Exclusion (newline separated): ',\n",
    "                                       style={'description_width': 'initial'})\n",
    "    investigate_button = widgets.Button(description='Investigate!')\n",
    "    investigate_button.on_click(investigate_on_click)\n",
    "    pdf_download_button = widgets.Button(description='Generate PDF Report', layout=widgets.Layout(width='25%'))\n",
    "    \n",
    "    pdf_download_button.on_click(pdf_button_on_click)\n",
    "    \n",
    "    wout = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "    display(widgets.VBox([wselect_model, \n",
    "                          wselect_count, \n",
    "                          wcheck,\n",
    "                          w_check_whole_paragraph,\n",
    "                          wtext_query, \n",
    "                          wtext_exclusion,\n",
    "                          deprioritization_toggles,\n",
    "                          wtext_str_exclusion,\n",
    "                          investigate_button,\n",
    "                          pdf_download_button,\n",
    "                          wout]))\n",
    "investigate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Queries\n",
    "\n",
    "1. Inhibition of N-glycosylation (using N-glycosylation inhibitors or Lectins) is a potential therapeutic approach for COVID-19 therapy.\n",
    "1. Is high blood / plasma sugar level or hyperglycemia associated with higher susceptibility to coronavirus infection or higher virus replication?\n",
    "1. Glucose or sugar is a risk factor for COVID-19.\n",
    "1. Ketogenic diet is protective against COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict['sugar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict['risk factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML('And everyone knows <font style=\"background-color: #992200\"> coronavirus</font> is dangerous.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_syns.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
