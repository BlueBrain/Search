{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "03.04.2020\n",
    "- Use 'bert-large-nli-mean-tokens'.\n",
    "\n",
    "06.04.2020\n",
    "- Add the lower ranking of some keywords (like 'diabetes').\n",
    "- Explore how synonyms impact sentence embeddings space search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context\n",
    "\n",
    "**Dataset**\n",
    "\n",
    "Human curated WHO papers + query* on PMC / bioRxiv / medRxiv.\n",
    "\n",
    "**Query**\n",
    "\n",
    "- \"COVID-19\"\n",
    "- OR Coronavirus\n",
    "- OR \"Corona virus\"\n",
    "- OR \"2019-nCoV\"\n",
    "- OR \"SARS-CoV\"\n",
    "- OR \"MERS-CoV\"\n",
    "- OR “Severe Acute Respiratory Syndrome”\n",
    "- OR “Middle East Respiratory Syndrome” "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "import hashlib\n",
    "import time\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "from functools import partial\n",
    "import datetime\n",
    "from collections import OrderedDict\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import ipywidgets as widgets\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "import sent2vec\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pdfkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"/raid/covid_data/data/2020-04-08\")\n",
    "cord_path = data_path / \"CORD-19-research-challenge\"\n",
    "databases_path = data_path / \"databases\"\n",
    "embeddings_path = data_path / \"embeddings\"\n",
    "assets_path = Path(\"/raid/covid_data/assets\")\n",
    "\n",
    "assert data_path.exists()\n",
    "assert cord_path.exists()\n",
    "assert databases_path.exists()\n",
    "assert embeddings_path.exists()\n",
    "assert assets_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build SQL Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user git+https://github.com/neuml/cord19q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install scispacy model\n",
    "# !pip install --user https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# spacy.load('en_core_sci_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cord19q.etl.execute import Execute as Etl\n",
    "\n",
    "# Build SQLite database for metadata.csv and json full text files\n",
    "# Etl.run(str(cord_path), str(databases_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data: SQL, JSON, Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(str(databases_path / \"articles.sqlite\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata_original = pd.read_csv(cord_path / \"metadata.csv\")\n",
    "df_metadata_original.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with no title and no SHA (why they are even in the table??)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_useless = df_metadata_original['title'].isna() & df_metadata_original['sha'].isna()\n",
    "df_metadata = df_metadata_original[~mask_useless]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate fake SHAs for entries that do not have full-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_metadata['sha'].isna()\n",
    "df_metadata.loc[mask, 'sha'] = df_metadata.loc[mask, 'title'].apply(\n",
    "    lambda text: hashlib.sha1(str(text).encode(\"utf-8\")).hexdigest())\n",
    "df_metadata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load JSON Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_json = len(list(data_path.rglob(\"*.json\")))\n",
    "json_files = []\n",
    "\n",
    "for f in tqdm(data_path.rglob(\"*.json\"), total=n_json):\n",
    "    json_files.append(json.load(open(f)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in missing titles from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for json_file in tqdm(json_files):\n",
    "    if json_file['metadata']['title'] == '':\n",
    "        sha = json_file['paper_id']\n",
    "        idx = np.where(df_metadata['sha'] == sha)[0]\n",
    "        if len(idx) > 0:\n",
    "            new_title = df_metadata['title'].iloc[idx[0]]\n",
    "            json_file['metadata']['title'] = new_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dictionary with JSON files based on their SHAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files_d = {\n",
    "    json_file['paper_id']: json_file\n",
    "    for json_file in json_files\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load USE\n",
    "use_version = 5\n",
    "use = hub.load(f\"https://tfhub.dev/google/universal-sentence-encoder-large/{use_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load SBERT\n",
    "sbert = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/ncbi-nlp/BioSentVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load BioSentVec\n",
    "bsv = sent2vec.Sent2vecModel()\n",
    "bsv.load_model(str(assets_path / 'BioSentVec_PubMed_MIMICIII-bigram_d700.bin'))\n",
    "\n",
    "bsv_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def bsv_preprocess(text):\n",
    "    text = text.replace('/', ' / ')\n",
    "    text = text.replace('.-', ' .- ')\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace('\\'', ' \\' ')\n",
    "    text = text.lower()\n",
    "    tokens = [token for token in word_tokenize(text)\n",
    "              if token not in punctuation and token not in bsv_stopwords]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict = dict()\n",
    "with open(assets_path / 'synonyms_list.txt', 'r', encoding='utf-8-sig') as f:\n",
    "    for l in [l_.strip().lower() for l_ in f]:\n",
    "        if l:\n",
    "            w = [l_.strip() for l_ in l.split('=')]\n",
    "            synonyms_dict[w[0]] = w[1:]\n",
    "\n",
    "del synonyms_dict['sars']\n",
    "\n",
    "synonyms_index = {x.lower(): k.lower() for k,v in synonyms_dict.items() for x in v}\n",
    "\n",
    "def sent_preprocessing(sentences, \n",
    "                      synonyms_index):\n",
    "    \"\"\"Preprocessing of the sentences. (Lower + Split + Replace Synonym)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences : List[str]\n",
    "        List of N strings.\n",
    "    synonyms_index: dict\n",
    "        Dictionary containing as key the synonym term and as values the reference of this term.\n",
    "    \"\"\"\n",
    "    \n",
    "    return [\" \".join(synonyms_index.get(y, y) for y in word_tokenize(x.lower()))\n",
    "            for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences(sentences, embedding_name, embedding_model):\n",
    "    if embedding_name == 'USE':\n",
    "        return embedding_model(sentences).numpy()\n",
    "    \n",
    "    elif embedding_name == 'SBERT':\n",
    "        return np.stack(embedding_model.encode(sentences), axis=0)\n",
    "    \n",
    "    elif embedding_name == 'BSV':\n",
    "        preprocessed = [bsv_preprocess(x) for x in sentences]\n",
    "        return embedding_model.embed_sentences(preprocessed)\n",
    "        \n",
    "    else:\n",
    "        raise NotImplementedError(f'Embedding {repr(embedding_name)} not '\n",
    "                                  f'available!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_NAMES = ['USE', 'SBERT', 'BSV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = dict()\n",
    "for embeddings_name in EMBEDDINGS_NAMES:\n",
    "    embeddings[embeddings_name] = np.load(embeddings_path / f'{embeddings_name}_sentence_embeddings.npz')[embeddings_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_syns = np.load(embeddings_path / 'sentence_embeddings_merged_synonyms.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"My logger\")\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_paragraph(uid, sentence, db):\n",
    "    \"\"\"Find the paragraph corresponding to the given sentece\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence\n",
    "    sentence: str\n",
    "        The sentence to highlight\n",
    "    db: sqlite3.Connection\n",
    "        The database connection\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence`\n",
    "    \"\"\"\n",
    "    \n",
    "    sha, where_from = db.execute(f'SELECT Article, Name FROM sections WHERE Id = {uid}').fetchall()[0]\n",
    "    logger.debug(f\"uid = {uid}\")\n",
    "    logger.debug(f\"sha = {sha}\")\n",
    "    logger.debug(f\"where_from = {where_from}\")\n",
    "    logger.debug(f\"sentence = {sentence}\")\n",
    "    if sha in list(df_metadata['sha']) and where_from in ['TITLE', 'ABSTRACT']:\n",
    "        df_row = df_metadata[df_metadata['sha'] == sha].iloc[0]\n",
    "        if sentence in df_row['title']:\n",
    "            paragraph = df_row['title']\n",
    "        elif sentence in df_row['abstract']:\n",
    "            paragraph = df_row['abstract']\n",
    "        else:\n",
    "            raise ValueError(\"Sentence not found in title nor in abstract\")\n",
    "    elif sha in json_files_d:\n",
    "        json_file = json_files_d[sha]\n",
    "        if sentence in json_file['metadata']['title']:\n",
    "            paragraph = json_file['metadata']['title']\n",
    "        else:\n",
    "            for text_chunk in json_file['abstract'] + json_file['body_text']:\n",
    "                paragraph = text_chunk['text']\n",
    "                if sentence in paragraph:\n",
    "                    break\n",
    "            else:\n",
    "                raise ValueError(\"sentence not found in body_text and abstract\")\n",
    "    else:\n",
    "        raise ValueError(\"SHA not found\")\n",
    "        \n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_in_paragraph(paragraph, sentence, width=80, indent=0):\n",
    "    \"\"\"Highlight a given sentence in the paragraph.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    uid : int\n",
    "        The identifier of the given sentence.\n",
    "    sentence: str\n",
    "        The sentence to highlight.\n",
    "    width : int\n",
    "        The width to which to wrapt the returned paragraph.\n",
    "    indent : int\n",
    "        The indentation for the lines in the returned apragraph.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    formatted_paragraph : str\n",
    "        The paragraph containing `sentence` with the sentence highlighted\n",
    "        in color\n",
    "    \"\"\"\n",
    "    COLOR_TEXT = '#222222'\n",
    "    COLOR_HIGHLIGHT = '#000000'\n",
    "    \n",
    "    start = paragraph.index(sentence)\n",
    "    end = start + len(sentence)\n",
    "    hightlighted_paragraph = f'''\n",
    "        <p style=\"font-size:13px; color:{COLOR_TEXT}\">\n",
    "        {paragraph[:start]}\n",
    "        <b style=\"color:{COLOR_HIGHLIGHT}\"> {paragraph[start:end]} </b>\n",
    "        {paragraph[end:]}\n",
    "        </p>\n",
    "        '''\n",
    "#     wrapped_lines = textwrap.wrap(hightlighted_paragraph, width=width)\n",
    "#     wrapped_lines = [' ' * indent + line for line in wrapped_lines]\n",
    "#     formatted_paragraph = '\\n'.join(wrapped_lines)\n",
    "    \n",
    "    return hightlighted_paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_id = 81135\n",
    "sentence = \"This agent binds towards the pocket entrance\"\n",
    "\n",
    "paragraph = find_paragraph(sentence_id, sentence, db)\n",
    "display(HTML(highlight_in_paragraph(paragraph, sentence, width=80, indent=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_widgets = OrderedDict()\n",
    "\n",
    "\n",
    "# Select model to compute Sentence Embeddings\n",
    "my_widgets['sent_embedder'] = widgets.ToggleButtons(\n",
    "    options=[ 'USE', 'SBERT', 'BSV'],\n",
    "    description='Model for Sentence Embedding',\n",
    "    tooltips=['Universal Sentence Encoder', 'Sentence BERT', 'BioSentVec'],\n",
    ")\n",
    "\n",
    "# Select n. of top results to return\n",
    "my_widgets['top_results'] = widgets.widgets.IntSlider(\n",
    "    value=10, \n",
    "    min=0, \n",
    "    max=100, \n",
    "    description='Top N results'\n",
    ")\n",
    "\n",
    "# Choose whether to merge synonyms or not\n",
    "my_widgets['merge_synonyms'] = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='Merge synonyms'\n",
    ")\n",
    "\n",
    "# Choose whether to print whole paragraph containing sentence highlighted, or just the sentence\n",
    "my_widgets['print_paragraph'] = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Show whole paragraph'\n",
    ")\n",
    "\n",
    "# Enter Query\n",
    "my_widgets['query_text'] = widgets.Textarea(\n",
    "    value='Glucose is a risk factor for COVID-19',\n",
    "    layout=widgets.Layout(width='90%', height='80px'),\n",
    "    description='Query'\n",
    ")\n",
    "\n",
    "# Enter Deprioritization Query\n",
    "my_widgets['deprioritize_text'] = widgets.Textarea(\n",
    "    value='',\n",
    "    layout=widgets.Layout(width='90%', height='80px'),\n",
    "    description='Deprioritize'\n",
    ")\n",
    "\n",
    "# Select Deprioritization Strength\n",
    "my_widgets['deprioritize_strength'] = widgets.ToggleButtons(\n",
    "        options=['None', 'Weak', 'Mild', 'Strong', 'Stronger'],\n",
    "        disabled=False,\n",
    "        button_style='info',\n",
    "        style={'description_width': 'initial', 'button_width': '80px'},\n",
    "        description='Deprioritization strength',\n",
    "    )\n",
    "\n",
    "# Enter Substrings Exclusions\n",
    "my_widgets['exclusion_text'] = widgets.Textarea(\n",
    "    layout=widgets.Layout(width='90%', height='80px'),                                   \n",
    "    value='',\n",
    "    style={'description_width': 'initial'},\n",
    "    description='Substring Exclusion (newline separated): '                                       \n",
    ")\n",
    "\n",
    "# Click to run Information Retrieval!\n",
    "my_widgets['investigate_button'] = widgets.Button(description='Investigate!')\n",
    "\n",
    "# Click to run Generate Report!\n",
    "my_widgets['report_button'] = widgets.Button(description='Generate PDF Report!', layout=widgets.Layout(width='25%'))\n",
    "    \n",
    "# Output Area    \n",
    "my_widgets['out'] = widgets.Output(layout={'border': '1px solid black'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HtmlReport:\n",
    "    def __init__(self):\n",
    "        self.report = None\n",
    "        \n",
    "    def investigate_on_click(self, b):\n",
    "        my_widgets['out'].clear_output()\n",
    "        with my_widgets['out']:\n",
    "            self.report = ''\n",
    "            print()\n",
    "            t0 = time.time()\n",
    "\n",
    "            sentence_embedder_name = my_widgets['sent_embedder'].value\n",
    "            merge_synonyms = my_widgets['merge_synonyms'].value\n",
    "            top_n_results = my_widgets['top_results'].value\n",
    "            print_whole_paragraph = my_widgets['print_paragraph'].value\n",
    "            query_text = my_widgets['query_text'].value\n",
    "            deprioritize_text = my_widgets['deprioritize_text'].value\n",
    "            deprioritize_strength = my_widgets['deprioritize_strength'].value\n",
    "            exclusion_text = my_widgets['exclusion_text'].value\n",
    "\n",
    "\n",
    "            if merge_synonyms:\n",
    "                query_text = sent_preprocessing([query_text], synonyms_index)\n",
    "                deprioritize_text = sent_preprocessing([deprioritize_text], synonyms_index)                \n",
    "            else:\n",
    "                query_text = [query_text]\n",
    "                deprioritize_text = [deprioritize_text]\n",
    "\n",
    "            print('Embedding query...    ', end=' ')\n",
    "            embedding_query = embed_sentences(query_text, \n",
    "                                              sentence_embedder_name, \n",
    "                                              eval(sentence_embedder_name.lower()))\n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "\n",
    "            if deprioritize_text[0]:\n",
    "                print('Embedding deprioritization...    ', end=' ')\n",
    "                embedding_exclu = embed_sentences(deprioritize_text, \n",
    "                                                  sentence_embedder_name, \n",
    "                                                  eval(sentence_embedder_name.lower()))\n",
    "                print(f'{time.time()-t0:.2f} s.')                \n",
    "\n",
    "            print('Computing similarities...', end=' ')\n",
    "\n",
    "            if merge_synonyms:\n",
    "                arr = embeddings_syns[sentence_embedder_name]\n",
    "            else:\n",
    "                arr = embeddings[sentence_embedder_name]\n",
    "            sentence_ids, embeddings_corpus = arr[:, 0], arr[:, 1:]\n",
    "            similarities_query = cosine_similarity(X=embedding_query, Y=embeddings_corpus).squeeze()\n",
    "\n",
    "            if deprioritize_text[0]:\n",
    "                similarities_exclu = cosine_similarity(X=embedding_exclu, Y=embeddings_corpus).squeeze()\n",
    "            else:\n",
    "                similarities_exclu = np.zeros_like(similarities_query)\n",
    "\n",
    "            deprioritizations ={\n",
    "                'None': (1, 0),\n",
    "                'Weak': (0.9, 0.1),\n",
    "                'Mild': (0.8, 0.3),\n",
    "                'Strong': (0.5, 0.5),\n",
    "                'Stronger': (0.5, 0.7), \n",
    "            }\n",
    "            # now: maximize L = a1 * cos(x, query) - a2 * cos(x, exclusions)\n",
    "            alpha_1, alpha_2 = deprioritizations[deprioritize_strength]\n",
    "            similarities = alpha_1 * similarities_query - alpha_2 * similarities_exclu\n",
    "\n",
    "            print(f'{time.time()-t0:.2f} s.')\n",
    "\n",
    "            print('Ranking documents...     ', end=' ')\n",
    "\n",
    "            # SUBSTRING EXCLUSIONS\n",
    "            excluded_words = [x for x in exclusion_text.lower().split('\\n') if x] # remove empty strings\n",
    "\n",
    "            indices = np.argsort(-similarities)\n",
    "            indices_without_excluded = []\n",
    "\n",
    "            ix = 0\n",
    "            while len(indices_without_excluded) < top_n_results:\n",
    "                sentence_text = db.execute('SELECT Text FROM sections WHERE Id = ?', \n",
    "                                           [sentence_ids[indices[ix]]]).fetchall()[0][0].lower()\n",
    "                is_contained = any([w in sentence_text for w in excluded_words])\n",
    "\n",
    "                if not is_contained:\n",
    "                    indices_without_excluded.append(indices[ix])\n",
    "\n",
    "                ix += 1\n",
    "\n",
    "\n",
    "            print(f'{time.time()-t0:.2f} s. Excluded {ix - top_n_results} items based on substrings.')\n",
    "\n",
    "            print(f'\\nInvestigating: {query_text[0]}\\n')\n",
    "\n",
    "            for i, (sentence_id_, sim_) in enumerate(zip(sentence_ids[indices_without_excluded], \n",
    "                                                         similarities[indices_without_excluded])):\n",
    "                article_sha, section_name, text = \\\n",
    "                db.execute('SELECT Article, Name, Text FROM sections WHERE Id = ?', \n",
    "                           [sentence_id_]).fetchall()[0]\n",
    "                article_auth, article_title, date, ref = \\\n",
    "                    db.execute('SELECT Authors, Title, Published, Reference FROM articles WHERE Id = ?', \n",
    "                               [article_sha]).fetchall()[0]\n",
    "                article_auth = article_auth.split(';')[0] + ' et al.'\n",
    "                date = date.split()[0]\n",
    "                ref = ref if ref else ''\n",
    "                section_name = section_name if section_name else ''\n",
    "\n",
    "                width = 80\n",
    "                if print_whole_paragraph:\n",
    "                    logger.debug(f\"UID={sentence_id_}\")\n",
    "                    try:\n",
    "                        paragraph = find_paragraph(sentence_id_, text, db)\n",
    "                        formatted_output = highlight_in_paragraph(paragraph, text, width=width, indent=2)\n",
    "                    except:\n",
    "                        formatted_output = \"<there was a problem retrieving the paragraph, the original sentence is:>\\n\"\n",
    "                        formatted_output += text\n",
    "                else:\n",
    "                    formatted_output = textwrap.fill(text, width=width)\n",
    "\n",
    "                COLOR_TITLE = '#1A0DAB'\n",
    "                COLOR_METADATA = '#006621'\n",
    "                article_metadata = f\"\"\"\n",
    "                <a href=\"{ref}\" style=\"color:{COLOR_TITLE}; font-size:17px\"> \n",
    "                    {article_title}\n",
    "                </a>\n",
    "                <br>\n",
    "                <p style=\"color:{COLOR_METADATA}; font-size:13px\"> \n",
    "                    {article_auth} &#183; {section_name.lower().title()}\n",
    "                </p>\n",
    "                \"\"\"\n",
    "\n",
    "                display(HTML(article_metadata))\n",
    "                display(HTML(formatted_output))\n",
    "                print()\n",
    "\n",
    "                self.report += article_metadata +  formatted_output + \"<br>\"\n",
    "\n",
    "html_report = HtmlReport()\n",
    "                \n",
    "my_widgets['investigate_button'].on_click(html_report.investigate_on_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_on_click(b):        \n",
    "    print(\"Saving results to a pdf file.\")\n",
    "    \n",
    "    COLOR_HYPERPARAMETERS = '#222222'\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    hyperparameters_section =  f'<h1> Search Parameters </h1>' + \\\n",
    "                               f'<ul style=\"font-size:13; color:{COLOR_HYPERPARAMETERS}\">' + \\\n",
    "                               '<li>' + '</li> <li>'.join(['<b>' + \n",
    "                                                           ' '.join(k.split('_')).title()  + \n",
    "                                                           '</b>' + \n",
    "                                                           f': {repr(v.value)}'\n",
    "                                                          for k, v in my_widgets.items() \n",
    "                                                          if hasattr(v, 'value')]) + '</li>' + \\\n",
    "                               f'</ul>'\n",
    "    \n",
    "    results_section = f\"<h1> Results </h1> {html_report.report}\"\n",
    "    pdfkit.from_string(hyperparameters_section + results_section, f\"report_{datetime.datetime.now()}.pdf\")\n",
    "\n",
    "my_widgets['report_button'].on_click(report_on_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(widgets.VBox(list(my_widgets.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_HYPERPARAMETERS = '#5F6368'\n",
    "\n",
    "hyperparameters_section =  f'<h1> Search Parameters </h1>' + \\\n",
    "                           f'<ul style=\"font-size:13; color:{COLOR_HYPERPARAMETERS}\">' + \\\n",
    "                           '<li>' + '</li> <li>'.join([' '.join(k.split('_')).title() + f': {repr(v.value)}' \n",
    "                                                      for k, v in my_widgets.items() \n",
    "                                                      if hasattr(v, 'value')]) + '</li>' + \\\n",
    "                           f'</ul>'\n",
    "\n",
    "display(HTML(hyperparameters_section))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Queries\n",
    "\n",
    "1. Inhibition of N-glycosylation (using N-glycosylation inhibitors or Lectins) is a potential therapeutic approach for COVID-19 therapy.\n",
    "1. Is high blood / plasma sugar level or hyperglycemia associated with higher susceptibility to coronavirus infection or higher virus replication?\n",
    "1. Glucose or sugar is a risk factor for COVID-19.\n",
    "1. Ketogenic diet is protective against COVID-19."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict['sugar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonyms_dict['risk factor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_syns.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
